[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Dr. Léo Belzile\n   4.850, Côte-Sainte-Catherine\n   leo.belzile@hec.ca\n\n\n\n\n\n   Winter 2024\n   Monday\n   8:30-11:30\n   C-Ste-Cath, Caracas"
  },
  {
    "objectID": "syllabus.html#textbooks",
    "href": "syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nCourse notes for the class can be found online.\nA comprehensive reference is Maxwell et al. (2017), which is available via HEC’s library online. I will also assign readings from Meier (2022) ANOVA and Mixed Models: A Short Intro Using R, which is available online for free reading."
  },
  {
    "objectID": "syllabus.html#other-references",
    "href": "syllabus.html#other-references",
    "title": "Syllabus",
    "section": "Other references",
    "text": "Other references\n\nKeppel & Wickens (2004): out of print; comprehensive reference with a focus on effect size and marginal effects, all calculations are done by hand.\nCox (1958): out of print, but beautifully written and nontechnical."
  },
  {
    "objectID": "syllabus.html#student-hours",
    "href": "syllabus.html#student-hours",
    "title": "Syllabus",
    "section": "Student hours",
    "text": "Student hours\nI am available Monday after class and from 13:30-15:00. My office, 4.850, is located next to the southern elevators in Côte-Sainte-Catherine building.\nPlease watch this video:\n\n\nStudent hours are set times dedicated to all of you (most professors call these “office hours”; I don’t3). This means that I will be in my office waiting for you to come by if you want to talk to me in person (or remotely) with whatever questions you have. This is the best and easiest way to find me and the best chance for discussing class material and concerns."
  },
  {
    "objectID": "syllabus.html#late-work",
    "href": "syllabus.html#late-work",
    "title": "Syllabus",
    "section": "Late work",
    "text": "Late work\nProblem sets and weekly check-ins are due by Sunday. Timely submission will allow me to discuss problem sets in class. The submission modules will stay open for two weeks after the due date. I will not assign late work penalties, but will gently nudge you to stay on track."
  },
  {
    "objectID": "syllabus.html#intellectual-integrity",
    "href": "syllabus.html#intellectual-integrity",
    "title": "Syllabus",
    "section": "Intellectual integrity",
    "text": "Intellectual integrity\nPlease don’t cheat! The official policy lists the school rules regarding plagiarism and academic integrity."
  },
  {
    "objectID": "syllabus.html#student-services",
    "href": "syllabus.html#student-services",
    "title": "Syllabus",
    "section": "Student services",
    "text": "Student services\nStudents with special needs should feel free to approach me so we can best discuss accommodations. Do check out HEC Montréal’s disabled students and psychological support services."
  },
  {
    "objectID": "syllabus.html#harassment-and-sexual-violence",
    "href": "syllabus.html#harassment-and-sexual-violence",
    "title": "Syllabus",
    "section": "Harassment and sexual violence",
    "text": "Harassment and sexual violence\nThe Center for Harassment Intervention (BIMH) is the unique access point for all members of the community subject to harassment or sexual violence. You can reach them at 514 343-7020 or by email at harcelement@hec.ca from Monday until Friday, from 8:30 until 4:30pm.\nIf you are in an emergency situation or fear for your safety, call emergency services at 911, followed by HEC Montréal security services at 514 340-6611.\nCheck the school official policy on these matters for more details."
  },
  {
    "objectID": "syllabus.html#family-policy",
    "href": "syllabus.html#family-policy",
    "title": "Syllabus",
    "section": "Family policy",
    "text": "Family policy\nHEC now has an official family policy, but the following guidelines reflect my own beliefs and commitments towards parent students4\n\nBabies are welcome in class as often as necessary for support feeding relationship.\nYou are welcome to bring your child to class in order to cover unforeseeable gaps in childcare.\nIf you come with babies or toddler, I ask that you sit close to the door so that, in case your little one needs special attention and is disrupting the learning of other students, you may step outside of class until their needs are met. Seats close to the door are reserved for parents attending class with their child."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe ability to compute custom contrasts and fit linear mixed models are deal breakers.↩︎\nThis is the easiest way to set up SAS and does not require a virtual machine with Windows alongside a Unix operating system.↩︎\nThere’s fairly widespread misunderstanding about what office hours actually are! Many students often think that they are the times I shouldn’t be disturbed, which is the exact opposite of what they’re for!↩︎\nShamelessly stolen/adapted from similar policy by Drs. Melissa Cheney, Guy Grossman and Rohan Alexander↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Experimental Designs and Statistical Methods\n        ",
    "section": "",
    "text": "Experimental Designs and Statistical Methods\n        \n        \n            Basic concepts for data collection planning; appropriate statistical analyses of these data and interpretation of results. Advantages and disadvantages of the various experimental designs.\n        \n        \n            MATH 80667A, Winter 2024HEC Montréal\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\n\nInstructor\n\n   Dr. Léo Belzile\n   4.850, Côte-Sainte-Catherine\n   leo.belzile@hec.ca\n\n\n\nCourse details\n\n   Winter 2024\n   Monday\n   8:30-11:30\n   C-Ste-Cath, Caracas\n\n\n\nContacting me\nI am best reached by email."
  },
  {
    "objectID": "example/threewayanova.html",
    "href": "example/threewayanova.html",
    "title": "Three-way analysis of variance",
    "section": "",
    "text": "The R code created in the video can be downloaded here and the SPSS code here.\n\nThere’s a set of videos that walks through each section below. To make it easier for you to jump around the video examples, I cut the long video into smaller pieces and included them all in one YouTube playlist.\n\nIntroduction\nInteraction plots\nMarginal contrast and simple effects\nMore contrasts and interactions\nSPSS walkthrough\n\nYou can also watch the playlist (and skip around to different sections) here:"
  },
  {
    "objectID": "example/threewayanova.html#data-description",
    "href": "example/threewayanova.html#data-description",
    "title": "Three-way analysis of variance",
    "section": "Data description",
    "text": "Data description\nThere are three factors: the first is feedback, which indicates the type of verbal feedback received by the participant: either none (\\(a_1\\)) to serve as control group, positive (\\(a_2\\)) or negative (\\(a_3\\)) feedback. The second factor is related to the type of learning material, one of low-frequency words with low emotional content (\\(b_1\\)), high-frequency words with low emotional content (\\(b_2\\)) and finally high-frequency words with high emotional content (\\(b_3\\)). The third factor is the target population: either fifth graders (\\(c_1\\)) or high school seniors (\\(c_2\\)). While most of the interest is in the first two factors, the three-way ANOVA is a more efficient design if we want to study both age groups.\nThe response variable is the number of words remembered (words) after one week. The design is balanced: there are \\(r=5\\) replications for each scenario, so \\(n=3 \\times 3 \\times 2 \\times 5\\) observations. If we fit the three-way model with all two ways and three way interactions, we need to estimate 19 parameters (18 means and one variance). We model each participant response assuming the measurements are independent and \\(Y_{ijkr} \\sim \\mathsf{No}(\\mu_{ijk}, \\sigma^2)\\): this indicates that each subgroup (\\(a_i, b_j, c_k\\)) has a different (theoretical) average \\(\\mu_{ijk}\\) and a common variance \\(\\sigma^2\\). The estimates \\(\\widehat{\\mu}_{ijk}\\) are simply the sample averages of each group, whereas the pooled variance estimator, \\[\\widehat{\\sigma}^2 = \\frac{1}{72}\\sum_{i=1}^3 \\sum_{j=1}^3 \\sum_{k=1}^2 \\sum_{r=1}^5 (y_{ijkr} - \\widehat{\\mu}_{ijk})^2,\\] is the sum of squared difference between replicate observations and their group average, divided by the residual degrees of freedom (total number of observations minus number of mean parameters).\nThe model can be reparametrized in terms of main effects and interaction: overall average for each factor, average of residual affect after accounting for effects of row, columns and depth and finally residual affect for the three-way. This parametrization is \\[\\begin{align*}\\underset{\\text{theoretical average}}{\\mathsf{E}(Y_{ijkr})} &= \\quad \\underset{\\text{global mean}}{\\mu} \\\\ &\\quad +\\underset{\\text{main effects}}{\\alpha_i + \\beta_j + \\gamma_k}  \\\\ & \\quad + \\underset{\\text{two-way interactions}}{(\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk}} \\\\ & \\quad + \\underset{\\text{three-way interaction}}{(\\alpha\\beta\\gamma)_{ijk}}\\end{align*}\\]"
  },
  {
    "objectID": "example/threewayanova.html#setup",
    "href": "example/threewayanova.html#setup",
    "title": "Three-way analysis of variance",
    "section": "Setup",
    "text": "Setup\nWhat are comparisons of interest? We may be interested in the effect of feedback, for example comparing whether any form of feedback increases word retention. Thus, we could look at the marginal contrast \\(\\mu_{1..} = \\frac{1}{2}(\\mu_{2..} + \\mu_{3..})\\), in essence treating the whole model as a one-way ANOVA but using all the data to estimate the standard deviation \\(\\sigma\\). For a balanced sample, the estimated average for example of the control group none, \\(\\widehat{\\mu}_{1..}\\), would be the sample average of the 30 participants assigned to this experimental condition.\nWe can proceed similarly for the other factors. One could be interested in whether the type of learning material impacts retention (overall effect), or if there are difference between high and low emotional content for high-frequency occurrences (\\(b_2\\) and \\(b_3\\)); amounting to ignoring all observations for the low emotion low-frequency group.\nThe first step is to load the data and the packages needed for the analysis.\n\n\nCode\n# Load packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(emmeans)\n# Load data\ndata(words, package = 'hecedsm')\n# Check balance\nxtabs(~feedback + age + material, \n      data = words)\n\n\n, , material = low freq/low emotion\n\n          age\nfeedback   fifth grade senior\n  none               5      5\n  positive           5      5\n  negative           5      5\n\n, , material = high freq/low emotion\n\n          age\nfeedback   fifth grade senior\n  none               5      5\n  positive           5      5\n  negative           5      5\n\n, , material = high freq/high emotion\n\n          age\nfeedback   fifth grade senior\n  none               5      5\n  positive           5      5\n  negative           5      5\n\n\nThis is a 3 by 3 by 2 factorial design with \\(r=5\\) replicates.\n\n\nCode\nmodel &lt;- aov(words ~ feedback * material * age,\n            data = words)\nanova(model)\n\n\nAnalysis of Variance Table\n\nResponse: words\n                      Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nfeedback               2  26.867  13.433  7.1118  0.001519 ** \nmaterial               2  64.867  32.433 17.1706 7.993e-07 ***\nage                    1  14.400  14.400  7.6235  0.007304 ** \nfeedback:material      4  14.667   3.667  1.9412  0.112829    \nfeedback:age           2   8.600   4.300  2.2765  0.109987    \nmaterial:age           2  16.200   8.100  4.2882  0.017397 *  \nfeedback:material:age  4  10.000   2.500  1.3235  0.269461    \nResiduals             72 136.000   1.889                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSince the data are balanced, we can use the aov function and look at the analysis of variance produced by aov.1 To get all pairwise and the triplewise interaction, we use response ~ factor1 * factor2 * factor3 notation. Since we have replications, the full model doesn’t fit the data exactly and there are residual observations to estimate the variability, but estimating reliably different variance in each of the 18 subgroup wouldn’t be possible with 5 observations. We can see that the three-way interaction isn’t significative, and the only two-way effect is for material:age."
  },
  {
    "objectID": "example/threewayanova.html#interaction-plot",
    "href": "example/threewayanova.html#interaction-plot",
    "title": "Three-way analysis of variance",
    "section": "Interaction plot",
    "text": "Interaction plot\nWe can try to infer whether there is an interaction by looking at averages for each pair of variable at a time, and then at all three factors concurrently. These plots theoretically are used to demonstrate the impact of interactions, but in practice the sample estimates are noisy proxies of the true subgroup averages.\nWe can confirm our findings by looking at the interaction plot, showing the group average for each combination of the factors.\n\n\nCode\nwords |&gt;\n  group_by(feedback, material, age) |&gt;\n  summarize(meanwords = mean(words)) |&gt;\nggplot(mapping = aes(x = feedback,\n                     y = meanwords,\n                     group = material,\n                     color = material)) +\n  geom_line() + # connect the dots\n  facet_wrap(~age) +\n  labs(subtitle = \"mean number of words remembered\",\n       y = \"\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nThese summary statistics hide the raw data: due to the discreteness of the response, which consists of counts, we can slightly perturb observations and jitter them.\n\n\nCode\nggplot(data = words,\n         mapping = aes(x = age,\n                       y = words,\n                       group = material,\n                       color = material)) +\n  geom_point(position = position_jitterdodge(jitter.height = 0.1,\n                                             dodge.width = 0.3)) +\n  facet_wrap(~feedback) + # wrap by third variable\n  theme_classic() + # change default theme\n  theme(legend.position = \"bottom\") + # move caption\n  labs(subtitle = \"mean number of words remembered\",\n       y = \"\")\n\n\n\n\n\nFigure 1: Individual results for an experiment on the impact of verbal feedback on retention of information.\n\n\n\n\nThe high frequency/high emotion group has significantly lower retention depending on feedback, with seemingly no difference with other material type when there is no verbal feedback, but a strong decrease for positive and even stronger for negative feedback. By contrasts, there is little to no effect with high school seniors, suggesting that the experimental manipulation has limited impact for these groups."
  },
  {
    "objectID": "example/threewayanova.html#main-effects-and-contrasts",
    "href": "example/threewayanova.html#main-effects-and-contrasts",
    "title": "Three-way analysis of variance",
    "section": "Main effects and contrasts",
    "text": "Main effects and contrasts\nThe whole purpose of Chapters 21 and 22 is to show how we can explore the three-way factorial design and extract information by treating it as a one-way ANOVA with 18 groups, a series of two-way ANOVA, etc.\nWe look at the estimated marginal means and start by computing the marginal mean by averaging over levels of material and age, since there did not seem to be interaction between feedback and the other variables.\nAs before with the emmeans package, we specify the variables we want to keep in specs, the other being ‘marginalized out’ by ignoring the counts. The package will print a warning since the model includes an interaction, indicating that the output is potentially misleading.\n\n\nCode\nmargA &lt;- emmeans(model, specs = \"feedback\")\nmargA\n\n\n feedback emmean    SE df lower.CL upper.CL\n none       8.37 0.251 72     7.87     8.87\n positive   7.30 0.251 72     6.80     7.80\n negative   7.13 0.251 72     6.63     7.63\n\nResults are averaged over the levels of: material, age \nConfidence level used: 0.95 \n\n\nSince the data are balanced, this is simply the average words recalled per feedback group. We can check that this is indeed the mean of each word count by feedback by computing the summary statistics and comparing them with the output of emmeans:\n\n\nCode\nwords |&gt;\n    group_by(feedback) |&gt;\n    summarize(mean = mean(words))\n\n\n# A tibble: 3 × 2\n  feedback  mean\n  &lt;fct&gt;    &lt;dbl&gt;\n1 none      8.37\n2 positive  7.3 \n3 negative  7.13\n\n\n\nMarginal contrast\nWe next compute a marginal contrast by comparing between no-feedback and the average of positive and negative. This amounts to treating our data as a one-way ANOVA and computing contrasts as usual.\nThe null hypothesis is \\[\\mathscr{H}_0: \\mu_{\\text{none}..} = \\frac{1}{2} \\left(\\mu_{\\text{pos}..} + \\mu_{\\text{neg}..}\\right)\\] which can be rexpressed in terms of contrast vector as \\((1, -0.5, -0.5)\\) or any non-zero multiple of this solution.\n\n\nCode\ncontrast(object = margA,\n         method = list(interaction = c(1, -0.5, -0.5)))\n\n\n contrast    estimate    SE df t.ratio p.value\n interaction     1.15 0.307 72   3.742  0.0004\n\nResults are averaged over the levels of: material, age \n\n\n\n\nSimple effects\nSince the two-way interaction between factors material and age is significant, we can test for differences between levels of material within each value of age (averaging over all levels of feedback).\nTo do this with emmeans, we first marginalize out over feedback by keeping only the two other variables in specs, then perform an \\(F\\) test for each value of age by using joint_tests and specifying the analysis is conditional on age.\n\n\nCode\nsimpleBpC &lt;-\n  emmeans(model,\n          specs = c(\"material\", \"age\")) |&gt;\n  joint_tests(by = \"age\")\nsimpleBpC\n\n\nage = fifth grade:\n model term df1 df2 F.ratio p.value\n material     2  72  19.306  &lt;.0001\n\nage = senior:\n model term df1 df2 F.ratio p.value\n material     2  72   2.153  0.1236\n\n\nWe could also look at simple contrasts by levels of age comparing the three material and looking at low versus high emotions. This amounts to reducing data to a two-way ANOVA in a first step, then taking each depth (age) group in turn and computing a contrast.\nUsing the notation \\(A \\times B \\times C\\) to denote the feedback, material and age groups, this compares\n\\[\\mathscr{H}_0: \\frac{1}{2} \\left(\\mu_{.1k} + \\mu_{.2k}\\right) = \\mu_{.3k}\\] for age group \\(k\\).\nIn both cases, we get two sets of tests statistics and \\(p\\)-values since there are two age groups.\n\n\nCode\n# Simple contrasts\nmargBC &lt;- emmeans(model,\n                  specs = c(\"material\", \"age\"),\n                  by = \"age\")\nlevels(margBC)\n\n\n$material\n[1] \"low freq/low emotion\"   \"high freq/low emotion\"  \"high freq/high emotion\"\n\n$age\n[1] \"fifth grade\" \"senior\"     \n\n\nCode\nmargBC |&gt;\n  contrast(method = list(contrast = c(-0.5, -0.5, 1)))\n\n\nage = fifth grade:\n contrast estimate    SE df t.ratio p.value\n contrast     -2.7 0.435 72  -6.212  &lt;.0001\n\nage = senior:\n contrast estimate    SE df t.ratio p.value\n contrast     -0.9 0.435 72  -2.071  0.0420\n\nResults are averaged over the levels of: feedback \n\n\nThere is strong (unadjusted) differences between low and high emotion for fifth grades, but the effect for senior is smaller and quite uncertain.\n\n\nInteraction components\nThe next potential object of interest is the interaction components based on marginal means.\nKeppel & Wickens (2004) compare the average of \\(b_2\\) and \\(b_3\\) between age groups, fixing the frequency and varying only the emotional content.\nThe first step with emmeans is to specify which variables to keep, then slice by age group. This gives us a set of 3 averages per age group. Next, we set up the contrast vector to compute for each age group. The last call, to joint_tests(), proceeds with comparing whether the average contrast results are the same in each age group by comparing the two. There is some evidence that high emotion words has a different impact, independent of feedback.\n\n\nCode\nemmeans(model,\n        specs = c(\"material\", \"age\"),\n        by = \"age\") |&gt;\n  contrast(method = list(b2vsb3 = c(0, 1, -1))) |&gt;\n  joint_tests()\n\n\n model term df1 df2 F.ratio p.value\n age          1  72   6.432  0.0134\n\n\nWe could also compute differences between feedback and material. Marginalizing over age yields a total of 9 cells: we treat the resulting two-way design as a one-way design with nine groups and simply compute the contrast vector, assigning weight zero for the low-frequency and low emotion group and then looking at none (weight \\(1\\)) and the average of positive and negative feedback (weights of \\(-0.5\\)). Since we want to compare this difference between \\(b_2\\) and \\(b_3\\), we give similar weights for the last three groups, but instead of \\((1, -0.5, -0.5)\\), we flip the signs.\n\n\nCode\nemmAB &lt;- emmeans(model, \n        specs = c(\"feedback\", \"material\"))\nlevels(emmAB)\n\n\n$feedback\n[1] \"none\"     \"positive\" \"negative\"\n\n$material\n[1] \"low freq/low emotion\"   \"high freq/low emotion\"  \"high freq/high emotion\"\n\n\nCode\ncontr &lt;- list(\"none vs feedback for b2 vs b3\" =\n  c(0, 0, 0, -1, 0.5, 0.5, 1, -0.5, -0.5)\n)\n\nemmAB |&gt;\n  contrast(method = contr)\n\n\n contrast                      estimate    SE df t.ratio p.value\n none vs feedback for b2 vs b3      1.4 0.753 72   1.860  0.0670\n\nResults are averaged over the levels of: age \n\n\n\n\nThree factor interaction\nThe most complicated type of comparison is the three factor interaction (which wasn’t deemed significant). We could look at a particular combination by looking at the difference of difference. This is the same as what we just computed, but this time we will compare this difference of difference between age groups to see if it is the same.\n\n\\(A\\) (feedback): no feedback vs feedback (none vs average of pos and neg),\n\\(B\\) (material): comparing low vs high emotional feedback (b2 vs b3)\n\\(C\\): fifth graders vs seniors\n\n\n\nCode\nthreewaycontrast &lt;- \n  emmeans(model, \n          specs = c(\"feedback\", \"material\",\"age\")) |&gt;\n  contrast(method = list(\n    contrast =c(0, 0, 0, -1, 0.5, 0.5,\n                1, -0.5, -0.5, 0, 0, 0,\n                1, -0.5, -0.5, -1, 0.5, 0.5)))\nsummary(threewaycontrast, \n        adjust = \"scheffe\",\n        scheffe.rank = 17)\n\n\n contrast estimate   SE df t.ratio p.value\n contrast        3 1.51 72   1.993  0.9992\n\nP value adjustment: scheffe method with rank 17 \n\n\nWe can adjust for multiplicity by using Scheffé’s method, but we need to set up the rank of the test: for general contrasts in a two-way slice, this would be the number of groups minus one, so 8 if we look at feedback and material, 2 if we look only at the feedback type, etc.\nThere is no evidence here that this setup comparison for looking at differences for feedback among high frequency groups is any different between age groups."
  },
  {
    "objectID": "example/threewayanova.html#footnotes",
    "href": "example/threewayanova.html#footnotes",
    "title": "Three-way analysis of variance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWith unbalanced data, we would need to fit the model using lm and use car::Anova to get type 2 or 3 effects.↩︎"
  },
  {
    "objectID": "example/power.html",
    "href": "example/power.html",
    "title": "Power calculations",
    "section": "",
    "text": "You can download G*Power to perform these calculations.\nThe following quotes are taken from the Reproducibility Project: Psychology\n\n\n\nIn Study 4a there are two effects of theoretical interest, a substantial main effect of anchor precision that replicates the first three studies and a small interaction (between precision and motivation within which people can adjust) that is not central to the paper. The main effect of anchor precision (effect size \\(\\eta^2_p=0.55\\)) would require a sample size of \\(10\\) for \\(80\\)% power, \\(12\\) for \\(90\\)% power, and \\(14\\) for \\(95\\)% power. The interaction (effect size \\(\\eta^2_p=0.11\\)) would require a sample size of \\(65\\) for \\(80\\)% power, \\(87\\) for \\(90\\)% power, and \\(107\\) for \\(95\\)% power. There was also a theoretically uninteresting main effect of motivation (people adjust more when told to adjust more).\n\n\nThe result that is object of this replication is the interaction between item strength (massed vs. spaced presentation) and condition (directed forgetting vs. control). The dependent variable is the proportion of correctly remembered items from the stimulus set (List 1). “(..) The interaction was significant, \\(F(1,94)=4.97\\), \\(p &lt;.05\\), \\(\\mathrm{MSE} =0.029\\), \\(\\eta^2=0.05\\), (…)”. (p. 412). Power analysis (G*Power (Version 3.1): ANOVA: Repeated measures, within-between interaction with a zero correlation between the repeated measures) indicated that sample sizes for \\(80\\)%, \\(90\\)% and \\(95\\)% power were respectively \\(78\\), \\(102\\) and \\(126\\).\n\n\nIn Experiment 2, the critical test of the cleanliness manipulation on ratings of morality was significant, \\(F(1, 41)=7.81\\), \\(p= 0.01\\), \\(d=0.87\\), \\(N=44\\). Assuming \\(\\alpha=0.05\\), the achieved power in this experiment was \\(80\\)%. Our proposed research will attempt to replicate this experiment with a level of power of \\(99\\)%. This will require a minimum of \\(100\\) participants (assuming equal sized groups with \\(d=0.87\\)) so we will collect data from \\(115\\) participants to ensure a properly powered sample in case of errors.\n\n\nOur study will directly replicate both experiments from Schall and colleagues (2008a). In Experiment 1, the critical test of the cleanliness prime on ratings of morality was marginally significant, \\(F(1, 38)=3.63\\), \\(p=0.06\\), \\(d=0.61\\), \\(N=40\\). Assuming \\(\\alpha=0.05\\), the achieved power in this experiment was \\(46\\)%. Our research will replicate this experiment with a level of power of \\(99\\)%. This will require a minimum of \\(200\\) participants (assuming equal sized groups with \\(d=0.61\\)).\n\n\nWe aim at testing the two main effects of prediction 1 and prediction 3. Given the \\(2 \\times 3\\) within factors design for both main effects, we calculated \\(\\eta^2_p\\) based on \\(F\\)-Values and degrees of freedom. This procedure resulted in \\(\\eta^2_p=0.427\\) and \\(\\eta^2_p=0.389\\) for the effect of prediction 1 (\\(F(1, 36)=22.88\\)) and prediction 3 (\\(F(1, 36)=26.88\\)), respectively. Accordingly, G*Power (Version 3.1) indicates that a power of \\(80\\)%, \\(90\\)%, and \\(95\\)% is achieved with sample sizes of \\(3\\), \\(4\\), and \\(4\\) participants, respectively, for both effects (assuming a correlation of \\(r=0.5\\) between repeated measures in all power calculations).\n\n\nThe original effect size for the one-sample t-test that tested the primary prediction was Cohen’s \\(d=0.93\\), \\(95\\)% Cl \\([0.72, 1.14]\\). A power analysis using G*Power to determine the sample sizes necessary to achieve \\(80\\)%, \\(90\\)%, \\(95\\)% power to detect the effect size indicates that samples with \\(12\\), \\(15\\), and \\(18\\) total participants are necessary.\n\n\nThe effect size for the finding that has been targeted for replication is a Cohen’s \\(d\\) of \\(0.451\\), which was the effect size found in the original study (the Reproducibility Project: Psychology guidelines specify using the original effect size when computing power). Consistent with the original study, a two-tailed test with an alpha of \\(0.05\\) will be used. Assuming an equal number of participants in each group, a sample size of \\(158\\) participants are needed to achieve a power of \\(80\\)% to detect an effect this large or larger. For \\(90\\)% power, \\(210\\) participants would be necessary, and for \\(95\\)% power, \\(258\\) participants would be necessary."
  },
  {
    "objectID": "example/nonparametric.html",
    "href": "example/nonparametric.html",
    "title": "Nonparametric tests",
    "section": "",
    "text": "We consider a within-subject design from Brodeur et al. (2021), who conducted an experiment to check distraction while driving from different devices including smartwatches using a virtual reality environment. The response is the number of road safety violations during the task.\n\n\nCode\nlibrary(coin, quietly = TRUE)\ndata(BRLS21_T3, package = \"hecedsm\")\nstr(BRLS21_T3)\n\n\nClasses 'tbl_df', 'tbl' and 'data.frame':   124 obs. of  3 variables:\n $ task      : Factor w/ 4 levels \"phone\",\"watch\",..: 1 2 3 4 1 2 3 4 1 2 ...\n $ nviolation: int  8 4 10 12 1 5 0 7 5 6 ...\n $ id        : Factor w/ 31 levels \"1\",\"2\",\"3\",\"4\",..: 6 6 6 6 11 11 11 11 26 26 ...\n\n\nCode\nxtabs(~ task + id, data = BRLS21_T3)\n\n\n         id\ntask      1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n  phone   1 1 1 1 1 1 1 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n  watch   1 1 1 1 1 1 1 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n  speaker 1 1 1 1 1 1 1 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n  texting 1 1 1 1 1 1 1 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n         id\ntask      27 28 29 30 31\n  phone    1  1  1  1  1\n  watch    1  1  1  1  1\n  speaker  1  1  1  1  1\n  texting  1  1  1  1  1\n\n\nA quick inspection reveals that the data are balanced with four tasks and 31 individuals. We can view the within-subject design with a single replication as a complete block design (with id as block) and task as experimental manipulation.\nHow could we compare the different tasks? The data here are clearly very far from normally distributed and there are notable outliers among the residuals, as evidenced by Figure 1. Conclusions probably wouldn’t be affected by using an analysis of variance, but it may be easier to convince reviewers that the findings are solid by ressorting to nonparametric procedures.\n\n\n\n\n\nFigure 1: Normal quantile-quantile plot of the block design. There are many outliers\n\n\n\n\nBoth the Friedman and the Quade tests are obtained by computing ranks within each block (participant) and then performing a two-way analysis of variance. The Friedman test is less powerful than Quade’s with a small number of groups. Both are applicable for block designs with a single factor. We can also obtain effect sizes for the rank test, termed Kendall’s \\(W\\). A value of 1 indicates complete agreement in the ranking: here, this would occur if the ranking of the number of violations was the same for each participant.\n\n\nCode\nfriedman &lt;- coin::friedman_test(\n  nviolation ~ task | id,\n  data = BRLS21_T3)\nquade &lt;- coin::quade_test(\n  nviolation ~ task | id,\n  data = BRLS21_T3)\neff_size &lt;- effectsize::kendalls_w(\n  x = \"nviolation\", \n  groups = \"task\", \n  blocks = \"id\", \n  data = BRLS21_T3)\n\n\nThe Friedman test is obtained by replacing observations by the rank within each block (so rather than the number of violations per task, we compute the rank among the four tasks). The Friedman’s test statistic is \\(18.97\\) and is compared to a benchmark \\(\\chi^2_3\\) distribution, yielding a \\(p\\)-value of \\(3\\times 10^{-4}\\). The estimated agreement (effect size) is \\(0.2\\).\nThe test reveals significant differences in the number of road safety violations across tasks. We could therefore perform all pairwise differences using the signed-rank test and adjust \\(p\\)-values to correct for the fact we have performed six hypothesis tests.\nTo do this, we modify the data and map them to wide-format (each line corresponds to an individual). We can then feed the data to compute differences, here for phone vs watch. We could proceed likewise for the five other pairwise comparisons and then adjust \\(p\\)-values.\n\n\nCode\nsmartwatch &lt;- tidyr::pivot_wider(\n  data = BRLS21_T3,\n  names_from = task,\n  values_from = nviolation)\ncoin::wilcoxsign_test(phone ~ watch,\n                      data = smartwatch)\n\n\n\n    Asymptotic Wilcoxon-Pratt Signed-Rank Test\n\ndata:  y by x (pos, neg) \n     stratified by block\nZ = 0.35399, p-value = 0.7233\nalternative hypothesis: true mu is not equal to 0\n\n\nYou can think of the test as performing a paired \\(t\\)-test for the 31 signed ranks \\(R_i =\\mathsf{sign}(D_i) \\mathsf{rank}(|D_i|)\\) and testing whether the mean is zero. The \\(p\\)-value obtained by doing this after discarding zeros is \\(0.73\\), which is pretty much the same as the more complicated approximation."
  },
  {
    "objectID": "example/nonparametric.html#study-1-distraction-from-smartwatches",
    "href": "example/nonparametric.html#study-1-distraction-from-smartwatches",
    "title": "Nonparametric tests",
    "section": "",
    "text": "We consider a within-subject design from Brodeur et al. (2021), who conducted an experiment to check distraction while driving from different devices including smartwatches using a virtual reality environment. The response is the number of road safety violations during the task.\n\n\nCode\nlibrary(coin, quietly = TRUE)\ndata(BRLS21_T3, package = \"hecedsm\")\nstr(BRLS21_T3)\n\n\nClasses 'tbl_df', 'tbl' and 'data.frame':   124 obs. of  3 variables:\n $ task      : Factor w/ 4 levels \"phone\",\"watch\",..: 1 2 3 4 1 2 3 4 1 2 ...\n $ nviolation: int  8 4 10 12 1 5 0 7 5 6 ...\n $ id        : Factor w/ 31 levels \"1\",\"2\",\"3\",\"4\",..: 6 6 6 6 11 11 11 11 26 26 ...\n\n\nCode\nxtabs(~ task + id, data = BRLS21_T3)\n\n\n         id\ntask      1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n  phone   1 1 1 1 1 1 1 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n  watch   1 1 1 1 1 1 1 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n  speaker 1 1 1 1 1 1 1 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n  texting 1 1 1 1 1 1 1 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n         id\ntask      27 28 29 30 31\n  phone    1  1  1  1  1\n  watch    1  1  1  1  1\n  speaker  1  1  1  1  1\n  texting  1  1  1  1  1\n\n\nA quick inspection reveals that the data are balanced with four tasks and 31 individuals. We can view the within-subject design with a single replication as a complete block design (with id as block) and task as experimental manipulation.\nHow could we compare the different tasks? The data here are clearly very far from normally distributed and there are notable outliers among the residuals, as evidenced by Figure 1. Conclusions probably wouldn’t be affected by using an analysis of variance, but it may be easier to convince reviewers that the findings are solid by ressorting to nonparametric procedures.\n\n\n\n\n\nFigure 1: Normal quantile-quantile plot of the block design. There are many outliers\n\n\n\n\nBoth the Friedman and the Quade tests are obtained by computing ranks within each block (participant) and then performing a two-way analysis of variance. The Friedman test is less powerful than Quade’s with a small number of groups. Both are applicable for block designs with a single factor. We can also obtain effect sizes for the rank test, termed Kendall’s \\(W\\). A value of 1 indicates complete agreement in the ranking: here, this would occur if the ranking of the number of violations was the same for each participant.\n\n\nCode\nfriedman &lt;- coin::friedman_test(\n  nviolation ~ task | id,\n  data = BRLS21_T3)\nquade &lt;- coin::quade_test(\n  nviolation ~ task | id,\n  data = BRLS21_T3)\neff_size &lt;- effectsize::kendalls_w(\n  x = \"nviolation\", \n  groups = \"task\", \n  blocks = \"id\", \n  data = BRLS21_T3)\n\n\nThe Friedman test is obtained by replacing observations by the rank within each block (so rather than the number of violations per task, we compute the rank among the four tasks). The Friedman’s test statistic is \\(18.97\\) and is compared to a benchmark \\(\\chi^2_3\\) distribution, yielding a \\(p\\)-value of \\(3\\times 10^{-4}\\). The estimated agreement (effect size) is \\(0.2\\).\nThe test reveals significant differences in the number of road safety violations across tasks. We could therefore perform all pairwise differences using the signed-rank test and adjust \\(p\\)-values to correct for the fact we have performed six hypothesis tests.\nTo do this, we modify the data and map them to wide-format (each line corresponds to an individual). We can then feed the data to compute differences, here for phone vs watch. We could proceed likewise for the five other pairwise comparisons and then adjust \\(p\\)-values.\n\n\nCode\nsmartwatch &lt;- tidyr::pivot_wider(\n  data = BRLS21_T3,\n  names_from = task,\n  values_from = nviolation)\ncoin::wilcoxsign_test(phone ~ watch,\n                      data = smartwatch)\n\n\n\n    Asymptotic Wilcoxon-Pratt Signed-Rank Test\n\ndata:  y by x (pos, neg) \n     stratified by block\nZ = 0.35399, p-value = 0.7233\nalternative hypothesis: true mu is not equal to 0\n\n\nYou can think of the test as performing a paired \\(t\\)-test for the 31 signed ranks \\(R_i =\\mathsf{sign}(D_i) \\mathsf{rank}(|D_i|)\\) and testing whether the mean is zero. The \\(p\\)-value obtained by doing this after discarding zeros is \\(0.73\\), which is pretty much the same as the more complicated approximation."
  },
  {
    "objectID": "example/nonparametric.html#study-2-online-vs-in-person-meetings",
    "href": "example/nonparametric.html#study-2-online-vs-in-person-meetings",
    "title": "Nonparametric tests",
    "section": "Study 2: Online vs in-person meetings",
    "text": "Study 2: Online vs in-person meetings\nBrucks & Levav (2022) measure the attention of participants based on condition using an eyetracker.\nWe compare here the time spend looking at the partner by experimental condition (face-to-face or videoconferencing). The authors used a Kruskal–Wallis test, but this is equivalent to Wilcoxon’s rank-sum test.\n\n\nCode\ndata(BL22_E, package = \"hecedsm\")\nmww &lt;- coin::wilcox_test(\n  partner_time ~ cond, \n  data = BL22_E, \n  conf.int = TRUE)\nwelch &lt;- t.test(partner_time ~ cond, \n  data = BL22_E, \n  conf.int = TRUE)\nmww\n\n\n\n    Asymptotic Wilcoxon-Mann-Whitney Test\n\ndata:  partner_time by cond (f2f, video)\nZ = -6.4637, p-value = 1.022e-10\nalternative hypothesis: true mu is not equal to 0\n95 percent confidence interval:\n -50.694 -25.908\nsample estimates:\ndifference in location \n               -37.808 \n\n\nThe output of the test includes, in addition to the \\(p\\)-value for the null hypothesis that both median time are the same, a confidence interval for the time difference (in seconds). This is obtained by computing all average of pairwise differences between observations of the two groups, so-called Walsh’s averages. The Hodges–Lehmann estimate of location (the median of Walsh’s differences) is \\(-37.81\\) seconds, with a 95% confidence interval for the difference of \\([-50.69, -25.91]\\) seconds.\nThese can be compared with the usual Welch’s two-sample \\(t\\)-test with unequal variance. The estimated mean difference is \\(-39.69\\) seconds for face-to-face vs group video, with a 95% confidence interval of \\([-52.93, -26.45]\\).\nIn either case, it’s clear that the videoconferencing translates into longer time spent gazing at the partner than in-person meetings."
  },
  {
    "objectID": "example/mixedmodel.html",
    "href": "example/mixedmodel.html",
    "title": "Linear mixed models",
    "section": "",
    "text": "The following examples serves to illustrate the structure of experimental designs with both crossed and nested effects. The models are fitted specifying both random and fixed effects. We use the R package lme4 to estimate the coefficients of the resulting linear mixed effect model and lmerTest for additional testing features.\nA word of caution: correctly formulating mixed models for complex experimental designs is far from trivial, and the correct interpretation of the output requires subject-matter expertise. Thus, rather than see the following sections as a definitive guide to doing such calculations on your own, it should be viewed as a way to acquire some terminology and intuition for speaking with a statistical consultant and formulating your needs, and gain familiarity for peer-reviewing purposes."
  },
  {
    "objectID": "example/mixedmodel.html#determining-model-structure",
    "href": "example/mixedmodel.html#determining-model-structure",
    "title": "Linear mixed models",
    "section": "Determining model structure",
    "text": "Determining model structure\nBased on this description alone, it is somewhat unclear what the structure of the experiment is. We can inspect the database.\n\n\nCode\nlibrary(lmerTest)\ndata(TVbo)\nstr(TVbo)\n\n\n'data.frame':   192 obs. of  19 variables:\n $ Assessor            : Factor w/ 8 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ TVset               : Factor w/ 3 levels \"TV1\",\"TV2\",\"TV3\": 3 2 1 3 2 1 3 2 1 3 ...\n $ Repeat              : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 2 2 1 1 1 2 ...\n $ Picture             : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 2 2 2 2 ...\n $ Coloursaturation    : num  10.4 9.9 7 9.8 10.6 7.5 7.1 9.9 5 10 ...\n $ Colourbalance       : num  5 4.1 9.8 4.8 4.3 9.4 8 7.5 6.4 7.5 ...\n $ Noise               : num  13.1 10.9 13.1 13.2 13.3 13.3 12 8.7 12.8 13.4 ...\n $ Depth               : num  3.1 7.4 6 5.7 6.7 5.8 8.2 6.3 3.9 8.3 ...\n $ Sharpness           : num  8.3 5.3 7.9 9.3 4.7 6.6 10.7 7.5 6 8.8 ...\n $ Lightlevel          : num  9.5 9.9 6.9 9.8 10.3 6.7 10.3 9.5 5.8 9.2 ...\n $ Contrast            : num  6.4 6.9 5.9 6.3 7.5 6.4 9.1 7 4.1 9.8 ...\n $ Sharpnessofmovement : num  11.6 11.6 11.5 8.1 8.1 8.1 4 8.1 5.1 4.5 ...\n $ Flickeringstationary: num  13.5 13.5 13.5 12.1 8.8 13.2 12.1 5.6 11.8 13.5 ...\n $ Flickeringmovement  : num  2 1.8 1.7 13.5 13.5 13.5 1.9 1.8 1.7 13.5 ...\n $ Distortion          : num  1.8 1.9 1.9 1.5 1.5 1.5 1.8 2 1.9 1.4 ...\n $ Dimglasseffect      : num  1.6 1.6 1.6 1.7 1.7 1.7 1.6 1.6 1.5 1.7 ...\n $ Cutting             : num  1.5 2.2 3.3 1.4 3.9 3.7 1.5 2.7 4.1 1.6 ...\n $ Flossyedges         : num  3.4 3.3 5.4 7.9 4.4 4.6 9.9 3.1 4.5 11.6 ...\n $ Elasticeffect       : num  4.9 2.7 1.7 4.4 2.8 1.7 4.2 2.3 1.7 4.9 ...\n\n\nWe have three factors for the model: Assessor, TVset and Picture. There are seemingly two repetitions for ratings based on the factor level. Let us investigate the two manipulated factors, TVset and Picture, and look at the number of occurences for each combination\n\n\nCode\nxtabs(~ TVset + Picture, data = TVbo)\n\n\n     Picture\nTVset  1  2  3  4\n  TV1 16 16 16 16\n  TV2 16 16 16 16\n  TV3 16 16 16 16\n\n\nThere are 16 measurements for each combination, something compatible with a repeated measure design in which each of the eight assessor gets to see all factors. Thus, Picture and TVset are crossed within-subject factors and the dataset is balanced. The contingency table shows that we can estimate interactions between TVset and Picture. Since we are not interested in individual assessors scores, we treat the factor as a random effect and any interaction with Assessor will also be treated as random.\nGiven that we have two replications per combination of TVset and Picture, we could in theory fit the three-way (random) effect by including the term (1 | Assessor:TVset:Picture). If we do so, the software gives a warning: boundary (singular) fit which indicates an estimated variance is exactly zero, a synonym in this example for overfitting. We thus remove the three-way interaction. However, the numerical routine fails to converge, showing zero estimated variance for Assessor:Picture. Thus, we refit the model without the three-way interaction (since it does not make sense to have a three-way interaction, but remove one of the two-way), and later refit another model without Assessor:Picture whose variability is still estimated to be zero. Sometimes, this may be due to poor starting value and we may try to change the default optimization algorithm, the problem persist."
  },
  {
    "objectID": "example/mixedmodel.html#model-formulation",
    "href": "example/mixedmodel.html#model-formulation",
    "title": "Linear mixed models",
    "section": "Model formulation",
    "text": "Model formulation\nSince there are 15 different response variables, we start with a single one, here Sharpness of the display. The model we fit is of the form\n\\[\\begin{align*}\nY_{ijkl} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\gamma_k + (\\beta\\gamma)_{jk} + \\varepsilon_{ijkl},\n\\end{align*}\\] where\n\n\\(Y_{ijkl}\\) is the Sharpness rating for the \\(i\\)th TVset, the \\(j\\)th Picture, the \\(k\\)th assessor and the \\(l\\)th replication.\n\\(\\mu\\) is the global average of all ratings\n\\(\\alpha_i\\) is the mean different for the \\(i\\)th TVset\n\\(\\beta_j\\) is the mean different for the \\(j\\)th Picture\n\\((\\alpha\\beta)_{ij}\\) is the interaction for the \\(i\\)th TVset and \\(j\\)th Picture.\n\\(\\gamma_k \\sim \\mathsf{No}(0, \\sigma^2_\\gamma)\\) is the random effect of assessors\n\\((\\alpha\\gamma)_{ik} \\sim \\mathsf{No}(0, \\sigma^2_{\\alpha\\gamma})\\) are random effects for assessors for television sets\n\\(\\varepsilon_{ijkl} \\sim \\mathsf{No}(0, \\sigma^2_\\varepsilon)\\) are measurements-specific error terms\n\nAll of the random effects, including error, are independent of one another.\n\n\nCode\nmmod &lt;- lmer(\n  Sharpness ~ TVset * Picture + \n       (1 | Assessor) + \n       # (1 | Assessor:Picture) +\n       # (1 | Assessor:TVset:Picture) +\n       (1 | Assessor:TVset) ,\n  # control = lmerControl(optimizer = \"bobyqa\"),\n  ## 'control' above for the optimization routine\n  data = TVbo)\n# summary(mmod)"
  },
  {
    "objectID": "example/mixedmodel.html#correlation-structure",
    "href": "example/mixedmodel.html#correlation-structure",
    "title": "Linear mixed models",
    "section": "Correlation structure",
    "text": "Correlation structure\nThe first section of the summary table gives estimations of the standard deviation of each error term. There is little variability due to Assessor:Picture interaction. The most important part of this output is perhaps to look at the number of instances to make sure our design specification is correct. The model assumes (correctly) that there are 32 combinations of Assessor and Picture (8 by 4), 24 combinations of Assessor and TVset (8 by 3) and 8 combinations for Assessor.\nRandom effects:\n Groups         Name        Variance Std.Dev.\n Assessor:TVset (Intercept) 0.4909   0.7006  \n Assessor       (Intercept) 1.0837   1.0410  \n Residual                   1.9366   1.3916  \nNumber of obs: 192, groups:  \nAssessor:TVset, 24; Assessor, 8\nOne of the main reason for using mixed models is to automatically account for the correlation between observations. If we fail to account for the dependence between ratings from the same assessor and their personal preferences, we will get standard errors for the estimated mean difference for the two fixed effect that are too small. Another popular option in finance and economics is use of so-called sandwich variance estimators to handle unequal variance and clustering. Among other, the sandwich package in R (Zeileis et al., 2020) offers multiple options should one decide on this avenue.\nFigure 1 shows the structure of the correlation matrix of the ratings: the blocks corresponds to the eight assessors: measurements from different assessors are independent, but they are correlated within assessors as a result of specifying (multiple) random effects with assessors. Since there is more than one source of variability here, the pattern is different from equicorrelation, with additional correlations between if data are from the same TV set or same picture level.\n\n\n\n\n\nFigure 1: Correlation structure for the linear mixed model fitted to the TVbo data"
  },
  {
    "objectID": "example/mixedmodel.html#testing",
    "href": "example/mixedmodel.html#testing",
    "title": "Linear mixed models",
    "section": "Testing",
    "text": "Testing\nBefore looking more in details at the output, let us see if the rating depends on the setting (TVset and Picture combination): if it did, we would have to basically compare all 12 combinations as a one-way analysis of variance model. In this context, the logical comparison would be to compute pairwise differences and check which are equivalent to the best combination.\n\n\nCode\nanova(mmod, ddf = \"Kenward-Roger\")\n\n\nType III Analysis of Variance Table with Kenward-Roger's method\n               Sum Sq Mean Sq NumDF DenDF F value    Pr(&gt;F)    \nTVset          90.869  45.435     2    14 23.4615 3.384e-05 ***\nPicture        26.304   8.768     3   159  4.5276  0.004478 ** \nTVset:Picture 105.183  17.531     6   159  9.0524 1.634e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIt is clear that there are differences all across the board. Constructing tests for mixed models in general is complicated and we need to correctly set up the model and compare the variability of the differences relative to the mean for each factor relative to different measures of variability. These will be the next random term that includes this factor and random effects (here either Assessor or the Error term). The correct source of variability are Assessor:TVset for TVset, Assessor:Picture for Picture and the error term for the two-way interaction between both fixed effects. Likewise, the degrees of freedom for the denominator are determined by the number of instances of the random term that involves the factor, so \\(8 \\times 3 - (1 - 2 - 7) = 14\\) degrees of freedom since there are 24 instances of TVset:Assessor and we have to estimate one grand mean, two mean differences for TVset and likewise 7 effects for the Assessor.\nThis decomposition is better seen looking directly at the model output when fitted using aov (since this is a balanced within-subject design).\n\n\nCode\nfm &lt;- aov(\n  Coloursaturation ~ TVset * Picture +\n    Error(Assessor/(Picture*TVset)), \n  data = TVbo)"
  },
  {
    "objectID": "example/mixedmodel.html#pairwise-comparisons",
    "href": "example/mixedmodel.html#pairwise-comparisons",
    "title": "Linear mixed models",
    "section": "Pairwise comparisons",
    "text": "Pairwise comparisons\nWe can next look at the estimated marginal means to find the television sets and the picture with the best ratings.\n\n\nCode\nlibrary(emmeans)\nemm &lt;- mmod |&gt; \n  emmeans(spec = c(\"TVset\",\"Picture\"))\n\n\nAll four best ratings are for TV set 2, with picture 4, 1, 2 and 3 (in decreasing order). We can see if these differences are significant by looking at all pairwise differences between combinations to find the best value. All of the differences in rating for TV set 2 could be attributed to noise after correcting for multiple testing using Tukey’s correction. The fifth top position is TV set 1, picture 4 and this one has a significantly lower rating (adjusted \\(p\\)-value of 0.0088) than the leading choice.\n\n\nCode\nemm\n\n\n TVset Picture emmean    SE   df lower.CL upper.CL\n TV1   1         8.01 0.564 23.7     6.85     9.18\n TV2   1         6.92 0.564 23.7     5.75     8.08\n TV3   1        10.38 0.564 23.7     9.22    11.55\n TV1   2         6.97 0.564 23.7     5.81     8.14\n TV2   2         8.18 0.564 23.7     7.02     9.35\n TV3   2        11.39 0.564 23.7    10.23    12.56\n TV1   3         8.27 0.564 23.7     7.10     9.43\n TV2   3         7.57 0.564 23.7     6.40     8.73\n TV3   3         8.30 0.564 23.7     7.14     9.46\n TV1   4         8.82 0.564 23.7     7.65     9.98\n TV2   4         7.25 0.564 23.7     6.09     8.41\n TV3   4        10.91 0.564 23.7     9.74    12.07\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\nCode\nemm |&gt; \n  pairs()\n\n\n contrast                    estimate    SE    df t.ratio p.value\n TV1 Picture1 - TV2 Picture1   1.0938 0.604  51.1   1.811  0.8045\n TV1 Picture1 - TV3 Picture1  -2.3687 0.604  51.1  -3.922  0.0126\n TV1 Picture1 - TV1 Picture2   1.0375 0.492 159.0   2.109  0.6174\n TV1 Picture1 - TV2 Picture2  -0.1688 0.604  51.1  -0.279  1.0000\n TV1 Picture1 - TV3 Picture2  -3.3813 0.604  51.1  -5.598  0.0001\n TV1 Picture1 - TV1 Picture3  -0.2562 0.492 159.0  -0.521  1.0000\n TV1 Picture1 - TV2 Picture3   0.4437 0.604  51.1   0.735  0.9998\n TV1 Picture1 - TV3 Picture3  -0.2875 0.604  51.1  -0.476  1.0000\n TV1 Picture1 - TV1 Picture4  -0.8063 0.492 159.0  -1.639  0.8920\n TV1 Picture1 - TV2 Picture4   0.7625 0.604  51.1   1.262  0.9803\n TV1 Picture1 - TV3 Picture4  -2.8937 0.604  51.1  -4.791  0.0008\n TV2 Picture1 - TV3 Picture1  -3.4625 0.604  51.1  -5.733  &lt;.0001\n TV2 Picture1 - TV1 Picture2  -0.0563 0.604  51.1  -0.093  1.0000\n TV2 Picture1 - TV2 Picture2  -1.2625 0.492 159.0  -2.566  0.3081\n TV2 Picture1 - TV3 Picture2  -4.4750 0.604  51.1  -7.409  &lt;.0001\n TV2 Picture1 - TV1 Picture3  -1.3500 0.604  51.1  -2.235  0.5333\n TV2 Picture1 - TV2 Picture3  -0.6500 0.492 159.0  -1.321  0.9752\n TV2 Picture1 - TV3 Picture3  -1.3813 0.604  51.1  -2.287  0.4985\n TV2 Picture1 - TV1 Picture4  -1.9000 0.604  51.1  -3.146  0.0993\n TV2 Picture1 - TV2 Picture4  -0.3312 0.492 159.0  -0.673  0.9999\n TV2 Picture1 - TV3 Picture4  -3.9875 0.604  51.1  -6.602  &lt;.0001\n TV3 Picture1 - TV1 Picture2   3.4062 0.604  51.1   5.640  &lt;.0001\n TV3 Picture1 - TV2 Picture2   2.2000 0.604  51.1   3.643  0.0279\n TV3 Picture1 - TV3 Picture2  -1.0125 0.492 159.0  -2.058  0.6531\n TV3 Picture1 - TV1 Picture3   2.1125 0.604  51.1   3.498  0.0412\n TV3 Picture1 - TV2 Picture3   2.8125 0.604  51.1   4.657  0.0013\n TV3 Picture1 - TV3 Picture3   2.0812 0.492 159.0   4.230  0.0023\n TV3 Picture1 - TV1 Picture4   1.5625 0.604  51.1   2.587  0.3137\n TV3 Picture1 - TV2 Picture4   3.1313 0.604  51.1   5.184  0.0002\n TV3 Picture1 - TV3 Picture4  -0.5250 0.492 159.0  -1.067  0.9956\n TV1 Picture2 - TV2 Picture2  -1.2063 0.604  51.1  -1.997  0.6930\n TV1 Picture2 - TV3 Picture2  -4.4188 0.604  51.1  -7.316  &lt;.0001\n TV1 Picture2 - TV1 Picture3  -1.2937 0.492 159.0  -2.630  0.2725\n TV1 Picture2 - TV2 Picture3  -0.5938 0.604  51.1  -0.983  0.9975\n TV1 Picture2 - TV3 Picture3  -1.3250 0.604  51.1  -2.194  0.5614\n TV1 Picture2 - TV1 Picture4  -1.8438 0.492 159.0  -3.747  0.0129\n TV1 Picture2 - TV2 Picture4  -0.2750 0.604  51.1  -0.455  1.0000\n TV1 Picture2 - TV3 Picture4  -3.9312 0.604  51.1  -6.509  &lt;.0001\n TV2 Picture2 - TV3 Picture2  -3.2125 0.604  51.1  -5.319  0.0001\n TV2 Picture2 - TV1 Picture3  -0.0875 0.604  51.1  -0.145  1.0000\n TV2 Picture2 - TV2 Picture3   0.6125 0.492 159.0   1.245  0.9843\n TV2 Picture2 - TV3 Picture3  -0.1187 0.604  51.1  -0.197  1.0000\n TV2 Picture2 - TV1 Picture4  -0.6375 0.604  51.1  -1.056  0.9953\n TV2 Picture2 - TV2 Picture4   0.9313 0.492 159.0   1.893  0.7620\n TV2 Picture2 - TV3 Picture4  -2.7250 0.604  51.1  -4.512  0.0020\n TV3 Picture2 - TV1 Picture3   3.1250 0.604  51.1   5.174  0.0002\n TV3 Picture2 - TV2 Picture3   3.8250 0.604  51.1   6.333  &lt;.0001\n TV3 Picture2 - TV3 Picture3   3.0938 0.492 159.0   6.288  &lt;.0001\n TV3 Picture2 - TV1 Picture4   2.5750 0.604  51.1   4.263  0.0045\n TV3 Picture2 - TV2 Picture4   4.1437 0.604  51.1   6.861  &lt;.0001\n TV3 Picture2 - TV3 Picture4   0.4875 0.492 159.0   0.991  0.9977\n TV1 Picture3 - TV2 Picture3   0.7000 0.604  51.1   1.159  0.9898\n TV1 Picture3 - TV3 Picture3  -0.0312 0.604  51.1  -0.052  1.0000\n TV1 Picture3 - TV1 Picture4  -0.5500 0.492 159.0  -1.118  0.9935\n TV1 Picture3 - TV2 Picture4   1.0188 0.604  51.1   1.687  0.8658\n TV1 Picture3 - TV3 Picture4  -2.6375 0.604  51.1  -4.367  0.0032\n TV2 Picture3 - TV3 Picture3  -0.7312 0.604  51.1  -1.211  0.9857\n TV2 Picture3 - TV1 Picture4  -1.2500 0.604  51.1  -2.070  0.6454\n TV2 Picture3 - TV2 Picture4   0.3187 0.492 159.0   0.648  1.0000\n TV2 Picture3 - TV3 Picture4  -3.3375 0.604  51.1  -5.526  0.0001\n TV3 Picture3 - TV1 Picture4  -0.5188 0.604  51.1  -0.859  0.9992\n TV3 Picture3 - TV2 Picture4   1.0500 0.604  51.1   1.738  0.8418\n TV3 Picture3 - TV3 Picture4  -2.6063 0.492 159.0  -5.297  &lt;.0001\n TV1 Picture4 - TV2 Picture4   1.5688 0.604  51.1   2.597  0.3081\n TV1 Picture4 - TV3 Picture4  -2.0875 0.604  51.1  -3.456  0.0459\n TV2 Picture4 - TV3 Picture4  -3.6562 0.604  51.1  -6.054  &lt;.0001\n\nDegrees-of-freedom method: kenward-roger \nP value adjustment: tukey method for comparing a family of 12 estimates"
  },
  {
    "objectID": "example/mixedmodel.html#checking-model-assumptions",
    "href": "example/mixedmodel.html#checking-model-assumptions",
    "title": "Linear mixed models",
    "section": "Checking model assumptions",
    "text": "Checking model assumptions\nWhen we set up random effect terms, these are specified as being normal random variables. This has a shrinkage effect: predictions of the random effect model will tend to be closer to the mean than corresponding fixed, more so for larger values. In linear mixed models, we can retrieve predictions of random effects and plot them against theoretical normal plotting positions to see if they are in line with the assumption (as we did for residuals).\n\n\nCode\nlibrary(lattice) # plots\n# plot residuals against fitted value (additivity / linearity)\nplot(mmod)\n# Normal quantile-quantile plot of residuals\nqqmath(mmod)\n# Normal quantile-quantile plots of random effect predictions\nrandom &lt;- ranef(mmod)\nplot(random)\n\n\n\n\n\n\n\nAs with other quantile-quantile plots, we expect the values to fall in line, which they seem to do here. There is one residual corresponding to a rating much below the average, but nothing to worry given the large sample size."
  },
  {
    "objectID": "example/mixedmodel.html#model-structure",
    "href": "example/mixedmodel.html#model-structure",
    "title": "Linear mixed models",
    "section": "Model structure",
    "text": "Model structure\n\n\nCode\n# Load packages\nlibrary(lmerTest)\nlibrary(emmeans)\nlibrary(ggplot2)\n# Set up sum-to-zero constraint for factors\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\ndata(C22, package = \"hecedsm\") # Load data\nstr(C22)\n\n\ntibble [256 × 7] (S3: tbl_df/tbl/data.frame)\n $ anchor     : Factor w/ 2 levels \"weak-first\",\"strong-first\": 2 2 2 2 2 2 2 2 2 2 ...\n $ vignette   : Factor w/ 2 levels \"1\",\"2\": 1 1 1 1 1 1 1 1 1 1 ...\n $ id         : Factor w/ 128 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ vorder     : Factor w/ 2 levels \"1\",\"2\": 1 1 1 1 1 1 1 1 1 1 ...\n $ verdictsyst: Factor w/ 2 levels \"two\",\"three\": 1 1 1 1 1 1 1 1 2 2 ...\n $ guilt      : num [1:256] 103.5 81.8 87.8 82.7 76.8 ...\n $ prior      : num [1:256] 67 101 65 60 75 73 66 86 59 93 ...\n\n\nCode\n# How are conditions manipulated?\nwith(C22, xtabs(~ vignette + id))\n\n\n        id\nvignette 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n       1 1 1 1 1 1 1 1 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n       2 1 1 1 1 1 1 1 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n        id\nvignette 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n       1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n       2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n        id\nvignette 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n       1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n       2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n        id\nvignette 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n       1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n       2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n        id\nvignette 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113\n       1  1  1  1  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n       2  1  1  1  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n        id\nvignette 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128\n       1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n       2   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n\n\nCode\nwith(C22, xtabs(~ vignette + anchor))\n\n\n        anchor\nvignette weak-first strong-first\n       1         64           64\n       2         64           64\n\n\nCode\nwith(C22, xtabs(~ anchor + id))\n\n\n              id\nanchor         1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n  weak-first   1 1 1 1 1 1 1 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n  strong-first 1 1 1 1 1 1 1 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n              id\nanchor         25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45\n  weak-first    1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n  strong-first  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n              id\nanchor         46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66\n  weak-first    1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n  strong-first  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n              id\nanchor         67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87\n  weak-first    1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n  strong-first  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n              id\nanchor         88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106\n  weak-first    1  1  1  1  1  1  1  1  1  1  1  1   1   1   1   1   1   1   1\n  strong-first  1  1  1  1  1  1  1  1  1  1  1  1   1   1   1   1   1   1   1\n              id\nanchor         107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122\n  weak-first     1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n  strong-first   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n              id\nanchor         123 124 125 126 127 128\n  weak-first     1   1   1   1   1   1\n  strong-first   1   1   1   1   1   1\n\n\nFor each individual we see each pair of anchor once, but people are assigned to a single vignette. With only two measurements, no interaction is possible within participants. Likewise, we cannot have interaction between subject-effect and with anchor and verdict system (why?)\nWe thus include a single random effect, that for subject. Curley et al. (2022) fitted two models: one with all three way interactions between PJAQ (prior), vignette and anchor, the other with additive effects.\nSince PJAQ is a continuous covariate, the interaction with vignette and anchor means that we expect the slope for PJAQ to be different for each group. This isn’t necessarily justified by any of the author’s hypotheses. If any interaction with the total pre-trial bias (PJAQ) is present, we would need to perform comparisons for different values of PJAQ."
  },
  {
    "objectID": "example/mixedmodel.html#model-fitting-and-tests",
    "href": "example/mixedmodel.html#model-fitting-and-tests",
    "title": "Linear mixed models",
    "section": "Model fitting and tests",
    "text": "Model fitting and tests\nWe fit the model without the interaction with PJAQ. Note that, using emmeans, it is better to center the covariate so that it’s average is zero, in which case comparisons are performed for this average value.\n\n\nCode\nC22_c &lt;- C22 |&gt;\n  dplyr::mutate(pjaq = scale(prior, scale = FALSE),\n                guilt = guilt / 10) \n# scale from 0 to 14\nmodel1 &lt;- lmer(\n  guilt ~ anchor*vignette*pjaq + (1 | id),\n  data = C22_c)\nmodel2 &lt;- lmer(\n  guilt ~ anchor*vignette + pjaq + (1 | id),\n  data = C22_c)\n\n\nNow that we have model coefficients, we can test the models. First, we check if the slope is the same for each of the four subgroups\n\n\nCode\n# Type III ANOVA from lmerTest\nanova(model1, model2)\n\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: C22_c\nModels:\nmodel2: guilt ~ anchor * vignette + pjaq + (1 | id)\nmodel1: guilt ~ anchor * vignette * pjaq + (1 | id)\n       npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmodel2    7 1114.4 1139.2 -550.22   1100.4                     \nmodel1   10 1119.5 1155.0 -549.77   1099.5 0.8908  3     0.8276\n\n\nThere appear to be no two-way or three-way interactions between any of the two factors and the covariate.\nWe can create an interaction plot to see what the estimated marginal means are for each of the four subconditions when the total pre-trial bias score is average.\n\n\nCode\nlibrary(ggplot2)\n# Interaction plot\nemmip(model2, \n      formula = anchor ~ vignette, \n      CIs = TRUE) +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nThere seems to be differences, but note the very large uncertainty (and that of difference would be larger than for the individual means). We can inspect the estimated parameters for the model coefficient and compute marginal means.\n\n\nCode\nsummary(model2)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: guilt ~ anchor * vignette + pjaq + (1 | id)\n   Data: C22_c\n\nREML criterion at convergence: 1115.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.98159 -0.47839  0.02203  0.48593  2.76710 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept) 3.472    1.863   \n Residual             2.129    1.459   \nNumber of obs: 256, groups:  id, 128\n\nFixed effects:\n                   Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)         8.18232    0.18826 125.00000  43.462  &lt; 2e-16 ***\nanchor1             0.25008    0.09119 126.00000   2.743  0.00699 ** \nvignette1           0.29655    0.09119 126.00000   3.252  0.00147 ** \npjaq                0.08231    0.01938 125.00000   4.247  4.2e-05 ***\nanchor1:vignette1   0.02823    0.18935 125.00000   0.149  0.88173    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) anchr1 vgntt1 pjaq  \nanchor1      0.000                     \nvignette1    0.000  0.000              \npjaq         0.000  0.000  0.000       \nanchr1:vgn1  0.000  0.000  0.000 -0.107\n\n\nCode\nemmeans(model2, spec = \"anchor\") |&gt;\n  contrast(method = \"pairwise\")\n\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n contrast                      estimate    SE  df t.ratio p.value\n (weak-first) - (strong-first)      0.5 0.182 126   2.743  0.0070\n\nResults are averaged over the levels of: vignette \nDegrees-of-freedom method: kenward-roger \n\n\nThere is a strong correlation, 0.59 between the responses from the same individuals so failing to account for the subject-specific random effect would lead to misleading conclusions, as reported \\(p\\)-values would be much smaller than they should be.\nThe correlation between pre-trial bias score (PJAQ) and the guilt score is 0.32. The coefficient in the additive model is 0.08 with a standard error of 0.19, indicating a strong positive effect of pre-trial bias on the guilt score.\nThe anchor has a large impact, with the estimated difference weak-first vs strong-first of 5 points for the guilt score overall."
  },
  {
    "objectID": "example/mixedmodel.html#model-structure-1",
    "href": "example/mixedmodel.html#model-structure-1",
    "title": "Linear mixed models",
    "section": "Model structure",
    "text": "Model structure\nThe id variable is a dummy for the participant, with associated covariate sex and bmi. We also have the three within-subject factors, position, phys_demand and task_diff and the order in which the tasks were given (counterbalanced). There are in total seven response variables: the inverse efficiency score global stimulus (ies), measures of brain activity (central_alpha, parietal_alpha, central_beta, parietal_beta) and two scales obtained from answers to a questionaire, attention and satisfaction.\nThe three manipulated factors are nested within subject and crossed, so we can estimate the three-way and two-way interactions with the experimental factors. The only logical random effect here is for subject, and we cannot have further sources of variability given the lack of replication.\nThe authors are not transparent as to what their model is and earns a failure grade for reproducibility: we have no idea of the specification, coefficients are not reported. There is clearly 8 coefficiens corresponding to the average of the subgroups, plus a random effect for subject and sex and body mass index as covariates. It appears from the output of Table 1 that there was an interaction term added for BMI and position (as standing may be more strenuous on overweight participants). Finally, we include the order of the tasks as a covariate to account for potential fatigue effects."
  },
  {
    "objectID": "example/mixedmodel.html#model-adjustment-and-testing",
    "href": "example/mixedmodel.html#model-adjustment-and-testing",
    "title": "Linear mixed models",
    "section": "Model adjustment and testing",
    "text": "Model adjustment and testing\nOur linear mixed model would take the form, here with ies as response\n\n\nCode\nlibrary(lmerTest)\nmod &lt;- lmer(ies ~ position*phys_demand*task_diff +\n  (1 | id) + sex + bmi*position + order, \n           data = LJLSFBM20) \n\n\nGiven the number of response variables and coefficients, it is clear one must account for testing multiplicity. Figuring out the size of the family, \\(m\\), is not trivial: testing interactions and main effects leads to 7 tests. We could also be interested in the interaction between body mass index and position if it affects performance, bringing the number of tests to 8 and throw the covariate effect for order (up to 11 if we include all controls). Further contrasts may inflate this number (for example, there are 28 pairwise comparisons if the three-way interaction is significant). With seven responses and linear mixed models parametrized in the same way, we have at least 56 tests and potentially up to 350! Controlling the type-I error requires using Bonferroni–Holm type correction, since the different responses are not part of the same models.\n\n\nCode\nanova(mod)\n\n\nType III Analysis of Variance Table with Satterthwaite's method\n                                   Sum Sq    Mean Sq NumDF DenDF  F value\nposition                           574983     574983     1   250   0.0361\nphys_demand                    1280635446 1280635446     1   250  80.3464\ntask_diff                      4927991208 4927991208     1   250 309.1796\nsex                              79185574   79185574     1    34   4.9681\nbmi                              10365621   10365621     1    34   0.6503\norder                           105580125  105580125     1   250   6.6240\nposition:phys_demand              9127601    9127601     1   250   0.5727\nposition:task_diff                1825943    1825943     1   250   0.1146\nphys_demand:task_diff           327914741  327914741     1   250  20.5732\nposition:bmi                      1224778    1224778     1   250   0.0768\nposition:phys_demand:task_diff    2299196    2299196     1   250   0.1443\n                                  Pr(&gt;F)    \nposition                         0.84952    \nphys_demand                    &lt; 2.2e-16 ***\ntask_diff                      &lt; 2.2e-16 ***\nsex                              0.03254 *  \nbmi                              0.42560    \norder                            0.01064 *  \nposition:phys_demand             0.44992    \nposition:task_diff               0.73530    \nphys_demand:task_diff          8.913e-06 ***\nposition:bmi                     0.78185    \nposition:phys_demand:task_diff   0.70441    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIf we focus on the sole results for ies, there is a significant two-way interaction (and main effects) for phys_demand and task_diff, but not for position. Further investigation would reveal better performance on the easy task and overall with a mouse.\n\n\nCode\nemmeans::emmeans(mod, spec = c(\"phys_demand\",\"task_diff\"))\n\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n phys_demand task_diff emmean  SE   df lower.CL upper.CL\n mouse       easy        2668 672 79.3     1332     4005\n touchpad    easy        4723 672 79.2     3387     6060\n mouse       difficult   8733 672 79.3     7396    10070\n touchpad    difficult  14998 672 79.2    13662    16335\n\nResults are averaged over the levels of: position, sex \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95"
  },
  {
    "objectID": "example/mixedmodel.html#model-assumptions",
    "href": "example/mixedmodel.html#model-assumptions",
    "title": "Linear mixed models",
    "section": "Model assumptions",
    "text": "Model assumptions\nOne may wonder whether there are any effect of spillover, learning or fatigue due to the repeated measures: tasks were much longer than is typical. The coefficient for order is -268.14, so the decrease for the inverse efficiency score global stimulus for each additional task, ceteris paribus, with a \\(p\\)-value of 0.011 suggesting participants improve over time.\n\n\nCode\nlibrary(ggplot2)\ng1 &lt;- ggplot(\n  data = data.frame(residuals = resid(mod), \n                    id = LJLSFBM20$id),\n  mapping = aes(x = id, \n                y = residuals)) + \n  geom_point() +\n  theme_classic()\ng2 &lt;- ggplot(\n  data = data.frame(residuals = resid(mod), \n                    order = LJLSFBM20$order),\n  mapping = aes(x = order, \n                y = residuals)) + \n  geom_point() +\n  theme_classic()\nlibrary(patchwork) # combine results\ng1 + g2\n\n\n\n\n\nFigure 2: Scatterplots of residuals from the linear mixed model against participant identifier (left) and against order of task (right).\n\n\n\n\nLooking at the residuals of the model per participants is also quite insightful. It is clear that measurements from participants 30 and 31 are abnormal, and these correspond to the values we see.\nWe can check the other model assumptions: the quantile-quantile plot of random effects suggests some large outliers, unsurprisingly due to the participants identified. The plot of fitted values vs residuals suggests our model is wholly inadequate: there is a clear trend in the residuals and strong evidence of heterogeneity.\n\n\nCode\nplot(ranef(mod))\nplot(mod)\n\n\n\n\n\n\n\nFigure 3: Normal quantile-quantile plot of predicted random effects (left) and Tukey’s plot of residual vs fitted values (right).\n\n\n\n\nWe could fit a more complex model to tackle the heterogeneity issue by including different variance per individual, at the expense of lower power."
  },
  {
    "objectID": "example/linearmediation.html",
    "href": "example/linearmediation.html",
    "title": "Linear mediation model",
    "section": "",
    "text": "Download the R code and the SPSS code.\nConsider linear regression models in a system where some response variable \\(Y\\) depends on a mediator, \\(M\\), and an experimental factor, \\(X \\in \\{0,1\\}\\). The mediator model has expected value \\[\nM = \\underset{\\text{intercept}}{c_M} + \\alpha X + \\underset{\\text{control variates}}{\\boldsymbol{Z}_M^\\top\\boldsymbol{\\delta}} + \\underset{\\text{error term}}{\\varepsilon_M}\n\\tag{1}\\] while we have for the response \\[\nY = \\underset{\\text{intercept}}{c_Y} + \\underset{\\text{direct effect}}{\\beta X} + \\gamma M + \\underset{\\text{control variates}}{\\boldsymbol{Z}_Y^\\top\\boldsymbol{\\omega}}  + \\underset{\\text{error term}}{\\varepsilon_Y}, \\label{eq-responsemodel}\n\\tag{2}\\] The explanatory variables \\(\\boldsymbol{Z}_M\\) for the mediator model of Equation 1, and \\(\\boldsymbol{Z}_Y\\) for the response model Equation 2 are control covariates to account for confounders, and need not be the same in both models (but could be). In a randomized experiment where we manipulate the mediator, there is no need to include any control in Equation 1 lest we mistakenly create correlation by including colliders. The error terms \\(\\varepsilon_Y\\) and \\(\\varepsilon_M\\) are assumed to be independent mean-zero with common variance.\n\n\n\n\n\nFigure 1: Directed acyclic graph for the linear mediation model.\n\n\n\n\nIn the linear mediation model defined above and represented in Figure 1, the total effect of \\(X\\) on \\(Y\\) is \\(\\beta + \\alpha \\gamma\\) (the average treatment effect, looking at the difference in response between treatment \\(X=1\\) and control \\(X=0\\)).\nThe average causal mediation effect (ACME) or natural indirect effect for treatment at level \\(x\\) is obtained by changing the potential outcome \\(Y_i\\{X, M_i(X)\\}\\) of individual \\(i\\) for the mediator \\(M_i(X)\\) while fixing the treatment level \\(X\\) to \\(x\\), so \\[\\begin{align*}\n\\mathsf{ACME} = \\mathsf{E}[Y_i\\{x, M_i(1)\\} - Y_i\\{x, M_i(0)\\}].\n\\end{align*}\\] In the linear mediation model, we get subtituting the value of \\(M\\) from equation Equation 1 into Equation 2 \\[\\begin{align*}\n\\mathsf{E}[Y_i\\{x, M_i(1)\\}]  &= \\underset{\\text{intercept}}{c_Y + \\gamma c_M} + \\underset{\\text{ACME}}{\\alpha\\gamma} + \\beta x+ \\underset{\\text{effect of control variates}}{\\gamma\\boldsymbol{Z}_M^\\top\\boldsymbol{\\delta}  + \\boldsymbol{Z}_Y^\\top\\boldsymbol{\\omega}}\\\\\n\\mathsf{E}[Y_i\\{x, M_i(0)\\}]  &=\\underset{\\text{intercept}}{c_Y + \\gamma c_M} + \\beta x +\\underset{\\text{effect of control variates}}{\\gamma\\boldsymbol{Z}_M^\\top\\boldsymbol{\\delta}  + \\boldsymbol{Z}_Y^\\top\\boldsymbol{\\omega}}\\\\\n\\end{align*}\n\\] so the average difference between the two equations is \\(\\mathsf{ACME} = \\alpha\\gamma\\).\n\nExample 1\nWe consider an example from Experiment 1 of Bastian et al. (2014), who experienced the effect of the effect of shared pain (through a manipulation) on bonding.\n\nThis effect of pain remained when controlling for age (p = .048), gender (p = .052), and group size (p = .050). None of these variables were significantly correlated with experimental condition (ps &gt; .136) or perceived bonding (ps &gt; .925). To determine whether the marginal tendency for the pain tasks to be viewed as more threatening than the control tasks mediated the effect of pain on perceived bonding, we conducted a bootstrap analysis (Preacher & Hayes, 2008) using 5,000 resamples. The results of this analysis revealed that threat was not a significant mediator, indirect effect = −0.11, SE = 0.09, 95% CI = [−0.34, 0.03].\n\nThere are several problems with the description: while it seems that some covariate (age, gender, group size) were added to regression models, it is unclear whether they could be confounders, whether their effect is linear and in which (if any model they are included). Stating “bootstrap analysis” is the equivalent of “running a statistical test”: so vague it could mean anything, and the fact the output is random does not help with reproducibility.\nThe response variable \\(Y\\) is bonding, the experimental factor condition and threat, the average ALES subscale about the perception of the physical task, is the postulated mediator.\nWe use the mediation package (Tingley et al., 2014) for the model; the package certainly isn’t needed (nor the PROCESS macros) to run the bootstrap, which we could obtain with a single for-loop. However, it has utilities, notably for checking model assumptions, that are convenient.\n\n\nCode\nlibrary(mediation, quietly = TRUE)\ndata(BJF14_S1, package = \"hecedsm\")\n\nMsX &lt;- lm(threat ~ condition + gender + groupsize + age, \n          data = BJF14_S1)\nYsXM &lt;- lm(bonding ~ condition + threat + gender + groupsize + age,\n           data = BJF14_S1)\n\n\n\n\nTable 1: Coefficients of the regression models\n\n\n\n\n\n\n(a) outcome model\n\n\nterm\nestimate\nstd.error\n\n\n\n\n(Intercept)\n2.66\n1.47\n\n\ncondition [pain]\n0.66\n0.32\n\n\nthreat\n-0.24\n0.36\n\n\ngender [female]\n-0.02\n0.33\n\n\ngroup size\n0.02\n0.13\n\n\nage\n0.03\n0.07\n\n\n\n\n\n\n\n\n\n\n\n\n(b) mediation model\n\n\nterm\nestimate\nstd.error\n\n\n\n\n(Intercept)\n-0.16\n0.59\n\n\ncondition [pain]\n0.30\n0.12\n\n\ngender [female]\n-0.05\n0.13\n\n\ngroup size\n-0.07\n0.05\n\n\nage\n0.07\n0.02\n\n\n\n\n\n\n\n\n\n\nBoth of the threat and bonding measures are average of Likert scales. We include the controls in the regression for the response to account for potential confounding between threat level and shared bonding: it is unclear whether the authors used the control covariates or whether these make sense.\n\n\nCode\nset.seed(80667)\nlinmed &lt;- mediate(\n  model.m = MsX,\n  model.y = YsXM,\n  sims = 1000L, # number of bootstrap simulations\n  boot = TRUE, # use bootstrap\n  boot.ci.type = \"perc\", # type of bootstrap: percentile\n  mediator = \"threat\", # name of mediator\n  treat = \"condition\", # name of treatment\n  control.value = \"Control\", # name of control (level of 'condition')\n  treat.value = \"Pain\") # name of treatment (level of 'condition')\nsummary(linmed)\n\n\n\n\n\n\nTable 2: Linear causal mediation analysis: parameter estimates, nonparametric bootstrap 95% confidence intervals and p-values with the percentile method based on 5000 bootstrap samples.\n\n\n\nestimate\nlower 95% CI\nupper 95% CI\np-value\n\n\n\n\nACME\n-0.071\n-0.363\n0.146\n0.548\n\n\nADE\n0.660\n-0.012\n1.257\n0.056\n\n\ntotal effect\n0.588\n0.012\n1.145\n0.048\n\n\nprop. mediated\n-0.121\n-1.501\n0.615\n0.564\n\n\n\n\n\n\n\n\nThe first line of Table 2 gives the average reverse natural indirect effect or average conditional mediated effect (labelled ACME), the second the average direct effect (ADE) and the third the total effect (ADE + ACME). The point estimate for ACME, \\(\\alpha\\gamma\\) is \\(\\widehat{\\alpha}\\widehat{\\gamma} = -0.07\\). The bootstrap sampling distribution is skewed to the left, a fact reflected by the asymmetric percentile confidence interval.\n\n\n\n\n\nFigure 2: Density of the 1000 nonparametric bootstrap estimates of the average conditional mediation effect \\(\\alpha\\gamma\\), with point estimate and 95% percentile bootstrap confidence intervals (vertical dashed lines).\n\n\n\n\nThe sequential ignorability assumption cannot be verified, but we can see what impacts violations would have on the coefficients: the expected value of the coefficient \\(\\widehat{\\gamma}\\) is \\(\\gamma + \\mathsf{Cov}(\\varepsilon_M, \\varepsilon_Y)/\\mathsf{Va}(\\varepsilon_M)\\); the second component is a bias term that does not vanish, even when the sample size grows (Bullock et al., 2010). The variance of the error of the mediation and response models can be estimated, and we can vary the correlation coefficient, \\(\\rho=\\mathsf{Cor}(\\varepsilon_M, \\varepsilon_Y)\\), to assess the sensitivity of our conclusions if there was confounding.\n\n\nCode\nlinmed_sensitivity &lt;- medsens(linmed)\nsummary(linmed_sensitivity)\n\n\n\nMediation Sensitivity Analysis for Average Causal Mediation Effect\n\nSensitivity Region\n\n      Rho    ACME 95% CI Lower 95% CI Upper R^2_M*R^2_Y* R^2_M~R^2_Y~\n[1,] -0.4  0.2533      -0.0236       0.5302         0.16       0.1144\n[2,] -0.3  0.1626      -0.0723       0.3975         0.09       0.0644\n[3,] -0.2  0.0805      -0.1288       0.2898         0.04       0.0286\n[4,] -0.1  0.0034      -0.1969       0.2037         0.01       0.0072\n[5,]  0.0 -0.0714      -0.2788       0.1360         0.00       0.0000\n[6,]  0.1 -0.1462      -0.3748       0.0825         0.01       0.0072\n[7,]  0.2 -0.2233      -0.4850       0.0385         0.04       0.0286\n\nRho at which ACME = 0: -0.1\nR^2_M*R^2_Y* at which ACME = 0: 0.01\nR^2_M~R^2_Y~ at which ACME = 0: 0.0072 \n\n\nCode\nplot(linmed_sensitivity)\n\n\n\n\n\nThe medsens function implements the sensitivity diagnostic presented in Section 5.1 of Imai et al. (2010) for the linear mediation model. By default, the correlation \\(\\rho\\) varies in 0.1 increments. We can see the wide range of the ACME if there was correlation between residuals from the mediation and the response model, highlighting the wide range of values that could be returned: the ACME could go from \\(0.2\\) to \\(-0.288\\) for correlations in the range \\(\\rho \\in [-0.4, 0.4]\\). In this example, nearly any correlation in this range would lead to “insignificant results”, mostly because of the small sample size. In a situation where we had found a significant (sic) result, we could observe how much correlation btween would be needed for this effect to be an artefact of correlation and vanish.\nAccording to the documentation of the medsens function (?medsens), there are two variants of the estimated effect size, either computing the proportion of the total (tilde, R^2_M~R^2_Y) or residual (starred, R^2_M*R^2_Y*) variance from the mediation and outcome models that are due to hypothetical unobserved confounders.\n\n\n\n\n\n\n\nReferences\n\nBastian, B., Jetten, J., & Ferris, L. J. (2014). Pain as social glue: Shared pain increases cooperation. Psychological Science, 25(11), 2079–2085. https://doi.org/10.1177/0956797614545886\n\n\nBullock, J. G., Green, D. P., & Ha, S. E. (2010). Yes, but what’s the mechanism? (Don’t expect an easy answer). Journal of Personality and Social Psychology, 98(4), 550–558. https://doi.org/10.1037/a0018933\n\n\nImai, K., Keele, L., & Yamamoto, T. (2010). Identification, inference and sensitivity analysis for causal mediation effects. Statistical Science, 25(1), 51–71. https://doi.org/10.1214/10-STS321\n\n\nTingley, D., Yamamoto, T., Hirose, K., Keele, L., & Imai, K. (2014). mediation: R package for causal mediation analysis. Journal of Statistical Software, 59(5), 1–38. https://doi.org/10.18637/jss.v059.i05"
  },
  {
    "objectID": "example/installation.html",
    "href": "example/installation.html",
    "title": "Installing R and RStudio",
    "section": "",
    "text": "You will do all of your work in this class with the open source (and free!) R programming language. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code."
  },
  {
    "objectID": "example/installation.html#learning-r",
    "href": "example/installation.html#learning-r",
    "title": "Installing R and RStudio",
    "section": "Learning R",
    "text": "Learning R\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g., “rstats scatterplot”).\nCheck out StackOverflow (a Q&A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse)."
  },
  {
    "objectID": "example/effectsizepower.html",
    "href": "example/effectsizepower.html",
    "title": "Effect size and power",
    "section": "",
    "text": "This example is devoted to correct calculation and reporting of effect sizes across a variety of experimental designs, including multi-way analysis of variance with and without blocking factors. We make use of the comprehensive effectsize package throughout.1\nWe also showcase how effect sizes can be recovered from the output of an analysis of variance table or from the reported test statistics and degrees of freedom, thus highlighting the impact of accurately reporting the latter.\nWe finally consider calculation of power in various replication studies using WebPower and G*Power; the latter considers the setup from the Power exercise series.\n\nThere’s a set of videos that walks through each section below. To make it easier for you to jump around the video examples, I cut the long video into smaller pieces and included them all in one YouTube playlist.\n\nEffect size and power\nCalculations using G*Power\n\nYou can also watch the playlist (and skip around to different sections) here:\n\n\n\n\n\n\n\nEffect size typically serve three purpose:\n\ninform readers of the magnitude of the effect,\nprovide a standardized quantity that can be combined with others in a meta-analysis, or\nserve as proxy in a power study to estimate the minimum number of observations needed.\n\nIf you report the exact value of the test statistic, the null distribution and (in short) all elements of an analysis of variance table in a complex design, it is possible by using suitable formulae to recover effect sizes, as they are functions of the test statistic summaries, degrees of freedom and correlation between observations (in the case of repeated measures).\nThe effectsize package includes a variety of estimators for standardized difference or ratio of variance. For example, for the latter, we can retrieve Cohen’s \\(f\\) via cohens_f, \\(\\widehat{\\epsilon}^2\\) via epsilon_squared or \\(\\widehat{\\omega}^2\\) via omega_squared.\nBy default, in a design with more than one factor, the partial effects are returned (argument partial = TRUE) — if there is a single factor, these coincide with the total effects and the distinction is immaterial.\nThe effectsize package reports confidence intervals2 calculated using the pivot method described in Steiger (2004). Check the documentation at ?effectsize::effectsize_CIs for more technical details.3\nIn general, confidence intervals for effect sizes are very wide, including a large range of potential values and sometimes zero. This reflects the large uncertainty surrounding their estimation and should not be taken to mean that the estimated effect is null.\n\n\n\nWe begin with the result of our Example on one-way ANOVA in Baumann et al. (1992). If we consider the global \\(F\\)-test of equality in means, we can report as corresponding effect size the percentage of variance that is explained by the experimental condition, group.\n\n\nCode\nlibrary(effectsize)\ndata(BSJ92, package = \"hecedsm\")\nmod &lt;- aov(posttest2 - pretest2 ~ group,\n           data = BSJ92)\nprint_md(omega_squared(anova(mod), partial = FALSE))\n\n\n\nEffect Size for ANOVA (Type I)\n\n\nParameter\nOmega2\n95% CI\n\n\n\n\ngroup\n0.16\n[0.03, 1.00]\n\n\n\nOne-sided CIs: upper bound fixed at [1.00].\n\n\nThe estimator employed is \\(\\widehat{\\omega}^2\\) and could be obtained directly using the formula provided in the slides. For a proportion of variance, the number is medium according to Cohen (1988) definition. Using \\(\\widehat{R}^2 \\equiv \\widehat{\\eta}^2\\) as estimator instead would give an estimated proportion of variance of 0.188, a slightly higher number.\nHaving found a significant difference in mean between groups, one could be interested in computing estimated marginal means and contrasts based on the latter. The emmeans function has a method for computing effect size (Cohen’s \\(d\\)) for pairwise differences if provided with the denominator standard deviation \\(\\sigma\\) and the degrees of freedom associated with the latter (i.e., how many observations were left from the total sample size after subtracting the number of subgroup means).\n\n\nCode\nlibrary(emmeans)\npair_diff &lt;- emmeans(\n  mod, spec = \"group\") |&gt; \n  eff_size(sigma = sigma(mod), \n           edf = df.residual(mod))\n\n\nThe confidence intervals reported by emmeans for \\(t\\)-tests are symmetric and different in nature from the one obtained previously.\nTechnical aside: while it is possible to create a \\(t\\)-statistic for a constrast by dividing the contrast estimator by it’s standard error, the construction of Cohen’s \\(d\\) here for the contrast consisting of, e.g., the pairwise difference between DRTA and TA would take the form \\[\nd_{\\text{DRTA}- \\text{TA}} = \\frac{\\mu_{\\text{DRTA}}- \\mu_{\\text{TA}}}{\\sigma},\n\\] where the denominator stands for the standard deviation of the observations.4\n\n\n\nArmed with effect sizes and a desired level of power, it is possible to determine the minimum number of observations that would yield such effect.\nJohnson et al. (2014) performs a replication study of Schnall, Benton, and Harvey (2008) who conjectured that physical cleanliness reduces the severity of moral judgments.\nThe following excerpt from the paper explain how sample size for the replication were calculated.\n\nIn Experiment 2, the critical test of the cleanliness manipulation on ratings of morality was significant, \\(F(1, 41) = 7.81\\), \\(p=0.01\\), \\(d=0.87\\), \\(N=44\\). Assuming \\(\\alpha=0.05\\), the achieved power in this experiment was \\(0.80\\). Our proposed research will attempt to replicate this experiment with a level of power = \\(0.99\\). This will require a minimum of 100 participants (assuming equal sized groups with \\(d=0.87\\)) so we will collect data from 115 participants to ensure a properly powered sample in case of errors.\n\nThe first step is to try and compute the effect size, here Cohen’s \\(d\\), from the reported \\(F\\) statistic to make sure it matches the quoted value.\n\n\nCode\ndhat &lt;- effectsize::F_to_d(\n  f = 7.81,\n  df = 1, \n  df_error = 41)\n\n\nThis indeed coincides with the value reported for Cohen’s \\(d\\) estimator. We can then plug-in this value in the power function with the desired power level \\(0.99\\) to find out a minimal number of 50 participants in each group, for a total of 100 if we do a pairwise comparison using a two-sample \\(t\\)-test.\n\n\nCode\nWebPower::wp.t(\n  d = 0.87,\n  power = 0.99,\n  type = \"two.sample\")\n\n\nTwo-sample t-test\n\n           n    d alpha power\n    49.53039 0.87  0.05  0.99\n\nNOTE: n is number in *each* group\nURL: http://psychstat.org/ttest\n\n\nThe effectsize package includes many functions to convert \\(F\\) and \\(t\\) statistics to effect sizes.5\n\n\n\nWhile software can easily compute effect sizes, the user should not blindly rely on the output, but rather think about various elements using the following guiding principles:\n\nwe are interested in partial effects when there are multiple factors\nthe denominator should consist of the variance of the effect of interest (say factor \\(A\\)), the variance of blocking factors and random effects and that of all interactions associated with them.\n\nConsider next the unbalanced two-way ANOVA example with Study 1 of Maglio & Polman (2014).\nWe pass here directly the output of the model. We use the lm function with the mean-to-zero parametrization, since we have unbalanced data.\n\n\nCode\ndata(MP14_S1, package = 'hecedsm')\n# Force mean-to-zero parametrization\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\n# Estimate two-way ANOVA with interaction\nmodel &lt;- lm(distance ~ station*direction, \n            data = MP14_S1)\n# Test only the interaction\nout &lt;- car::Anova(model, type = 3)\n\n\nBy default, the variance terms for each factor and interaction are estimated using the anova call. When the data aren’t balanced and you have multiple factors in the mean equation, these are the sequential sum of square estimates (type I). This means that the resulting effect size would depend on the order in which you specify the terms, an unappealing feature. The model can alternatively take as argument the analysis of variance table produced by the Anova function in package car, e.g., car::Anova(..., type = 3). Note that it is of paramount importance to pass the correct arguments and to use the mean-to-zero parametrization in order to get sensible results. The package warns user about this.\n\n\nCode\nomsq &lt;- omega_squared(out)\n\n\nThe estimated effect size for the main effect of direction is negative with \\(\\widehat{\\omega}^2_{\\langle \\text{direction}\\rangle}\\): either reporting a negative value or zero. This reflects that the estimated effect is very insignificant.\nEquipped with the estimated effect size, we can now transform our partial \\(\\widehat{\\omega}^2_{\\langle\\text{AB}\\rangle}\\) measure into an estimated Cohen’s \\(f\\) via \\[\\widetilde{f} = \\left( \\frac{\\widehat{\\omega}^2}{1-\\widehat{\\omega}^2}\\right)^{1/2},\\] which is then fed into WebPower package functionality to compute the post-hoc power. Since we are dealing with a two-way ANOVA with 8 subgroups, we set ng=8 and then ndf corresponding to the degrees of freedom of the estimated interaction (here \\((n_a-1)\\times (n_b-1)=3\\), the number of coefficients needed to capture the interaction).\nGiven all but one of the following collection\n\nthe power,\nthe number of groups and degrees of freedom from the design,\nthe effect size and\nthe sample size,\n\nit is possible to deduce the last one assuming a balanced sample. Below, we use the information to compute the so-called post-hoc power. Such terminology is misleading because there is no guarantee that we are under the alternative, and effect sizes are really noisy proxy so the range of potential values for the missing ingredient is oftentimes quite large. Because studies in the literature have inflated effect size, the power measures are more often than not misleading.\n\n\nCode\n# Power calculations\n# Convert omega-squared to Cohen's f\ncohensf1 &lt;- effectsize::eta2_to_f(\n  omsq$Omega2_partial[3])\nWebPower::wp.kanova(\n  n = 202, # sample size, assumed *balanced*\n  ndf = 3, # degrees of freedom\n  f = cohensf1, # Cohen's f estimate\n  ng = 8) # number of subgroups of ANOVA\n\n\nMultiple way ANOVA analysis\n\n      n ndf ddf         f ng alpha     power\n    202   3 194 0.4764222  8  0.05 0.9999821\n\nNOTE: Sample size is the total sample size\nURL: http://psychstat.org/kanova\n\n\nHere, the interaction is unusually strong (a fifth of the variance is explained by it!) and we have an extremely large post-hoc power estimate. This is rather unsurprising given the way the experiment was set up.\nWe can use the same function to determine how many observations the study would need to minimally achieve a certain power, below of 99% — the number reported must be rounded up to the nearest integer. Depending on the design or function, this number may be the overall sample size or the sample size per group.\n\n\nCode\n# Sample size calculations\ncohensf2 &lt;- cohens_f(out)$Cohens_f_partial[3]\nWebPower::wp.kanova(\n  ndf = 3, \n  f = cohensf1, \n  ng = 8, \n  power = 0.99)\n\n\nMultiple way ANOVA analysis\n\n        n ndf      ddf         f ng alpha power\n    107.8   3 99.80004 0.4764222  8  0.05  0.99\n\nNOTE: Sample size is the total sample size\nURL: http://psychstat.org/kanova\n\n\nCode\nWebPower::wp.kanova(\n  ndf = 3, \n  f = cohensf2, \n  ng = 8,\n  power = 0.99)\n\n\nMultiple way ANOVA analysis\n\n           n ndf      ddf         f ng alpha power\n    97.61394   3 89.61394 0.5017988  8  0.05  0.99\n\nNOTE: Sample size is the total sample size\nURL: http://psychstat.org/kanova\n\n\nThe total sample size using \\(\\widehat{\\omega}^2\\) is 108, whereas using the biased estimator \\(\\widehat{f}\\) directly (itself obtained from \\(\\widehat{\\eta}^2\\)) gives 98: this difference of 10 individuals can have practical implications."
  },
  {
    "objectID": "example/effectsizepower.html#effect-sizes",
    "href": "example/effectsizepower.html#effect-sizes",
    "title": "Effect size and power",
    "section": "",
    "text": "Effect size typically serve three purpose:\n\ninform readers of the magnitude of the effect,\nprovide a standardized quantity that can be combined with others in a meta-analysis, or\nserve as proxy in a power study to estimate the minimum number of observations needed.\n\nIf you report the exact value of the test statistic, the null distribution and (in short) all elements of an analysis of variance table in a complex design, it is possible by using suitable formulae to recover effect sizes, as they are functions of the test statistic summaries, degrees of freedom and correlation between observations (in the case of repeated measures).\nThe effectsize package includes a variety of estimators for standardized difference or ratio of variance. For example, for the latter, we can retrieve Cohen’s \\(f\\) via cohens_f, \\(\\widehat{\\epsilon}^2\\) via epsilon_squared or \\(\\widehat{\\omega}^2\\) via omega_squared.\nBy default, in a design with more than one factor, the partial effects are returned (argument partial = TRUE) — if there is a single factor, these coincide with the total effects and the distinction is immaterial.\nThe effectsize package reports confidence intervals2 calculated using the pivot method described in Steiger (2004). Check the documentation at ?effectsize::effectsize_CIs for more technical details.3\nIn general, confidence intervals for effect sizes are very wide, including a large range of potential values and sometimes zero. This reflects the large uncertainty surrounding their estimation and should not be taken to mean that the estimated effect is null."
  },
  {
    "objectID": "example/effectsizepower.html#example-1---one-way-anova",
    "href": "example/effectsizepower.html#example-1---one-way-anova",
    "title": "Effect size and power",
    "section": "",
    "text": "We begin with the result of our Example on one-way ANOVA in Baumann et al. (1992). If we consider the global \\(F\\)-test of equality in means, we can report as corresponding effect size the percentage of variance that is explained by the experimental condition, group.\n\n\nCode\nlibrary(effectsize)\ndata(BSJ92, package = \"hecedsm\")\nmod &lt;- aov(posttest2 - pretest2 ~ group,\n           data = BSJ92)\nprint_md(omega_squared(anova(mod), partial = FALSE))\n\n\n\nEffect Size for ANOVA (Type I)\n\n\nParameter\nOmega2\n95% CI\n\n\n\n\ngroup\n0.16\n[0.03, 1.00]\n\n\n\nOne-sided CIs: upper bound fixed at [1.00].\n\n\nThe estimator employed is \\(\\widehat{\\omega}^2\\) and could be obtained directly using the formula provided in the slides. For a proportion of variance, the number is medium according to Cohen (1988) definition. Using \\(\\widehat{R}^2 \\equiv \\widehat{\\eta}^2\\) as estimator instead would give an estimated proportion of variance of 0.188, a slightly higher number.\nHaving found a significant difference in mean between groups, one could be interested in computing estimated marginal means and contrasts based on the latter. The emmeans function has a method for computing effect size (Cohen’s \\(d\\)) for pairwise differences if provided with the denominator standard deviation \\(\\sigma\\) and the degrees of freedom associated with the latter (i.e., how many observations were left from the total sample size after subtracting the number of subgroup means).\n\n\nCode\nlibrary(emmeans)\npair_diff &lt;- emmeans(\n  mod, spec = \"group\") |&gt; \n  eff_size(sigma = sigma(mod), \n           edf = df.residual(mod))\n\n\nThe confidence intervals reported by emmeans for \\(t\\)-tests are symmetric and different in nature from the one obtained previously.\nTechnical aside: while it is possible to create a \\(t\\)-statistic for a constrast by dividing the contrast estimator by it’s standard error, the construction of Cohen’s \\(d\\) here for the contrast consisting of, e.g., the pairwise difference between DRTA and TA would take the form \\[\nd_{\\text{DRTA}- \\text{TA}} = \\frac{\\mu_{\\text{DRTA}}- \\mu_{\\text{TA}}}{\\sigma},\n\\] where the denominator stands for the standard deviation of the observations.4"
  },
  {
    "objectID": "example/effectsizepower.html#example-2-sample-size-for-replication-studies",
    "href": "example/effectsizepower.html#example-2-sample-size-for-replication-studies",
    "title": "Effect size and power",
    "section": "",
    "text": "Armed with effect sizes and a desired level of power, it is possible to determine the minimum number of observations that would yield such effect.\nJohnson et al. (2014) performs a replication study of Schnall, Benton, and Harvey (2008) who conjectured that physical cleanliness reduces the severity of moral judgments.\nThe following excerpt from the paper explain how sample size for the replication were calculated.\n\nIn Experiment 2, the critical test of the cleanliness manipulation on ratings of morality was significant, \\(F(1, 41) = 7.81\\), \\(p=0.01\\), \\(d=0.87\\), \\(N=44\\). Assuming \\(\\alpha=0.05\\), the achieved power in this experiment was \\(0.80\\). Our proposed research will attempt to replicate this experiment with a level of power = \\(0.99\\). This will require a minimum of 100 participants (assuming equal sized groups with \\(d=0.87\\)) so we will collect data from 115 participants to ensure a properly powered sample in case of errors.\n\nThe first step is to try and compute the effect size, here Cohen’s \\(d\\), from the reported \\(F\\) statistic to make sure it matches the quoted value.\n\n\nCode\ndhat &lt;- effectsize::F_to_d(\n  f = 7.81,\n  df = 1, \n  df_error = 41)\n\n\nThis indeed coincides with the value reported for Cohen’s \\(d\\) estimator. We can then plug-in this value in the power function with the desired power level \\(0.99\\) to find out a minimal number of 50 participants in each group, for a total of 100 if we do a pairwise comparison using a two-sample \\(t\\)-test.\n\n\nCode\nWebPower::wp.t(\n  d = 0.87,\n  power = 0.99,\n  type = \"two.sample\")\n\n\nTwo-sample t-test\n\n           n    d alpha power\n    49.53039 0.87  0.05  0.99\n\nNOTE: n is number in *each* group\nURL: http://psychstat.org/ttest\n\n\nThe effectsize package includes many functions to convert \\(F\\) and \\(t\\) statistics to effect sizes.5"
  },
  {
    "objectID": "example/effectsizepower.html#example-3-two-way-anova-with-unbalanced-data",
    "href": "example/effectsizepower.html#example-3-two-way-anova-with-unbalanced-data",
    "title": "Effect size and power",
    "section": "",
    "text": "While software can easily compute effect sizes, the user should not blindly rely on the output, but rather think about various elements using the following guiding principles:\n\nwe are interested in partial effects when there are multiple factors\nthe denominator should consist of the variance of the effect of interest (say factor \\(A\\)), the variance of blocking factors and random effects and that of all interactions associated with them.\n\nConsider next the unbalanced two-way ANOVA example with Study 1 of Maglio & Polman (2014).\nWe pass here directly the output of the model. We use the lm function with the mean-to-zero parametrization, since we have unbalanced data.\n\n\nCode\ndata(MP14_S1, package = 'hecedsm')\n# Force mean-to-zero parametrization\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\n# Estimate two-way ANOVA with interaction\nmodel &lt;- lm(distance ~ station*direction, \n            data = MP14_S1)\n# Test only the interaction\nout &lt;- car::Anova(model, type = 3)\n\n\nBy default, the variance terms for each factor and interaction are estimated using the anova call. When the data aren’t balanced and you have multiple factors in the mean equation, these are the sequential sum of square estimates (type I). This means that the resulting effect size would depend on the order in which you specify the terms, an unappealing feature. The model can alternatively take as argument the analysis of variance table produced by the Anova function in package car, e.g., car::Anova(..., type = 3). Note that it is of paramount importance to pass the correct arguments and to use the mean-to-zero parametrization in order to get sensible results. The package warns user about this.\n\n\nCode\nomsq &lt;- omega_squared(out)\n\n\nThe estimated effect size for the main effect of direction is negative with \\(\\widehat{\\omega}^2_{\\langle \\text{direction}\\rangle}\\): either reporting a negative value or zero. This reflects that the estimated effect is very insignificant.\nEquipped with the estimated effect size, we can now transform our partial \\(\\widehat{\\omega}^2_{\\langle\\text{AB}\\rangle}\\) measure into an estimated Cohen’s \\(f\\) via \\[\\widetilde{f} = \\left( \\frac{\\widehat{\\omega}^2}{1-\\widehat{\\omega}^2}\\right)^{1/2},\\] which is then fed into WebPower package functionality to compute the post-hoc power. Since we are dealing with a two-way ANOVA with 8 subgroups, we set ng=8 and then ndf corresponding to the degrees of freedom of the estimated interaction (here \\((n_a-1)\\times (n_b-1)=3\\), the number of coefficients needed to capture the interaction).\nGiven all but one of the following collection\n\nthe power,\nthe number of groups and degrees of freedom from the design,\nthe effect size and\nthe sample size,\n\nit is possible to deduce the last one assuming a balanced sample. Below, we use the information to compute the so-called post-hoc power. Such terminology is misleading because there is no guarantee that we are under the alternative, and effect sizes are really noisy proxy so the range of potential values for the missing ingredient is oftentimes quite large. Because studies in the literature have inflated effect size, the power measures are more often than not misleading.\n\n\nCode\n# Power calculations\n# Convert omega-squared to Cohen's f\ncohensf1 &lt;- effectsize::eta2_to_f(\n  omsq$Omega2_partial[3])\nWebPower::wp.kanova(\n  n = 202, # sample size, assumed *balanced*\n  ndf = 3, # degrees of freedom\n  f = cohensf1, # Cohen's f estimate\n  ng = 8) # number of subgroups of ANOVA\n\n\nMultiple way ANOVA analysis\n\n      n ndf ddf         f ng alpha     power\n    202   3 194 0.4764222  8  0.05 0.9999821\n\nNOTE: Sample size is the total sample size\nURL: http://psychstat.org/kanova\n\n\nHere, the interaction is unusually strong (a fifth of the variance is explained by it!) and we have an extremely large post-hoc power estimate. This is rather unsurprising given the way the experiment was set up.\nWe can use the same function to determine how many observations the study would need to minimally achieve a certain power, below of 99% — the number reported must be rounded up to the nearest integer. Depending on the design or function, this number may be the overall sample size or the sample size per group.\n\n\nCode\n# Sample size calculations\ncohensf2 &lt;- cohens_f(out)$Cohens_f_partial[3]\nWebPower::wp.kanova(\n  ndf = 3, \n  f = cohensf1, \n  ng = 8, \n  power = 0.99)\n\n\nMultiple way ANOVA analysis\n\n        n ndf      ddf         f ng alpha power\n    107.8   3 99.80004 0.4764222  8  0.05  0.99\n\nNOTE: Sample size is the total sample size\nURL: http://psychstat.org/kanova\n\n\nCode\nWebPower::wp.kanova(\n  ndf = 3, \n  f = cohensf2, \n  ng = 8,\n  power = 0.99)\n\n\nMultiple way ANOVA analysis\n\n           n ndf      ddf         f ng alpha power\n    97.61394   3 89.61394 0.5017988  8  0.05  0.99\n\nNOTE: Sample size is the total sample size\nURL: http://psychstat.org/kanova\n\n\nThe total sample size using \\(\\widehat{\\omega}^2\\) is 108, whereas using the biased estimator \\(\\widehat{f}\\) directly (itself obtained from \\(\\widehat{\\eta}^2\\)) gives 98: this difference of 10 individuals can have practical implications."
  },
  {
    "objectID": "example/effectsizepower.html#footnotes",
    "href": "example/effectsizepower.html#footnotes",
    "title": "Effect size and power",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are many other alternatives.↩︎\nReally, these are fiducial intervals based on confidence distributions.↩︎\nNote that, when the test statistic representing the proportion of variance explained is strictly positive, like a \\(F\\) or \\(\\chi^2\\) statistic, the corresponding effect size is an estimated percentage of variance returned by, e.g., \\(\\widehat{\\omega}^2\\). To ensure consistency, the confidence intervals are one-sided, giving a lower bound (for the minimum effect size compatible with the data), while the upper bound is set to the maximum value, e.g., 1 for a proportion.↩︎\nIt isn’t always obvious when marginalizing out a one-way ANOVA from a complex design or when we have random effects or blocking factor what the estimated standard deviation should be, so it is left to the user to specify the correct quantity.↩︎\nAs the example of Baumann et al. (1992) showed, however, not all statistics can be meaningfully converted to effect size.↩︎"
  },
  {
    "objectID": "example/ancova.html",
    "href": "example/ancova.html",
    "title": "Analysis of covariance",
    "section": "",
    "text": "The R code can be downloaded here and the SPSS script here."
  },
  {
    "objectID": "example/ancova.html#graphical-representation",
    "href": "example/ancova.html#graphical-representation",
    "title": "Analysis of covariance",
    "section": "Graphical representation",
    "text": "Graphical representation\nThis section can be omitted upon first reading. We may want to compare the scores prior and post-intervention graphically to get a sense of the improvement for groups and check that the randomization was properly conducted. One way to look at the data is a longitudinal measurement (time ordered). Thus, we could draw line segments joining pre and post scores, with a different color for each group. To do this, we first combine the data in long format (one response per line) by making prior and posts into a single column labelled score. Then, we map the two categorical variables (time and group) to the \\(x\\)-axis and colour, respectively. Since the scores are discrete, points are jittered horizontally and vertically to avoid overlay.\n\n\nCode\nreading_long &lt;- BSJ92 |&gt;\n  # keep only useful columns\n  select(group, posttest2, pretest2) |&gt; \n  # add a new column with id for person\n  rowid_to_column(\"id\") |&gt; \n  # rename response\n  rename(post = posttest2, \n         pre = pretest2) |&gt;\n  # pivot data (response in score, columns to labels)\n  pivot_longer(cols = c(\"pre\",\"post\"),\n               values_to = \"score\",\n               names_to = \"time\") |&gt;\n  # Make new labels to factor\n  mutate(time = relevel(factor(time), ref = \"pre\"))\n\n# Spaghetti plot\nggplot(data = reading_long,\n       mapping = aes(x = time,\n                     color = group,\n                     group = id,\n                     y = score)) +\n  # Add point for observation with small jittering\n  geom_point(size = 0.5,\n             position = position_jitter(width = 0.02, \n                                        height = 0.25, \n                                        seed = 123)) + \n  # The values were selected after trial and error\n  # set seed so that ponts and segments are moved by same amount\n  # Draw a line between pre and post scores\n  geom_line(alpha = 0.2,\n            position = position_jitter(width = 0.02, \n                                       height = 0.25, \n                                       seed = 123)) +\n  # Add average per group (pre/post) and draw lines\n  stat_summary(aes(group = group),\n               fun = \"mean\",\n               geom = \"line\",\n               linewidth = 1.2) +\n  # Reduce space left-right of pre-post\n  scale_x_discrete(expand = c(0, 0.1)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nThe graphical display indicates a larger increase for the teaching aloud (TA) group, but the two other (directed reading and the hybrid method) seem on par. There is no discernible difference in strength between experimental condition prior to the experiment, but not the two strong grades in the DR group (who do worse)."
  },
  {
    "objectID": "example/ancova.html#analysis-of-variance-vs-analysis-of-covariance",
    "href": "example/ancova.html#analysis-of-variance-vs-analysis-of-covariance",
    "title": "Analysis of covariance",
    "section": "Analysis of variance vs analysis of covariance",
    "text": "Analysis of variance vs analysis of covariance\nIn this simple setting, we begin by comparing the drop in variability (leading to an increase in power) for the analysis of covariance. We thus set up a linear model with\n\\[\\mathsf{E}(\\texttt{post}_{ir}) = \\mu + \\alpha_i + \\beta \\texttt{pre}_{ir}\\] with \\(\\mathsf{Va}(\\texttt{post}_{ir})=\\sigma^2\\). We can view this model as a one-way analysis of variance for \\(\\texttt{post}_{ir} - \\beta \\texttt{pre}_{ir}\\), with the difference that we estimate the coefficient \\(\\beta\\) based on the data. Under our usual sum-to-zero constraint, \\(\\mu\\) is the global average of the residual scores and \\(\\alpha_i\\) is the difference between group \\(i\\) and this average, with the constraint \\(\\alpha_{\\texttt{DR}} + \\alpha_{\\texttt{DRTA}} + \\alpha_{\\texttt{TA}}=0\\).\n\n\nCode\n# Model the scores on tests 2\nmodel1 &lt;- lm(posttest2 ~ group + pretest2, data = BSJ92)\n# Model assumes parallel slopes with equal differences\nanova(model1)\n# Only look for the difference between groups\nmodel2 &lt;- lm(posttest2 ~ group, data = BSJ92)\nanova(model2)\n# Compare the mean squared residuals\n# and the p-value for group\n\n\nAs in many instances, there is strong correlation between prior and posterior scores and this helps reduce the leftover unexplained variability, thus increasing power to detect differences between groups. One may also consider directly modelling the improvement in scores, by setting posttest2 - pretest2 as response variable. While the interpretation would be clear, it is clear we can do better by including pretest2 as a covariate instead: the model for the difference in score is equivalent to fixing the parameter \\(\\beta\\) associated to pretest2 to 1.\n\n\n\n\n\nAnalysis of variance tables for Model 1 (ANCOVA, left) and Model 2 (ANOVA, right).\n\n\n\n\n\n\n\nterm\ndf\nsum of squares\nF statistic\np-value\n\n\n\n\ngroup\n2\n95.12\n8.88\n0.0004\n\n\npretest2\n1\n24.22\n4.52\n0.0375\n\n\nResiduals\n62\n332.19\n\n\n\n\n\n\n\n\n\n\nterm\ndf\nsum of squares\nF statistic\np-value\n\n\n\n\ngroup\n2\n95.12\n8.41\n6e-04\n\n\nResiduals\n63\n356.41\n\n\n\n\n\n\n\n\n\n\n\n\nWe can compare the one-way ANOVA model (Model 1) with the analysis of covariance (Model 2). In the present context, either are valid but one measures the average, the other some form of improvement between two time periods. About one fifth of the explained variability is due to pretest. Here, the impact of group is significant and so the inclusion of a covariate made no difference as to the findings of a difference, but strengthened our conclusions."
  },
  {
    "objectID": "example/ancova.html#contrasts",
    "href": "example/ancova.html#contrasts",
    "title": "Analysis of covariance",
    "section": "Contrasts",
    "text": "Contrasts\nThe study focused on the comparison between the business-as-usual scenario (DA) and the average of the other two methods. They also compared the relative effectiveness of DA vs DRTA. We can compute marginal means and compute these contrasts as usual.\nBecause we assume the difference post vs pre is the same on average (the slope between groups is the same), the difference between group is the same regardless of the score on the pre-test. The marginal means are computed at the average of prettest2 score.\nThe two contrasts we have correspond to \\(\\mathscr{H}_{01}: \\mu_{\\texttt{DR}} = (\\mu_{\\texttt{DRTA}} + \\mu_{\\texttt{TA}})/2\\) and \\(\\mathscr{H}_{02}: \\mu_{\\texttt{DRTA}} = \\mu_{\\texttt{TA}}\\), which can be rewritten with weights \\((2, -1, -1)\\) and \\((0, 1, -1)\\) assuming that the groups are ordered in alphabetical order. The two contrasts are orthogonal so use disjoint bits of information thanks to the conditioning. We proceed below with Holm–Bonferroni’s correction (for two tests), but we would normally need to filter out these \\(p\\)-values and perform the corrections with all other tests. The effect size for the contrasts are obtained by dividing by the estimated variance for the residuals, \\(\\widehat{\\sigma}^2\\), which is smaller for the ANCOVA model since part of the variability is explained by students strength, captured through prettest2.\n\n\nCode\n# Marginal means and planned contrasts\n# Groups are in alphabetical order, so (DR, DRTA, TA)\nplanned_contrasts &lt;-\n  list(c1 = c(2, -1, -1), #H0: DR = average(DRTA, TA)\n       c2 = c(0, 1, -1))  #H0: DRTA = TA\ncontrasts1 &lt;- \n  contrast(emmeans(model1, specs = \"group\"),\n         method = planned_contrasts,\n         p.adjust = \"holm\")\n# Measure of effect size\ncohens_d &lt;- emmeans::eff_size(contrasts1,\n                  method = \"identity\",\n                  sigma = sigma(model1),\n                  edf = df.residual(model1))\n\n\n\n\n\n\n\nPreplanned contrasts for Model 1 (ANCOVA, left) and Model 2 (ANOVA, right); c1 denotes the comparison between control (DR) and average of other methods, while c2 is the pairwise comparison between DRTA and TA.\n\n\n\n\n\n\n\ncontrast\nestimate\nse\ndf\nt statistic\np-value\n\n\n\n\nc1\n-3.64\n1.21\n62\n-3.01\n0.0038\n\n\nc2\n-2.17\n0.70\n62\n-3.11\n0.0028\n\n\n\n\n\n\n\n\ncontrast\nestimate\nse\ndf\nt statistic\np-value\n\n\n\n\nc1\n-3.50\n1.24\n63\n-2.82\n0.0065\n\n\nc2\n-2.14\n0.72\n63\n-2.98\n0.0041\n\n\n\n\n\n\n\n\n\n\nThe values for the contrasts show that \\(p\\)-values have been reduced by 30% to 40%, so while there are clear differences in treatment, it could help increase the power in more borderline cases. The estimated Cohen’s \\(d\\) measure for contrast 1 is -1.57 (95% CI: -2.65, -0.49) for Model 1 (ANCOVA), versus -1.47 (95% CI: -2.55, -0.4) for Model 2 (ANOVA). These confidence interval are unadjusted, but are narrower with the added covariate. The estimated effect size is large, and the upper bounds for the effect imply a medium-sized effect at the very least."
  },
  {
    "objectID": "example/ancova.html#testing-model-assumptions",
    "href": "example/ancova.html#testing-model-assumptions",
    "title": "Analysis of covariance",
    "section": "Testing model assumptions",
    "text": "Testing model assumptions\nGenerally, there are three additional assumptions to consider in the analysis of covariance model.\n\nwhether the slopes in each experimental condition are the same or not.\nwhether the linear relationship between the covariate and the response holds.\nwhether the covariate is measured with error.\n\nIn principle, we could fit a different slope for the response variable in each group. However, if we do this, the difference between groups depends on the value of the covariate, which makes comparisons. If the slope is the same, all lines are parallel and the effect of the experimental factor is the same regardless of the other explanatory.\nTo assess the hypothesis of equal slope, we can fit a model with different slopes in each group (via an interaction between the experimental condition and the covariate) and assess the increase in goodness-of-fit with the model that has the same slope. Such models are nested and can be compared in terms of the increase in sum of square.\nThe other two additional assumptions are equally important, but harder to quantify. The linearity of the relationship can be verified graphically through residual plots (ordinary residuals vs fitted values), as before. If there is evidence of (local) structure, one may hope to pick it up. Measurement error depends on the context.\nBy design, observations are independent and the randomization was well performed; our quick graphical depiction of the data showed that there were no difference in mean apriori.\n\nLinearity: effect of covariate is linear\nThe analysis of covariance is a linear model, so inherits a set of assumptions. The first assumption is linearity, which means here that the relationship between pre- and post-tests is well described by a straight line. We can produce a scatterplot of the data to check this, but usually one looks at the residuals from the model (the difference between the observation and its estimated average, or fitted value \\(e_i=\\texttt{posttest2}_i - \\widehat{\\texttt{posttest2}}_i\\). These are uncorrelated with \\(\\texttt{pretest2}_i\\), so the plot should show no pattern (trends, cycles, quadratic trend, etc.)\n\n\nCode\nlibrary(patchwork) # combine plots\ng1 &lt;- ggplot(data = BSJ92, \n       mapping = aes(x = pretest2, y = posttest2)) + \n  geom_point(position = position_jitter(width = 0.1, \n                                        height = 0.1, \n                                        seed = 80667)) + \n  geom_smooth(method = \"lm\", se = FALSE)\nreading_aug &lt;- BSJ92 |&gt;\n  mutate(.resid = resid(model1), # observation minus fitted\n         .rstudent = rstudent(model1), \n         # studentized resid. (equal variance)\n         .fitted = fitted(model1)) # fitted values yhat\ng2 &lt;- ggplot(data = reading_aug,\n             aes(x = pretest2,\n                 y = .resid)) + \n  geom_point() +\n  geom_smooth() + # Fit local trend to detect patterns\n  labs(x = 'ordinary residuals')\ng1 + g2\n\n\n\n\n\nScatterplot of pre and post intervention tests (left) and scatterplot of ordinary residuals against covariate pre-test2, with LOESS curve (right).\n\n\n\n\nCode\n# To save for a publication\n# ggsave(\"scatterplot.pdf\", plot = g1, width = 5, height = 5)\n\n\nSince the measurements are discrete and scarce, it is hard to see much but this assumption is reasonable. An added line (local fit, LOESS) is added to help detect departures from linearity but be careful not to over-interpret the graphs. Another way to see this is to create replicate data by permuting residuals (a form of graphical test); if we can easily detect the dataset from the list, then there may be residual structure (otherwise the patterns are due to sampling variability). For example, could you spot the true residuals from the 20 panels?\n\n\n\n\n\nPanels with 20 replicate datasets obtained by permuting residuals. The true data is in position 16.\n\n\n\n\n\n\nLinearity: are the slopes the same?\nThe second part of the linearity assumption lies in the assumption that the linear trend for pre-test and post-test is the same for each group. If they were different, we would have an interaction (between a categorical and a continuous variable). We can use the \\(F\\) test to compare the model with and without interaction\n\n\nCode\nmodel3 &lt;- lm(posttest2 ~ group*pretest2, data = BSJ92)\nanova(model1, model3)\n\n\nThe \\(p\\)-value is 0.9, so there is no evidence that the slopes are not parallel.\n\n\nTest for equality of variance\nAnother assumption of the linear model is that of constant variance. In analysis of variance models, we specify that each (sub)group has its own mean, but that the variance \\(\\sigma^2\\) of the measurements is constant. This is useful because we can pool all observations to more reliably estimate it if the assumption holds true. If it isn’t true, what we obtain is some average of small and large variance, and this impacts our power (comparing groups with small variance).\nIf we move away from equal variance, what is the alternative? For one, we could simply estimate a different variance for each level of the experimental factors. For one-way designs, it’s pretty clear how this could be modelled, but there is more choice in complex factorial designs: should we have a different variance per row? or by column? or let each cell have its own variance? The more variance we estimate, the more we need to have observations per cell. Moving away from equal variance has also some consequences: when we talked about effect size, we looked at proportion of variance relative to the variance of the residual (which now depends on the group) or for Cohen’s \\(d\\) at standardized mean difference (which have no analog).\nIf we want to check that the variance are equal, we can use Levene’s test. There is a catch: normally, Levene’s test is just a one-way ANOVA where for (sub)group \\(i\\) and replication \\(r\\), we use as response the absolute value of the centered observations, \\(|Y_{ir} - \\widehat{\\mu}_i|\\), as input to the model. With continuous covariates included, an alternative is to instead perform a test but with the detrended values, or use the absolute value of the ordinary residuals \\(|Y_{ir} - \\widehat{Y}_{ir}|\\).\n\n\nCode\ncar::leveneTest(resid(model1) ~ group,\n                data = BSJ92,\n                center = mean)\n\n\n\n\n\n\n\n\ndf\nstatistic\np-value\n\n\n\n\ngroup\n2\n1.48\n0.23\n\n\n\n63\n\n\n\n\n\n\n\nFitting a variance per group and doing a comparison between group mean is oftentimes referred to as Welch’s ANOVA. If we are only interested in the global null, the function oneway.test() does this assuming by default unequal variance. This only returns the \\(F\\) statistic for the global null hypothesis \\(\\mu_{\\texttt{DR}} = \\mu_{\\texttt{DRTA}} = \\mu_{\\texttt{TA}}\\).\n\n\nCode\nlm_detrend &lt;- lm(posttest2 ~ pretest2, \n                       data = BSJ92)\noneway.test(resid(lm_detrend)~ group, \n            data = BSJ92)"
  },
  {
    "objectID": "example/ancova.html#models-with-different-variance-per-group",
    "href": "example/ancova.html#models-with-different-variance-per-group",
    "title": "Analysis of covariance",
    "section": "Models with different variance per group",
    "text": "Models with different variance per group\nIf we wanted to do pairwise comparisons or look at custom contrasts, we need rather a more explicit model in which the mean and variance differ by group, using the nlme package. The argument weights is used to specify the variance.\n\n\nCode\nmodel_samevar &lt;- nlme::gls(posttest2 ~ group + pretest2,\n                     data = BSJ92)\nmodel_diffvar &lt;- nlme::gls(posttest2 ~ group + pretest2,\n                    data = BSJ92,\n                    weights = nlme::varIdent(form =  ~ 1 | group))\n\n\nThe syntax nlme::varIdent(form =  ~ 1 | group) specifies that the variance is constant (~ 1) for observations within a group, but changes from one experimental group to the next.\nThe first benefit of this approach is that we get another test of equality of variance; this is a likelihood ratio test, where we are using the normality assumption to fit the model and make comparisons. Comparison of two nested models (think Russian dolls) that have the same specification for the mean can be done using the function anova.\n\n\nCode\n# compute likelihood ratio test\nanova(model_diffvar, model_samevar)\n\n\nIn large samples and under normality, the likelihood ratio statistic follows a \\(\\chi^2_{g-1}\\) distribution, where \\(g\\) is the number of groups.1 From the table, we can extract the value of the test statistic (L.Ratio), which is 4.65. We compare this to the null \\(\\chi^2_2\\) distribution, giving a tail probability of 0.098; there is weak evidence of heterogeneity.\nThe benefit of using this framework is that emmeans will recognize the unequal variance and use them accordingly for computing contrasts. The \\(t\\)-test values for the contrasts have non-integer degrees of freedom, since the null distribution is intractable; however, the Student-\\(t\\) distribution provides a reasonable approximation if we evaluate it at a particular value for the degrees of freedom, a method know as Satterthwaite’s approximation. See this mention in the output of the method.\n\n\nCode\ncontrast(emmeans(model_diffvar, specs = \"group\"),\n         method = planned_contrasts,\n         p.adjust = \"holm\")\n\n\n contrast estimate    SE   df t.ratio p.value\n c1          -3.64 1.095 51.7  -3.329  0.0016\n c2          -2.18 0.757 38.0  -2.875  0.0066\n\nDegrees-of-freedom method: satterthwaite"
  },
  {
    "objectID": "example/ancova.html#footnotes",
    "href": "example/ancova.html#footnotes",
    "title": "Analysis of covariance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis follows the general rule when comparing two nested models: the degrees of freedom are the difference between the number of parameters under the alternative (the more complicated model) and the null model (which we upon by setting restrictions on the alternative model).↩︎"
  },
  {
    "objectID": "evaluations/weekly-check-in.html",
    "href": "evaluations/weekly-check-in.html",
    "title": "Questions",
    "section": "",
    "text": "Every week, after you finish working through the content, I want to hear about what you learned and what questions you still have. To facilitate this, and to encourage engagement with the course content, you’ll need to fill out a short questionnaire. This should be ~150 words.\nYou should answer the following three questions each week:\n\nWhat was the most exciting thing you learned from the session? Why?\nWhat was the muddiest thing from the session this week? What are you still wondering about?\n\nWhich activity did you find the most useful? What could have been skipped?"
  },
  {
    "objectID": "evaluations/index.html",
    "href": "evaluations/index.html",
    "title": "Evaluations",
    "section": "",
    "text": "The main goals of this class are to help you understand the steps require to design and analyse the results of an experiment."
  },
  {
    "objectID": "evaluations/index.html#weekly-check-in",
    "href": "evaluations/index.html#weekly-check-in",
    "title": "Evaluations",
    "section": "Weekly check-in",
    "text": "Weekly check-in\nI want to hear back from you regarding the course material, activities and learning material.\nI will grade these check-ins using a check system:\n\n110%: Response shows phenomenal thought and engagement with the course content. I will not assign these often.\n100%: Response is thoughtful, well-written, and shows engagement with the course content. This is the expected level of performance.\n50%: Response is hastily composed, too short, and/or only cursorily engages with the course content. This grade signals that you need to improve next time. I will hopefully not assign these often.\n\nNotice that is essentially a pass/fail or completion-based system. I’m not grading your writing ability, I’m not counting the exact number of words you’re writing, and I’m not looking for encyclopedic citations of every single reading to prove that you did indeed read everything. I’m looking for thoughtful engagement, that’s all. Do good work and you’ll get a ✔.\nYou will submit these responses via ZoneCours."
  },
  {
    "objectID": "evaluations/index.html#problem-sets",
    "href": "evaluations/index.html#problem-sets",
    "title": "Evaluations",
    "section": "Problem sets",
    "text": "Problem sets\nThere are 12 problem sets on the schedule. I will keep the highest grades for 11 of them. I will drop the lowest score - this means you can skip one of the problem sets. You need to show that you made a good faith effort to work each question. I will not grade these in detail. The problem sets will be graded using a check system:\n\n110%: Assignment is 100% completed. Every question was attempted and answered, and all answers are correct. Document is clean and easy to follow. Work is exceptional. I will not assign these often.\n100%: Assignment is complete. Every question was attempted and answered, and most answers are correct. This is the expected level of performance.\n75%: Assignment is mostly complete and most answers are correct.\n50%: Assignment is less than 50% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often.\n\nYou may (and should!) work together on the problem sets, but you must turn in your own answers. You cannot work in groups of more than three people, and you must note who participated in the group in your assignment."
  },
  {
    "objectID": "evaluations/index.html#final-project",
    "href": "evaluations/index.html#final-project",
    "title": "Evaluations",
    "section": "Final project",
    "text": "Final project\nFor your final project, you will perform a critical review of a peer-reviewed paper from a list containing an experiment and which uses one of the statistical techniques covered in class. You will pay particular attention to reproducibility, readability and the correctness of the analysis, discussing points of improvement, statistical fallacies. This will help you develop skills for efficient peer-review of the statistical methodology section."
  },
  {
    "objectID": "evaluations/index.html#final-examination",
    "href": "evaluations/index.html#final-examination",
    "title": "Evaluations",
    "section": "Final examination",
    "text": "Final examination\nThere will be a closed-book final exam covering the content of the whole semester. Most of the questions will relate to the tasks you performed in problem set and to discussions we had in class. The exam will be in-person (see the Syllabus for details)."
  },
  {
    "objectID": "evaluations/12-problem-set.html",
    "href": "evaluations/12-problem-set.html",
    "title": "Problem set 12",
    "section": "",
    "text": "Complete this task individually.\nSubmission information: please submit on ZoneCours\nElliott et al. (2021) attempt to replicate a study of Flavell et al. (1966) and study unprompted verbalization by children aged 5 to 10 in an experiment. The data containing the counts of the number of verbalization across all labs are given in the FBC66_T1 and MULTI21_D1 datasets. You can also download the SPSS databases via these links (FBC66_T1 and MULTI21_D1).\nThe data from Table 1 of Flavell et al. (1966) is the number of children who verbalized without prompting overall in the first two experimental conditions (immediate recall and delayed recall subtasks).1\nFor the \\(\\chi^2\\) test to be valid, we need the expected number of counts to be at least five in each subcondition (otherwise the large sample approximation breaks down and is unreliable).\nThe number of students for each grade (kindergarden, second and fifth) is 20. The rows of the table are the ordered number of instances of verbalization.\n# Reinstall package\n# remotes::install_github(\"lbelzile/hecedsm\")\ndata(FBC66_T1, package = \"hecedsm\")\nchisq.test(xtabs(count ~ grade + frequency, data = FBC66_T1))\n\n\n    Pearson's Chi-squared test\n\ndata:  xtabs(count ~ grade + frequency, data = FBC66_T1)\nX-squared = 28.358, df = 4, p-value = 1.055e-05"
  },
  {
    "objectID": "evaluations/12-problem-set.html#footnotes",
    "href": "evaluations/12-problem-set.html#footnotes",
    "title": "Problem set 12",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe last task, point-and-name, is not considered because it specifically instructed students to speak out loud.↩︎\nFor a two-way contingency table under the null hypothesis of no interaction, the expected counts \\(E_{ij}\\) of cell \\((i,j)\\) is obtained as the (total of row \\(i\\)) times (total of column \\(j\\)), divided by the overall count. If the expected number is less than 5, the asymptotic \\(\\chi^2\\) approximation is dubious. We could resort to permutation and simulation to assess the accuracy of the \\(\\chi^2\\) benchmark. The row sum for the counts (per grade) is fixed to 20, but the columns are allowed to vary.↩︎"
  },
  {
    "objectID": "evaluations/10-problem-set.html",
    "href": "evaluations/10-problem-set.html",
    "title": "Problem set 10",
    "section": "",
    "text": "Complete this task individually or in teams of up to three students.\nSubmission information: please submit on ZoneCours\n\na PDF report\nyour code\n\nRead the chocolate example from Meier (2022) and some of the examples from the course notes.\n\n\nElliott et al. (2021) attempt to replicate a study of Flavell et al. (1966) and study unprompted verbalization by children aged 5 to 10 in an experiment. Data MULTI21_D2 from package hecedsm contains the data, including unique participant id, lab, age group (either 5, 6, 7 or 10 year old), different timing for the recall task (either point-and-name, which is always last, delayed recall with 15 seconds after presenting the last image, or immediate response). You can also download the SPSS database via this link.\n\nUsing Oehlert (2000) approach\n\nIdentify sources of variation\nIdentify whether factors are crossed or nested\nDetermine whether factors should be fixed or random\nFigure out which interactions can exist and whether they can be fitted.\n\nFit a linear mixed model with task timing and age categories, accounting for lab and individual-specific variability. Report the conclusions of the two-way analysis of variance:\n\nare there difference between recall task?\nis the increase between 5 and 7 years the same as from 7 to 10?\n\nReport estimated marginal means across age groups (separately for each timing if there is an interaction), with standard errors.\nProduce an interaction plot for mean memory span to mimic Figure 4 of Elliot.\nReport the lab-specific variability and comment on regional differences, if any.\nCompute the correlation of measurements for different individuals in a given lab\n\n\n\n\n\n\n\n\nReferences\n\nElliott, E. M., Morey, C. C., AuBuchon, A. M., Cowan, N., Jarrold, C., Adams, E. J., Attwood, M., Bayram, B., Beeler-Duden, S., Blakstvedt, T. Y., Büttner, G., Castelain, T., Cave, S., Crepaldi, D., Fredriksen, E., Glass, B. A., Graves, A. J., Guitard, D., Hoehl, S., … Voracek, M. (2021). Multilab direct replication of Flavell, Beach, and Chinsky (1966): Spontaneous verbal rehearsal in a memory task as a function of age. Advances in Methods and Practices in Psychological Science, 4(2), 1–20. https://doi.org/10.1177/25152459211018187\n\n\nFlavell, J. H., Beach, D. R., & Chinsky, J. M. (1966). Spontaneous verbal rehearsal in a memory task as a function of age. Child Development, 37(2), 283–299. http://proxy2.hec.ca/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=pbh&AN=10398223&lang=fr&site=ehost-live\n\n\nMeier, L. (2022). ANOVA and mixed models: A short introduction using R (Chapman & Hall/CRC, Eds.). https://doi.org/10.1201/9781003146216\n\n\nOehlert, G. (2000). A first course in design and analysis of experiments. W. H. Freeman. http://users.stat.umn.edu/~gary/Book.html"
  },
  {
    "objectID": "evaluations/08-problem-set.html",
    "href": "evaluations/08-problem-set.html",
    "title": "Problem set 8",
    "section": "",
    "text": "Submission information: please submit on ZoneCours\nLook at the effect size and power; this should be helpful in completing the problem set.\nThe following text is a quote from Investigating variation in replicability: A “Many Labs” Replication Project by Richard A. Klein, Kate A. Ratliff, and Brian A. Nosek (pp. 13-14) about a replication of Oppenheimer & Monin (2009) work on the retrospective gambler fallacy."
  },
  {
    "objectID": "evaluations/08-problem-set.html#footnotes",
    "href": "evaluations/08-problem-set.html#footnotes",
    "title": "Problem set 8",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn R, check out the function F_to_omega2 and F_to_f from the effectsize package. You can also use the formula presented in the slides.↩︎\nHint: are the original effect size in line with the replications or not?↩︎"
  },
  {
    "objectID": "evaluations/06-problem-set.html",
    "href": "evaluations/06-problem-set.html",
    "title": "Problem set 6",
    "section": "",
    "text": "Submission information: please submit on ZoneCours"
  },
  {
    "objectID": "evaluations/06-problem-set.html#task-1",
    "href": "evaluations/06-problem-set.html#task-1",
    "title": "Problem set 6",
    "section": "Task 1",
    "text": "Task 1\nRead Wilkinson (1999)’s guidelines for reporting statistical analyses.\nNext, look at one of the experiments conducted in Stekelenburg et al. (2021). Cross-check with the guidelines and formulate criticism (positive or negative) in relation with the reporting done by the authors."
  },
  {
    "objectID": "evaluations/06-problem-set.html#task-2",
    "href": "evaluations/06-problem-set.html#task-2",
    "title": "Problem set 6",
    "section": "Task 2",
    "text": "Task 2\nRead Nosek et al. (2018) and list three benefits of preregistration. Reflect on the practical aspects and on the impact.1\nWe will organize a discussion in the near future in class on preregistration and the replication crisis."
  },
  {
    "objectID": "evaluations/06-problem-set.html#footnotes",
    "href": "evaluations/06-problem-set.html#footnotes",
    "title": "Problem set 6",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFeel free to discuss field-specific issues.↩︎"
  },
  {
    "objectID": "evaluations/04-problem-set.html",
    "href": "evaluations/04-problem-set.html",
    "title": "Problem set 4",
    "section": "",
    "text": "I encourage you to work in teams for this problem set.\nSubmission information: please submit on ZoneCours\nfollowing the naming convention PS4-studentid.extension where studentid is replaced with your student ID and extension is the file extension (e.g., .pdf, .R, .Rmd, .sps)\nInstructions: We consider data collected for Study 3 of Grossmann & Kross (2014) (click the links to download the paper and the Supplementary material). You can access these data directly from R from the hecedsm package or download the SPSS data.\nThe purpose of the exercise is to reproduce part of Table S4 of the Supplementary material. We will compute linear contrasts and adjust the resulting comparisons to account for multiple testing.1\nWe will fit one-way ANOVA model for each of the four response (compr,limits,persp and change) and for each age group separately. Thus, we will have four contrasts for each of the eight models (one per outcome and age group). The size of the family, assuming the global \\(F\\)-tests are not of interest, is thus \\(m=32\\).\nUse the helper code to get started. Don’t be intimidated: this task will involve a lot of copy-pasting. Fill the helper file code by replacing the #TODO statements and reach out for help if you are stuck."
  },
  {
    "objectID": "evaluations/04-problem-set.html#footnotes",
    "href": "evaluations/04-problem-set.html#footnotes",
    "title": "Problem set 4",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnical aside: there are four correlated outcome variables, so this is technically a multivariate problem. ↩︎\nThe sign may be different from the paper depending on how you set up the contrasts, but the \\(p\\)-values should be the same.↩︎\nHint: Bonferroni’s method amounts to making the tests at level \\(\\alpha/m\\). Try modifying the level in the code for the confidence intervals to see which is largest.↩︎"
  },
  {
    "objectID": "evaluations/02-problem-set.html",
    "href": "evaluations/02-problem-set.html",
    "title": "Problem set 2",
    "section": "",
    "text": "We consider the hypothesis test performed by Bastian et al. (2014) with the data they collected in their Experiment 1. The R and SPSS files containing code for performing the testing procedures and extracting the output are provided. You can also download the SPSS data file and the paper via OpenStats Lab.\n\nBriefly comment on the generalisability of the study in light of the sample composition and data collection.\nThe authors used a two-sample t-test to assess the hypothesis that people in the pain condition felt more threatened by the tasks (a manipulation check). One criticism of using such a test statistic is that it assumes that observations in both group have potentially different group average, but similar standard deviation.1 An alternative test which doesn’t assume equal variance in each experimental condition is Welch’s t-test. Use the latter instead to model the manipulation check for threat and report the output. Do the conclusions change?\nThe authors report results based on a one-way analysis of variance, a testing procedure that compares the mean of \\(m\\) different groups. The latter is equivalent to a two-sample t-test when there are only \\(m=2\\) groups.\n\nUsing the two-sample t-test, obtain a 95% and a 99% confidence interval for the mean difference score for perceived bonding.\nExplain how the two methods (p-value and confidence intervals) are equivalent for performing an hypothesis test with a significance level of 5% and 1%, respectively. Report the conclusions of your procedure in each case.\nList an advantage of each over the other.\n\nExtract the degrees of freedom, the value of the \\(F\\)-statistic and the p-value, suitably rounded, and report these.\nWhat can we reasonably conclude about the effect of bonding based on the output of the testing procedure? Is this reflected in the writing of the authors?\nFigure 1 of Bastian et al. (2014) shows a dynamite plot, i.e., a bar plot with 95% confidence intervals for each condition. Note that this is standard display, but overall it is poor graphical choice (why?). Briefly summarize the blog post."
  },
  {
    "objectID": "evaluations/02-problem-set.html#footnotes",
    "href": "evaluations/02-problem-set.html#footnotes",
    "title": "Problem set 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe will see modelling assumptions in Week 3, take my word on it for now.↩︎"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Readings, lectures, and videos",
    "section": "",
    "text": "Each class session has a set of required readings that you should complete before watching the lecture.\nEvery class session also has a YouTube playlist of short recorded videos for each of the lecture sections. The lecture slides are special HTML files made with the R package xaringan . On each class session page you’ll see buttons for opening the presentation in a new tab or for downloading a PDF of the slides in case you want to print them or store them on your computer:\n\n View all slides in new window  Download PDF of all slides\n\nThe slides are also embedded on each page. You can click in the slides and navigate through them with ← and →. If you type ? (or shift + /) while viewing the slides you can see a list of slide-specific commands (like f for fullscreen or p for presenter mode if you want to see my notes)."
  },
  {
    "objectID": "content/12-content.html",
    "href": "content/12-content.html",
    "title": "Mediation and moderation",
    "section": "",
    "text": "Linear mediation model\nModeration"
  },
  {
    "objectID": "content/12-content.html#content",
    "href": "content/12-content.html#content",
    "title": "Mediation and moderation",
    "section": "",
    "text": "Linear mediation model\nModeration"
  },
  {
    "objectID": "content/12-content.html#learning-objectives",
    "href": "content/12-content.html#learning-objectives",
    "title": "Mediation and moderation",
    "section": "Learning objectives",
    "text": "Learning objectives\nAt the end of the session, students should be capable of\n\nUsing the linear mediation model\ndiscussing limitations of the linear mediation model\ntesting for moderating variables (interactions)"
  },
  {
    "objectID": "content/12-content.html#readings",
    "href": "content/12-content.html#readings",
    "title": "Mediation and moderation",
    "section": "Readings",
    "text": "Readings\n\nChapter 11 of the course notes\nChapter 11 of VanderWeele (2015)\nLinear mediation model (Baron & Kenny, 1986)\nLimitations of the linear mediation model approach (Bullock et al., 2010)\nModel assumptions (Imai et al., 2010; Pearl, 2014)"
  },
  {
    "objectID": "content/12-content.html#slides",
    "href": "content/12-content.html#slides",
    "title": "Mediation and moderation",
    "section": "Slides",
    "text": "Slides\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/12-content.html#code",
    "href": "content/12-content.html#code",
    "title": "Mediation and moderation",
    "section": "Code",
    "text": "Code\n\nR script\nSPSS script\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of slide-specific commands."
  },
  {
    "objectID": "content/10-content.html",
    "href": "content/10-content.html",
    "title": "Intro to mixed models",
    "section": "",
    "text": "Introduction to mixed models"
  },
  {
    "objectID": "content/10-content.html#content",
    "href": "content/10-content.html#content",
    "title": "Intro to mixed models",
    "section": "",
    "text": "Introduction to mixed models"
  },
  {
    "objectID": "content/10-content.html#learning-objectives",
    "href": "content/10-content.html#learning-objectives",
    "title": "Intro to mixed models",
    "section": "Learning objectives",
    "text": "Learning objectives\nAt the end of the session, students should be capable of\n\ndetermining the structure of an experimental design with random and fixed effects\ncorrectly determining whether effects are crossed or nested\nsetting up a linear mixed model"
  },
  {
    "objectID": "content/10-content.html#readings",
    "href": "content/10-content.html#readings",
    "title": "Intro to mixed models",
    "section": "Readings",
    "text": "Readings\n\nChapter 6 Random and Mixed-Effects Models of Lukas Meier’s ANOVA and Mixed Models: A Short Intro Using R]\n Chapter 10 of the course notes"
  },
  {
    "objectID": "content/10-content.html#complementary-readings",
    "href": "content/10-content.html#complementary-readings",
    "title": "Intro to mixed models",
    "section": "Complementary readings",
    "text": "Complementary readings\n\n Chapters 10 and 15 of Maxwell et al. (2017)."
  },
  {
    "objectID": "content/10-content.html#slides",
    "href": "content/10-content.html#slides",
    "title": "Intro to mixed models",
    "section": "Slides",
    "text": "Slides\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/10-content.html#code",
    "href": "content/10-content.html#code",
    "title": "Intro to mixed models",
    "section": "Code",
    "text": "Code\n\nR script\nSPSS script\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of slide-specific commands."
  },
  {
    "objectID": "content/08-content.html",
    "href": "content/08-content.html",
    "title": "Effect size and power",
    "section": "",
    "text": "Measures of effect size\nPower calculations\nInterplay between sample size, effect and power"
  },
  {
    "objectID": "content/08-content.html#content",
    "href": "content/08-content.html#content",
    "title": "Effect size and power",
    "section": "",
    "text": "Measures of effect size\nPower calculations\nInterplay between sample size, effect and power"
  },
  {
    "objectID": "content/08-content.html#learning-objectives",
    "href": "content/08-content.html#learning-objectives",
    "title": "Effect size and power",
    "section": "Learning objectives",
    "text": "Learning objectives\nAt the end of the session, students should be capable of\n\ncorrectly report effect size for common statistics in analysis of variance models\ndeduce the sample size necessary to replicate a study at a given power\nexplain the interplay between sample size, power and effect size."
  },
  {
    "objectID": "content/08-content.html#readings",
    "href": "content/08-content.html#readings",
    "title": "Effect size and power",
    "section": "Readings",
    "text": "Readings\n\n Chapter 7 of the course notes\n Chapter 8 of the course notes\n Effect size and power example"
  },
  {
    "objectID": "content/08-content.html#complementary-readings",
    "href": "content/08-content.html#complementary-readings",
    "title": "Effect size and power",
    "section": "Complementary readings",
    "text": "Complementary readings\n\n Chapters 3 (section Power of the \\(F\\) Test) and 4 (section Measures of Effects) of Maxwell et al. (2017).\n Lakens (2013)\n Steiger (2004)\n Kelley & Preacher (2012)"
  },
  {
    "objectID": "content/08-content.html#slides",
    "href": "content/08-content.html#slides",
    "title": "Effect size and power",
    "section": "Slides",
    "text": "Slides\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of slide-specific commands."
  },
  {
    "objectID": "content/08-content.html#exercise",
    "href": "content/08-content.html#exercise",
    "title": "Effect size and power",
    "section": "Exercise",
    "text": "Exercise\n\nPower calculations"
  },
  {
    "objectID": "content/08-content.html#code",
    "href": "content/08-content.html#code",
    "title": "Effect size and power",
    "section": "Code",
    "text": "Code\n\nR script\nExercise solution"
  },
  {
    "objectID": "content/06-content.html",
    "href": "content/06-content.html",
    "title": "Multiway factorial designs",
    "section": "",
    "text": "Unbalanced designs\nMultiway factorial designs"
  },
  {
    "objectID": "content/06-content.html#content",
    "href": "content/06-content.html#content",
    "title": "Multiway factorial designs",
    "section": "",
    "text": "Unbalanced designs\nMultiway factorial designs"
  },
  {
    "objectID": "content/06-content.html#learning-objectives",
    "href": "content/06-content.html#learning-objectives",
    "title": "Multiway factorial designs",
    "section": "Learning objectives",
    "text": "Learning objectives\nAt the end of the session, students should be capable of\n\nlisting the benefits of balanced designs\ncorrectly interpreting analysis of variance table\nspecifying and calculating custom contrasts in multiway factorial designs\ncorrecting for multiple testing"
  },
  {
    "objectID": "content/06-content.html#readings",
    "href": "content/06-content.html#readings",
    "title": "Multiway factorial designs",
    "section": "Readings",
    "text": "Readings\n\n Chapter 5 of the course notes\n Two-way ANOVA example\n Three-way ANOVA example"
  },
  {
    "objectID": "content/06-content.html#complementary-readings",
    "href": "content/06-content.html#complementary-readings",
    "title": "Multiway factorial designs",
    "section": "Complementary readings",
    "text": "Complementary readings\n\n Chapter 8 of Maxwell et al. (2017).\n Chapters 21-22 of Keppel & Wickens (2004)."
  },
  {
    "objectID": "content/06-content.html#videos",
    "href": "content/06-content.html#videos",
    "title": "Multiway factorial designs",
    "section": "Videos",
    "text": "Videos"
  },
  {
    "objectID": "content/06-content.html#slides",
    "href": "content/06-content.html#slides",
    "title": "Multiway factorial designs",
    "section": "Slides",
    "text": "Slides\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of slide-specific commands."
  },
  {
    "objectID": "content/06-content.html#code",
    "href": "content/06-content.html#code",
    "title": "Multiway factorial designs",
    "section": "Code",
    "text": "Code\n\nR script\nSPSS script"
  },
  {
    "objectID": "content/04-content.html",
    "href": "content/04-content.html",
    "title": "Contrasts and multiple testing",
    "section": "",
    "text": "Contrasts\nMultiple testing"
  },
  {
    "objectID": "content/04-content.html#content",
    "href": "content/04-content.html#content",
    "title": "Contrasts and multiple testing",
    "section": "",
    "text": "Contrasts\nMultiple testing"
  },
  {
    "objectID": "content/04-content.html#learning-objectives",
    "href": "content/04-content.html#learning-objectives",
    "title": "Contrasts and multiple testing",
    "section": "Learning objectives",
    "text": "Learning objectives\nAt the end of the session, students should be capable of\n\nspecifying and calculating custom contrasts in factorial designs\ndetermining the number of tests in a family that need to be corrected for\nunderstanding how to correct p-values to account for multiple testing\nlisting multiplicity testing methods suitable depending on context"
  },
  {
    "objectID": "content/04-content.html#readings",
    "href": "content/04-content.html#readings",
    "title": "Contrasts and multiple testing",
    "section": "Readings",
    "text": "Readings\n\n Chapter 4 of the course notes\n Chapter 3 of Meier (2022)\n One-way ANOVA example"
  },
  {
    "objectID": "content/04-content.html#complementary-readings",
    "href": "content/04-content.html#complementary-readings",
    "title": "Contrasts and multiple testing",
    "section": "Complementary readings",
    "text": "Complementary readings\n\nGelman & Carlin (2014)\n Chapter 5 of Maxwell et al. (2017).\n Chapters 4 and 6 of Keppel & Wickens (2004)."
  },
  {
    "objectID": "content/04-content.html#slides",
    "href": "content/04-content.html#slides",
    "title": "Contrasts and multiple testing",
    "section": "Slides",
    "text": "Slides\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of slide-specific commands."
  },
  {
    "objectID": "content/02-content.html",
    "href": "content/02-content.html",
    "title": "Hypothesis testing",
    "section": "",
    "text": "Sampling variability\nHypothesis testing\nPairwise comparisons"
  },
  {
    "objectID": "content/02-content.html#content",
    "href": "content/02-content.html#content",
    "title": "Hypothesis testing",
    "section": "",
    "text": "Sampling variability\nHypothesis testing\nPairwise comparisons"
  },
  {
    "objectID": "content/02-content.html#learning-objectives",
    "href": "content/02-content.html#learning-objectives",
    "title": "Hypothesis testing",
    "section": "Learning objectives",
    "text": "Learning objectives\nAt the end of the session, students should be capable of\n\nunderstanding the mechanics behind generic hypothesis tests\ninterpreting the output of generic tests\ncorrectly reporting the output of a testing procedure"
  },
  {
    "objectID": "content/02-content.html#readings",
    "href": "content/02-content.html#readings",
    "title": "Hypothesis testing",
    "section": "Readings",
    "text": "Readings\n\n Chapter 5 (Foundations for inference) of Matthew Crump’s course notes. These notes are non-technical, but do a good job at explaining the notion of sampling variability and chance. If you find them too basic, skip directly to the next item.\n Chapter 2 of the Course notes\n The permutation test by Jared Wilson"
  },
  {
    "objectID": "content/02-content.html#complementary-readings",
    "href": "content/02-content.html#complementary-readings",
    "title": "Hypothesis testing",
    "section": "Complementary readings",
    "text": "Complementary readings\n\n Chapter 2 of Maxwell et al. (2017)\n Chapter 3 of Keppel & Wickens (2004)."
  },
  {
    "objectID": "content/02-content.html#slides",
    "href": "content/02-content.html#slides",
    "title": "Hypothesis testing",
    "section": "Slides",
    "text": "Slides\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of slide-specific commands."
  },
  {
    "objectID": "content/02-content.html#case-study",
    "href": "content/02-content.html#case-study",
    "title": "Hypothesis testing",
    "section": "Case study",
    "text": "Case study\nWe will look at the way authors report the conclusion of their statistical tests with\n\nRosen & Jerdee (1974)\nBrucks & Levav (2022)\nLiu et al. (2023), Experiment 1"
  },
  {
    "objectID": "content/02-content.html#code",
    "href": "content/02-content.html#code",
    "title": "Hypothesis testing",
    "section": "Code",
    "text": "Code\nThe following code reproduces the applications in the course slides and the case study\n\nR script\nSPSS scripts\nPython script for the case study"
  },
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "Introduction",
    "section": "",
    "text": "Syllabus and learning objectives\nReview\n\nPopulation and samples\nObservational versus experimental studies\nSampling schemes: random, stratified and cluster sampling\n\nIntroduction to experimental designs\nTerminology of experimental design\nRequirements for a good experiment"
  },
  {
    "objectID": "content/01-content.html#content",
    "href": "content/01-content.html#content",
    "title": "Introduction",
    "section": "",
    "text": "Syllabus and learning objectives\nReview\n\nPopulation and samples\nObservational versus experimental studies\nSampling schemes: random, stratified and cluster sampling\n\nIntroduction to experimental designs\nTerminology of experimental design\nRequirements for a good experiment"
  },
  {
    "objectID": "content/01-content.html#learning-objectives",
    "href": "content/01-content.html#learning-objectives",
    "title": "Introduction",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nLearning the terminology associated to experiments.\nAssessing the generalizability of a study based on the consideration of the sample characteristics, sampling scheme and population.\nDistinguishing between observational and experimental studies.\nUnderstanding the rationale behind the requirements for good experimental studies."
  },
  {
    "objectID": "content/01-content.html#preliminaries",
    "href": "content/01-content.html#preliminaries",
    "title": "Introduction",
    "section": "Preliminaries",
    "text": "Preliminaries\n\nFamiliarize yourself with the syllabus, content, examples, and evaluations pages for this class.\n Read Chapter 1 (Intro to Data) of OpenIntro Statistics and the accompanying  videos"
  },
  {
    "objectID": "content/01-content.html#readings",
    "href": "content/01-content.html#readings",
    "title": "Introduction",
    "section": "Readings",
    "text": "Readings\n\n\n\n\n\n\nWarning\n\n\n\nThese readings should be completed before class, to ensure timely understanding and let us discuss the concepts together through various examples and case studies — the strict minimum being the course notes. If you feel a section is redundant and overlaps with the latter, feel free to skim through the rest of the material.\n\n\n\n Chapter 1 of the Course notes\n Chapter 2 of Introduction to Modern Statistics\n\nFeel free to inform me in the weekly check-in of your take on these references."
  },
  {
    "objectID": "content/01-content.html#complementary-readings",
    "href": "content/01-content.html#complementary-readings",
    "title": "Introduction",
    "section": "Complementary readings",
    "text": "Complementary readings\n\n\n\n\n\n\nWarning\n\n\n\nComplementary readings are additional sources of information that are not required readings, but may be useful substitutes. Sometimes, they go beyond the scope of what we cover (in more details).\n\n\n\n Chapter 1 (Preliminaries) in Planning of experiments (Cox, 1958). Out of print, but addresses the basic concepts using a variety of examples (mostly from agricultural field trials), and particularly well written.\n Chapter 1 and Sections 2.1-2.2 in Design and Analysis of Experiments (Dean et al., 2017)"
  },
  {
    "objectID": "content/01-content.html#slides",
    "href": "content/01-content.html#slides",
    "title": "Introduction",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of slide-specific commands."
  },
  {
    "objectID": "content/01-content.html#case-study",
    "href": "content/01-content.html#case-study",
    "title": "Introduction",
    "section": "Case study",
    "text": "Case study\nWe will discuss the summary of Abaluck et al. (2022) “The Impact of Mask Distribution and Promotion on Mask Uptake and COVID-19 in Bangladesh” (pdf) in class.\nDuring the activity, you will be asked to identify in teams the following:\n\nthe objective of the study\nthe target population (which findings generalize?)\nthe sampling scheme\nthe observational and experimental units\nthe treatments\nthe outcome variable(s)"
  },
  {
    "objectID": "content/03-content.html",
    "href": "content/03-content.html",
    "title": "Analysis of variance",
    "section": "",
    "text": "ANOVA and F-test statistic\nModel assumptions"
  },
  {
    "objectID": "content/03-content.html#content",
    "href": "content/03-content.html#content",
    "title": "Analysis of variance",
    "section": "",
    "text": "ANOVA and F-test statistic\nModel assumptions"
  },
  {
    "objectID": "content/03-content.html#learning-objectives",
    "href": "content/03-content.html#learning-objectives",
    "title": "Analysis of variance",
    "section": "Learning objectives",
    "text": "Learning objectives\nAt the end of the session, students should be capable of\n\ncarry-out a one-way analysis of variance test\ncheck model assumptions using hypothesis tests and graphics."
  },
  {
    "objectID": "content/03-content.html#readings",
    "href": "content/03-content.html#readings",
    "title": "Analysis of variance",
    "section": "Readings",
    "text": "Readings\n\n Chapter 3 of the Course notes"
  },
  {
    "objectID": "content/03-content.html#complementary-readings",
    "href": "content/03-content.html#complementary-readings",
    "title": "Analysis of variance",
    "section": "Complementary readings",
    "text": "Complementary readings\n\n Chapter 3 of Maxwell et al. (2017) (up to page 113).\n Chapter 2 of Meier (2022)\n Chapter 3 and 7 of Keppel & Wickens (2004)."
  },
  {
    "objectID": "content/03-content.html#slides",
    "href": "content/03-content.html#slides",
    "title": "Analysis of variance",
    "section": "Slides",
    "text": "Slides\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of slide-specific commands."
  },
  {
    "objectID": "content/03-content.html#code",
    "href": "content/03-content.html#code",
    "title": "Analysis of variance",
    "section": "Code",
    "text": "Code\n\nR script\nSPSS script"
  },
  {
    "objectID": "content/05-content.html",
    "href": "content/05-content.html",
    "title": "Complete factorial designs",
    "section": "",
    "text": "Benefits of factorial designs\nInteractions\nSimple, marginal and interaction contrasts"
  },
  {
    "objectID": "content/05-content.html#content",
    "href": "content/05-content.html#content",
    "title": "Complete factorial designs",
    "section": "",
    "text": "Benefits of factorial designs\nInteractions\nSimple, marginal and interaction contrasts"
  },
  {
    "objectID": "content/05-content.html#learning-objectives",
    "href": "content/05-content.html#learning-objectives",
    "title": "Complete factorial designs",
    "section": "Learning objectives",
    "text": "Learning objectives\nAt the end of the session, students should be capable of\n\nlisting the benefits of factorial designs\ninterpreting an interaction plot and being aware of it’s limitation\ninterpreting model effects in sum-to-zero parametrization\nreporting results from a two-way analysis of variance\ndetermining whether or not simple or main effects are most suitable"
  },
  {
    "objectID": "content/05-content.html#readings",
    "href": "content/05-content.html#readings",
    "title": "Complete factorial designs",
    "section": "Readings",
    "text": "Readings\n\n Chapter 4 of Meier (2022)\n Chapter 5 of the course notes"
  },
  {
    "objectID": "content/05-content.html#complementary-readings",
    "href": "content/05-content.html#complementary-readings",
    "title": "Complete factorial designs",
    "section": "Complementary readings",
    "text": "Complementary readings\n\n Chapter 7 of Maxwell et al. (2017).\n Chapters 10-13 of Keppel & Wickens (2004)."
  },
  {
    "objectID": "content/05-content.html#slides",
    "href": "content/05-content.html#slides",
    "title": "Complete factorial designs",
    "section": "Slides",
    "text": "Slides\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of slide-specific commands."
  },
  {
    "objectID": "content/05-content.html#code",
    "href": "content/05-content.html#code",
    "title": "Complete factorial designs",
    "section": "Code",
    "text": "Code\n\nR script\nSPSS script"
  },
  {
    "objectID": "content/07-content.html",
    "href": "content/07-content.html",
    "title": "Designs to reduce the error",
    "section": "",
    "text": "Blocking\nAnalysis of covariance"
  },
  {
    "objectID": "content/07-content.html#content",
    "href": "content/07-content.html#content",
    "title": "Designs to reduce the error",
    "section": "",
    "text": "Blocking\nAnalysis of covariance"
  },
  {
    "objectID": "content/07-content.html#learning-objectives",
    "href": "content/07-content.html#learning-objectives",
    "title": "Designs to reduce the error",
    "section": "Learning objectives",
    "text": "Learning objectives\nAt the end of the session, students should be capable of\n\nexplain the benefits of blocked design and ANCOVA over completely randomized designs\nusing control covariates or blocking factors\ntesting equality of slopes"
  },
  {
    "objectID": "content/07-content.html#readings",
    "href": "content/07-content.html#readings",
    "title": "Designs to reduce the error",
    "section": "Readings",
    "text": "Readings\n\n Chapter 6 of the course notes\n Chapter 5 (Complete Block Designs) of Meier (2022) (5.1-5.2)"
  },
  {
    "objectID": "content/07-content.html#complementary-readings",
    "href": "content/07-content.html#complementary-readings",
    "title": "Designs to reduce the error",
    "section": "Complementary readings",
    "text": "Complementary readings\n\n Chapter 9 of Maxwell et al. (2017).\n Chapter 4 of Cox (1958)."
  },
  {
    "objectID": "content/07-content.html#videos",
    "href": "content/07-content.html#videos",
    "title": "Designs to reduce the error",
    "section": "Videos",
    "text": "Videos"
  },
  {
    "objectID": "content/07-content.html#slides",
    "href": "content/07-content.html#slides",
    "title": "Designs to reduce the error",
    "section": "Slides",
    "text": "Slides\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/07-content.html#code",
    "href": "content/07-content.html#code",
    "title": "Designs to reduce the error",
    "section": "Code",
    "text": "Code\n\nR script\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of slide-specific commands."
  },
  {
    "objectID": "content/09-content.html",
    "href": "content/09-content.html",
    "title": "Repeated measures and multivariate ANOVA",
    "section": "",
    "text": "Analysis of within-subject designs\nRepeated measures and test of sphericity\nMultivariate analysis of variance"
  },
  {
    "objectID": "content/09-content.html#content",
    "href": "content/09-content.html#content",
    "title": "Repeated measures and multivariate ANOVA",
    "section": "",
    "text": "Analysis of within-subject designs\nRepeated measures and test of sphericity\nMultivariate analysis of variance"
  },
  {
    "objectID": "content/09-content.html#learning-objectives",
    "href": "content/09-content.html#learning-objectives",
    "title": "Repeated measures and multivariate ANOVA",
    "section": "Learning objectives",
    "text": "Learning objectives\nAt the end of the session, students should be capable of\n\nadequately modelling repeated measures\ncorrectly interpreting tests of sphericity using software\nexplaining the benefits and disadvantages of repeated-measure designs\ntesting model assumptions in multivariate model"
  },
  {
    "objectID": "content/09-content.html#readings",
    "href": "content/09-content.html#readings",
    "title": "Repeated measures and multivariate ANOVA",
    "section": "Readings",
    "text": "Readings\n\n Chapter 9 of the course notes"
  },
  {
    "objectID": "content/09-content.html#complementary-readings",
    "href": "content/09-content.html#complementary-readings",
    "title": "Repeated measures and multivariate ANOVA",
    "section": "Complementary readings",
    "text": "Complementary readings\n\n Zhang & Yuan (2018), pages 143–148 of the web manual\n Chapters 11–13 of Maxwell et al. (2017).\n Chapter 5.2 of Price et al. (2017)\n Chapter 16 to 18 of Keppel & Wickens (2004)"
  },
  {
    "objectID": "content/09-content.html#slides",
    "href": "content/09-content.html#slides",
    "title": "Repeated measures and multivariate ANOVA",
    "section": "Slides",
    "text": "Slides\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/09-content.html#code",
    "href": "content/09-content.html#code",
    "title": "Repeated measures and multivariate ANOVA",
    "section": "Code",
    "text": "Code\n\nR script\nSPSS script\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of slide-specific commands."
  },
  {
    "objectID": "content/11-content.html",
    "href": "content/11-content.html",
    "title": "Introduction to causal inference",
    "section": "",
    "text": "Introduction to causal inference"
  },
  {
    "objectID": "content/11-content.html#content",
    "href": "content/11-content.html#content",
    "title": "Introduction to causal inference",
    "section": "",
    "text": "Introduction to causal inference"
  },
  {
    "objectID": "content/11-content.html#learning-objectives",
    "href": "content/11-content.html#learning-objectives",
    "title": "Introduction to causal inference",
    "section": "Learning objectives",
    "text": "Learning objectives\nAt the end of the session, students should be capable of\n\nunderstanding the importance of listing potential confounders\ndetermining which variables to control for (confounders vs colliders)\ndrawing a directed acyclic graph describing the interrelation between variables\nexplaining the differences between experimental and observational studies for studying mediation"
  },
  {
    "objectID": "content/11-content.html#readings",
    "href": "content/11-content.html#readings",
    "title": "Introduction to causal inference",
    "section": "Readings",
    "text": "Readings\n\nChapter 2 of VanderWeele (2015)\n\nAndrew Heiss’ course notes on directed acyclic graphs (DAG) and types of association.\n\nCausal models (video and slides)\nPaths, doors, and adjustment (video and slides)\n\nThe structural equation modelling (SEM) approach to mediation\n\nPaper popularizing linear mediation (Baron & Kenny, 1986)\nLimitations of the linear mediation model approach (Bullock et al., 2010)\n\nThe causal inference approach\n\nPearl (2014)\nImai et al. (2010)"
  },
  {
    "objectID": "content/11-content.html#complementary-readings",
    "href": "content/11-content.html#complementary-readings",
    "title": "Introduction to causal inference",
    "section": "Complementary readings",
    "text": "Complementary readings\n\nPearl et al. (2016), Chapter 3\nRohrer (2018)"
  },
  {
    "objectID": "content/11-content.html#slides",
    "href": "content/11-content.html#slides",
    "title": "Introduction to causal inference",
    "section": "Slides",
    "text": "Slides\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of slide-specific commands."
  },
  {
    "objectID": "content/13-content.html",
    "href": "content/13-content.html",
    "title": "Nonparametric tests and count data",
    "section": "",
    "text": "Models and tests for count data\nNonparametric tests"
  },
  {
    "objectID": "content/13-content.html#content",
    "href": "content/13-content.html#content",
    "title": "Nonparametric tests and count data",
    "section": "",
    "text": "Models and tests for count data\nNonparametric tests"
  },
  {
    "objectID": "content/13-content.html#learning-objectives",
    "href": "content/13-content.html#learning-objectives",
    "title": "Nonparametric tests and count data",
    "section": "Learning objectives",
    "text": "Learning objectives\nAt the end of the session, students should be capable of\n\nUnderstanding the benefits and drawback of rank-based tests.\nPerforming tests for count data and correctly report their output."
  },
  {
    "objectID": "content/13-content.html#readings",
    "href": "content/13-content.html#readings",
    "title": "Nonparametric tests and count data",
    "section": "Readings",
    "text": "Readings\n\nChapter 12 Categorical data analysis of Navarro (2019)\nChapters 12 and 13 of the course notes"
  },
  {
    "objectID": "content/13-content.html#slides",
    "href": "content/13-content.html#slides",
    "title": "Nonparametric tests and count data",
    "section": "Slides",
    "text": "Slides\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/13-content.html#code",
    "href": "content/13-content.html#code",
    "title": "Nonparametric tests and count data",
    "section": "Code",
    "text": "Code\n\nR script\nSPSS script\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of slide-specific commands."
  },
  {
    "objectID": "evaluations/01-problem-set.html",
    "href": "evaluations/01-problem-set.html",
    "title": "Problem set 1",
    "section": "",
    "text": "Task 1: Identifying information.\nClayton (2018) considers measures of implicit bias for multiple participants from an “in-field implicit association tests”. The codebook contains a description of the data.\n\nWhat is the population under study?\nIdentify the type of sampling.\nWhat are the observational and experimental units? How many observations are there for each in the sample?\nComment on the quality of the sample and the generalizability of the findings.\nIdentify the response variable, the factor and the treatment levels.\n\nConsider next the first field experiment of Goldstein et al. (2008):\n\nBriefly describe the experimental setup\nWhat is the population of interest?\nIdentify the experimental units and the sample size.\n\n\n\nTask 2: Getting started with programming\nStarting from next week, you will need to perform statistical analyses using a suitable software of your choosing.\nYour job is to install a software of your liking. Tell me which one you chose and report back on your success! Also go through tutorials to get started with loading data, producing basic plots and computing simple descriptive statistics.\n\n\n\n\n\n\nReferences\n\nClayton, A. (2018). Do gender quotas really reduce bias? Evidence from a policy experiment in Southern Africa. Journal of Experimental Political Science, 5(3), 182--194. https://doi.org/10.1017/XPS.2018.8\n\n\nGoldstein, N. J., Cialdini, R. B., & Griskevicius, V. (2008). A room with a viewpoint: Using social norms to motivate environmental conservation in hotels. Journal of Consumer Research, 35(3), 472–482. https://doi.org/10.1086/586910"
  },
  {
    "objectID": "evaluations/03-problem-set.html",
    "href": "evaluations/03-problem-set.html",
    "title": "Problem set 3",
    "section": "",
    "text": "Submission information: please submit on ZoneCours\n\na PDF report\nyour code\n\nfollowing the naming convention PS3-studentid.extension where studentid is replaced with your student ID and extension is the file extension (e.g., .pdf, .R, .Rmd, .sps)\nInstructions: We consider data collected for Study 3 of Grossmann & Kross (2014) (click the links to download the paper and the Supplementary material). You can access these data directly from R from the hecedsm package or download the SPSS data.\nIf you work with R, check out this helper code\nHave a quick look at the paper and fit the one way analysis of variance model for the consideration of other’s perspective (persp) scores as a function of experimental condition, restricting attention to the subsample consisting of young adults (20 to 40 years).\n\nReport the \\(F\\)-test statistic, the null distribution and the \\(p\\)-value.\nProvide a conclusion in the context of the study.\nThere are missing values (see code output below). One thing to check normally is whether the lack of response is due to the treatment (for example, tasks that are more difficult or longer lead to higher dropout rate). Using the code provided below, inspect the pattern. Do you think there is a cause for concern in the present context?\n\nWe next look at model assumptions\n\nIs the independence assumption plausible in the context?\nHow many observations are there in each group (excluding missing values)? Is the number sufficient to reliably estimate the sample mean of each experimental condition and forego normality checks?\nProduce a normal quantile-quantile plot and comment on the distribution of the residuals (check out this response on CrossValidated first).\nCheck the equality of variance assumption using a suitable test statistic and report the results.\nUse Welch’s one-way analysis of variance model (oneway.test() in R) and compare the output and the conclusion with that of the usual \\(F\\)-test.\n\nHints and R snippets\nTo subset the data, you can use the following commands:\n\ndata(GK14_S3, package = \"hecedsm\")\ndb &lt;- GK14_S3 |&gt;\n    dplyr::filter(\n      !is.na(persp), # exclude missing values\n      age == \"young\") # only young adults\n\nThe following code filters out missing values for persp for young adults and shows the counts for each condition:\n\nGK14_S3 |&gt;\n  dplyr::filter(is.na(persp),      \n                age == \"young\") |&gt; \n  dplyr::group_by(condition) |&gt; \n  dplyr::summarize(count = dplyr::n()) \n\n# A tibble: 4 × 2\n  condition       count\n  &lt;fct&gt;           &lt;int&gt;\n1 self immersed       3\n2 self distanced      3\n3 other immersed      3\n4 other distanced     2\n\n\nand finally the following code chunk creates box and whiskers plots and overlays the (jittered) data.\n\nlibrary(ggplot2) # grammar of graphics\nggplot(data = db,\n       mapping = aes(x = condition,\n                     y = persp,\n                     color = condition)) +\n  geom_boxplot() +\n  geom_jitter()\n\n\n\n\n\n\n\n\nReferences\n\nGrossmann, I., & Kross, E. (2014). Exploring Solomon’s paradox: Self-distancing eliminates the self-other asymmetry in wise reasoning about close relationships in younger and older adults. Psychological Science, 25(8), 1571–1580. https://doi.org/10.1177/0956797614535400"
  },
  {
    "objectID": "evaluations/05-problem-set.html",
    "href": "evaluations/05-problem-set.html",
    "title": "Problem set 5",
    "section": "",
    "text": "Complete this task individually\nSubmission information: please submit on ZoneCours\n\na PDF report\nyour code\n\nWe consider data from Experiment 2 of Jordan et al. (2022), who measured the confidence of participants on their ability to land successfully a plane if the pilot was incapacitated, after they were exposed to a trivially uninformative 3 minute video of a pilot landing a plane, but filmed in such a way that it was utterly useless. The authors pre-registered a comparison between experimental conditions video vs no video, and found that people watching the video answered higher for the question “How confident are you that you would be able to land the plane without dying”, but there was no discernible effect for “How confident are you that you would be able to successfully land the plane as well as a pilot could”, contrary to expectations. They found that the order in which the questions were asked (order, either pilot first, or dying first) changed the response.\nThe database in package hecedsm in R is labelled JZBJG22_E2. You can also download the SPSS database via this link.\n\nA dataset is said to be balanced if there are the same number of people in each experimental condition. Are the data balanced over condition and order? Justify your answer.\nPlot the response variables using a suitable graph (e.g., half-violin or density plot), for each of the four conditions. Comment on the repartition of the scores along the scale.\nUsing Levene’s test, check whether the variance in each subgroup is the same.\nPerform the two-way analysis of variance for both conf_dying and conf_pilot and report the null and alternative hypotheses, the test statistic, the \\(P\\)-value and the conclusion of the test.\nFor each response variable conf_dying and conf_pilot, perform follow tests. If the interaction is significant, compute simple effects and 95% confidence intervals for the difference in score for video conditions. Otherwise, compute marginal effects.\n\nHint: to create a pretty plot in R, try the following:\n\n# install.packages(c(\"ggplot2\",\"ggdist\"))\n# remotes::install_github(\"lbelzile/hecedsm\")\ndata(JZBJG22_E2, package = \"hecedsm\")\nlibrary(ggplot2)\nlibrary(ggdist)\n\nggplot(data = JZBJG22_E2, \n       mapping = aes(x = condition, group = order, y = conf_dying)) + \n  ggdist::stat_halfeye(adjust = .5, width = .3, .width = c(0.5, 1)) + \n  ggdist::stat_dots(side = \"left\", dotsize = .4, justification = 1.05, binwidth = .1) +\n  facet_grid(~order) + \n  labs(y = \"\", subtitle = \"confidence in landing without dying\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nReferences\n\nJordan, K., Zajac, R., Bernstein, D., Joshi, C., & Garry, M. (2022). Trivially informative semantic context inflates people’s confidence they can perform a highly complex skill. Royal Society Open Science, 9(3), 211977. https://doi.org/10.1098/rsos.211977"
  },
  {
    "objectID": "evaluations/07-problem-set.html",
    "href": "evaluations/07-problem-set.html",
    "title": "Problem set 7",
    "section": "",
    "text": "Complete this task in teams of two or three students.\nSubmission information: please submit on ZoneCours\n\na PDF report\nyour code\n\nLook at the ANCOVA example; this should be helpful in completing the problem set.\nWe consider Experiment 3 of Stekelenburg et al. (2021); (download the paper). The database can be found in SSVB21_S3 in the hecedsm package. You can also download the SPSS database via this link.\n\nHave a quick look at the exclusion guidelines in the preregistration. Do they make sense? think about other potential criteria that could have been listed.\nWrite down potential improvement points related to statistics and experimental setup that you could raise if you were assigned to peer-review the paper.\nThe author proceed with splitting the data set in three groups and performing each pairwise comparisons in turn, corresponding to the different hypotheses (circa lines 499-533 of the script). Why might this approach be suboptimal relative to the one that consists in fitting a model to the three categories (pooled variance) and proceeding later with computing pairwise differences and contrasts?\nTest the assumptions of\n\nequal variance per experimental condition with the ANCOVA\nproper randomization based only on Prior scores\nequality of slopes\n\nIf the variance are unequal, fit the model with unequal variances per group (see the ANCOVA example) — otherwise proceed as usual.\n\nLook at the pairwise difference between between Boost+ and consensus only condition and report the results of this test.\nDoes the conclusion of the test change relative to that reported in the paper?\n\n\n\n\n\n\nReferences\n\nStekelenburg, A. van, Schaap, G., Veling, H., & Buijzen, M. (2021). Boosting understanding and identification of scientific consensus can help to correct false beliefs. Psychological Science, 32(10), 1549–1565. https://doi.org/10.1177/09567976211007788"
  },
  {
    "objectID": "evaluations/09-problem-set.html",
    "href": "evaluations/09-problem-set.html",
    "title": "Problem set 9",
    "section": "",
    "text": "Complete this task in teams of two or three students.\nSubmission information: please submit on ZoneCours\nWe consider Study 2 of Bobak et al. (2019), who collected responses for 52 participants and computed the average of the correctly identified figures as response out of 96 trials (32 per condition). Bobak et al. (2019) report that\nPerform a repeated measure one-way ANOVA with color as within-factor and pcorr as response with the BMH19_S2 data, which can be found in the R package hecedsm. You can also download the SPSS database via this link."
  },
  {
    "objectID": "evaluations/09-problem-set.html#footnotes",
    "href": "evaluations/09-problem-set.html#footnotes",
    "title": "Problem set 9",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you fit the model with afex package functions aov_ez, the output contains a slot $Anova with the value of this test.↩︎"
  },
  {
    "objectID": "evaluations/11-problem-set.html",
    "href": "evaluations/11-problem-set.html",
    "title": "Problem set 11",
    "section": "",
    "text": "Complete this task in teams of up to three students.\nSubmission information: please submit on ZoneCours"
  },
  {
    "objectID": "evaluations/11-problem-set.html#task-1",
    "href": "evaluations/11-problem-set.html#task-1",
    "title": "Problem set 11",
    "section": "Task 1",
    "text": "Task 1\nRead Rohrer et al. (2022) and write a summary."
  },
  {
    "objectID": "evaluations/11-problem-set.html#task-2",
    "href": "evaluations/11-problem-set.html#task-2",
    "title": "Problem set 11",
    "section": "Task 2",
    "text": "Task 2\nLiu et al. (2023) postulated in their Experiment 5b that the underestimation of the appreciation of initiators relative to recipients was mediated by the degree of surprise of the recipient. Their data can be obtained from LRMM23_S5b in the R package hecedsm. You can also download the SPSS database via this link.\n\nFit the linear mediation model using the PROCESS macro or the mediate function from the mediation R package\n\nuse the nonparametric bootstrap with the percentile method to get confidence intervals\ncompare the coefficients with those of Figure 1 from the paper.\n\nZhao et al. (2010) review the typology of mediation. Identify the type of mediation (complementary, competitive or indirect only) based on coefficients.\n\ncomplementary mediation when both direct and indirect effects are of the same sign and non-zero.\ncompetitive mediation when direct and indirect effects are of opposite signs.\nindirect-only mediation when the direct effect of \\(X \\to Y\\) is null, but the effect \\(X \\to M \\to Y\\) isn’t.\n\nThe paper does not discuss any of the model assumptions. List the assumptions of the linear mediation model and explain how some may fail to be valid, thus casting doubt on the conclusions drawn by Liu et al. (2023)."
  },
  {
    "objectID": "evaluations/11-problem-set.html#task-3",
    "href": "evaluations/11-problem-set.html#task-3",
    "title": "Problem set 11",
    "section": "Task 3",
    "text": "Task 3\nStudy 4 of Risen & Gilovich (2008) (pp. 297-299) perform a mediation analysis with a two-way ANOVA using the Baron & Kenny (1986) methodology.\n\nRead the description and comment on the following aspects:\n\nuse of the Baron and Kenny original testing procedure1\nthe plausibility of the causal model implied by the directed acyclic graph drawn in Figure 2.\n\nUsing the summary statistics and coefficients estimates reported, recompute Sobel’s statistic2 and the p-value and compare them with the values reported.3\nList the assumptions of the linear causal mediation model. Are there any check of these and, if so, do they support the claims of the authors?\nCan the authors successfully claim mediation considering the study uses an experimental design and randomly allocates experimental condition? Why or why not?"
  },
  {
    "objectID": "evaluations/11-problem-set.html#footnotes",
    "href": "evaluations/11-problem-set.html#footnotes",
    "title": "Problem set 11",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe latter is said to be suboptimal; explain why in your words.↩︎\nThe square of the std. error of \\(\\widehat{\\gamma}\\widehat{\\alpha}\\) is \\(\\widehat{\\gamma}^2\\mathsf{Va}(\\widehat{\\alpha}) + \\widehat{\\alpha}^2\\mathsf{Va}(\\widehat{\\gamma}) + \\mathsf{Va}(\\widehat{\\gamma})\\mathsf{Va}(\\widehat{\\alpha})\\), where \\(\\mathsf{Va}(\\widehat{\\alpha})\\) is the square of the standard error for \\(\\widehat{\\alpha}\\) in the summary table of the linear regression. Some authors may be excluding the \\(\\mathsf{Va}(\\widehat{\\gamma})\\mathsf{Va}(\\widehat{\\alpha})\\) term from the equation.↩︎\nIn R, the \\(p\\)-value for the two-sided test can be computed via 2*pnorm(abs(stat), lower.tail = FALSE), where stat is Sobel’s statistic.↩︎"
  },
  {
    "objectID": "evaluations/final-exam.html",
    "href": "evaluations/final-exam.html",
    "title": "Final examination",
    "section": "",
    "text": "The final is a 180 minutes closed-book examination covering all of the material from class.\nThe exam format is as follows:\n\nOne question on the fundamentals of experimental designs (blocking, randomization, etc.)\nOne question on reproducibility/replication crisis\nOne question on key concepts (power, effect size, multiple testing).\nOne question on advanced topics (mixed models, causal inference and mediation analysis, etc.)\nTwo data analysis questions, using completely randomized or blocked designs (between-subjects or within-subject) with multi-way analysis of variance, ANCOVA, mixed model, with specific questions on various aspects including\n\nparametrization of ANOVA\nadequate setup of model\ninteraction plots\nuse of different tests and methods\ncalculation of degrees of freedom of the model / sample size\nconclusions related to output of tests\ninterpretation of effect sizes\ncontrasts and pairwise comparisons\ntests for model assumptions and diagnostic plots\n\n\nA practice final can be downloaded here, along with the answers."
  },
  {
    "objectID": "evaluations/paper-review.html",
    "href": "evaluations/paper-review.html",
    "title": "Paper review",
    "section": "",
    "text": "You will be assigned a scientific paper with open access material (data, code, etc.) to criticize. The purpose of the project is to let you practice peer-reviewing, focusing on reproducibility and correctness of the statistical analyses therein.\nComment on the methodology, the statistical analyses and the report of the statistical results in the paper. If your selected paper has more than one study, you will be assigned to a particular one for the purpose of the project.\nYou should focus on the following elements in your review:\n\nStrengths and weaknesses of the methodology used to test the research hypotheses.\nAppropriateness of experimental design and the choice of dependent variables, control variables, treatment conditions.\nAppropriateness of statistical analyses.\nPresentation and discussion of the results (e.g., descriptive statistics, estimation of effect sizes, power calculations, etc.)\nDiscussion of the limits of the study\nPossible sources of bias\nPre-registration, reproducibility and transparency of the study. Assessment of the potential for replicability.\n\nIn addition to the material seen in class, see Wilkinson (1999) and Campion (1993) to inform your review."
  },
  {
    "objectID": "evaluations/paper-review.html#description-of-the-task",
    "href": "evaluations/paper-review.html#description-of-the-task",
    "title": "Paper review",
    "section": "",
    "text": "You will be assigned a scientific paper with open access material (data, code, etc.) to criticize. The purpose of the project is to let you practice peer-reviewing, focusing on reproducibility and correctness of the statistical analyses therein.\nComment on the methodology, the statistical analyses and the report of the statistical results in the paper. If your selected paper has more than one study, you will be assigned to a particular one for the purpose of the project.\nYou should focus on the following elements in your review:\n\nStrengths and weaknesses of the methodology used to test the research hypotheses.\nAppropriateness of experimental design and the choice of dependent variables, control variables, treatment conditions.\nAppropriateness of statistical analyses.\nPresentation and discussion of the results (e.g., descriptive statistics, estimation of effect sizes, power calculations, etc.)\nDiscussion of the limits of the study\nPossible sources of bias\nPre-registration, reproducibility and transparency of the study. Assessment of the potential for replicability.\n\nIn addition to the material seen in class, see Wilkinson (1999) and Campion (1993) to inform your review."
  },
  {
    "objectID": "evaluations/paper-review.html#deliverables",
    "href": "evaluations/paper-review.html#deliverables",
    "title": "Paper review",
    "section": "Deliverables",
    "text": "Deliverables\nA portable document file (PDF) of your critique (maximum 3 pages, excluding bibliography)"
  },
  {
    "objectID": "evaluations/paper-review.html#grading-rubric",
    "href": "evaluations/paper-review.html#grading-rubric",
    "title": "Paper review",
    "section": "Grading rubric",
    "text": "Grading rubric\n\nIdentification of strength and weaknesses: 10\nCorrectness of the interpretation of statistical results: 10\nScope: 10\nClarity of report: 10\nUsefulness of suggestions: 5\nWriting and referencing: 5"
  },
  {
    "objectID": "example/SPSS.html",
    "href": "example/SPSS.html",
    "title": "Programming with SPSS",
    "section": "",
    "text": "The databases for SPSS can be downloaded through this link.\n\nVideos\nThere’s a set of videos that walks through each section below. To make it easier for you to jump around the video examples, I cut the long video into smaller pieces and included them all in one YouTube playlist.\n\nLoading data and two-sample t-test\nPairwise contrasts\nOne-way ANOVA\nOne-way ANOVA (website example)\nLogical filters\nContrasts and post-hoc comparisons\nCreating categorical variables with if-else statements\nTwo-way ANOVA\nTwo-way ANOVA (website example)\nThree-way ANOVA (website example)\nANCOVA (website example)\nRepeated measures\nRepeated measures (within-between design)\nMANOVA\nRepeated measures using linear mixed model (Syntax)\nRepeated measures using linear mixed model (Menus)\nIncomplete mixed design\nNested random effects\nLinear mediation\n\nYou can also watch the playlist (and skip around to different sections) here:\n\n\n\n\n\n\nCode\n\nOne-way analysis of variance\nTwo-way analysis of variance\nThree-way analysis of variance\nAnalysis of covariance\nRepeated measures\nMANOVA\nRepeated measures using mixed models\nMixed models\nLinear mediation"
  },
  {
    "objectID": "example/counts.html",
    "href": "example/counts.html",
    "title": "Count data",
    "section": "",
    "text": "Many experiments can have categorical outcomes, which themselves are function of one or several experimental factors. These data can be understood in the same way as other ANOVA models. Such data are often found in papers in the form of contingency tables, giving the total count per factor combination.\nThe analogy with analysis of variance doesn’t stop there. If we have a two-factor design, the most complicated model is the saturated model which has one parameter per cell (since, with count data, we have a single count per cell). The model without the interaction should have the same proportion of observations in both row means and columns, and we can compare the difference in goodness-of-fit arising from dropping the interaction.\nIf we have more than two factors, we could pool and aggregate the counts and ignore one dimension to run a series of tests with the two-dimensional tables, amounting to marginalization. Much like in ANOVA, this only makes sense if there is no interaction between the factors. If there is an interaction, we must look at simple effects and fix the level of one factor while comparing the others.\nThe following section is an introduction to the topic, showcasing examples of tests and their application. It highlights the unifying theme of the course: statistics as summary of evidence, and decision-making in the presence of uncertainty. We use examples drawn from published articles in management sciences."
  },
  {
    "objectID": "example/counts.html#setup",
    "href": "example/counts.html#setup",
    "title": "Count data",
    "section": "Setup",
    "text": "Setup\nWe consider for simplicity a bivariate contingency table with \\(I\\) rows and \\(J\\) columns and \\(n\\) observations overall; the count in the \\((i,j)\\) entry of the matrix is \\(n_{ij}\\).\nPearson’s \\(X^2\\) goodness-of-fit test examines discrepancies between postulated proportions in each cell (with the sum of the probabilities summing to one, \\(\\sum_{i=1}^I \\sum_{j=1}^J p_{ij0}=1\\)). The test compare the expected counts \\(E_{ij}=n \\cdot p_{ij0}\\) with the observed counts \\(O_{ij}=n_{ij}\\). As summary of evidence, we take the statistic \\[ X^2 = \\sum_{i,j} \\frac{(E_{ij}-O_{ij})^2}{E_{ij}},\\] the squared difference between expected and observed counts, divided by the expected counts.\nThe contingency table described above represents a two-way factorial design on which we impose \\(IJ-1\\): the one constraint comes from the fact the overall counts must sum to \\(n\\), so one of the numbers is predetermined by others.\nRather than specify a full description.\nThe model with the two-way interaction has \\(IJ\\) parameters, one for each cell. There, the estimated proportions are simply the observed counts in each cell, divided by the overall sample size. Such model has as many parameters as observations and is said to be saturated. The first departure one can consider is thus having different marginal proportions in each row and columns, but no interaction. This hypothesis of independence between factors thus compares the model with interaction to the one without."
  },
  {
    "objectID": "example/counts.html#study-1---lee-and-choi-2019",
    "href": "example/counts.html#study-1---lee-and-choi-2019",
    "title": "Count data",
    "section": "Study 1 - Lee and Choi (2019)",
    "text": "Study 1 - Lee and Choi (2019)\nLee & Choi (2019) study the perception of consumers when faced with inconsistent descriptions of times (when the description doesn’t match the image). The dataset LC19_T2 contains the counts for the expected number of toothbrushes for each combination of image and text.\nWe compute the \\(X^2\\) test of independence between\n\nthe text description (text) and the expected number of toothbrushes (expected).\nthe image and expected number.\n\nIn R, the chisq.test function will compute the test of independence between rows and columns if you provide a matrix with the cross-counts.\n\n\nCode\ndata(LC19_T2, package = \"hecedsm\")\ncontingency_tab &lt;- \n  with(LC19_T2, \n       xtabs(count ~ text + expected))\n# Score test, chi-square (2) null\nchisq.test(contingency_tab)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_tab\nX-squared = 81.071, df = 2, p-value &lt; 2.2e-16\n\n\nWe can check that our summaries match those reported by the authors, so the results are reproducible.\nModels for count data are often obtained by specifying a Poisson distribution for the response and setting factors as explanatories. This makes it perhaps clearer what the \\(\\chi^2\\) test of independence is computing. Note that this isn’t the only statistic to compare: below, I fit both the saturated model and the model without interaction and use regular interactions to fit them. The regression model specifies that the response is counts (family=poisson).\n\n\nCode\n# Fit Poisson model\ncmod1 &lt;- glm(\n  count ~ text * expected,\n  data = LC19_T2, \n  family = poisson)\ncmod0 &lt;- glm(\n  count ~ text + expected,\n  data = hecedsm::LC19_T2, \n  family = poisson)\n# Likelihood ratio test, chi-square (2) null\ncar::Anova(cmod1, type = 3)\n\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: count\n              LR Chisq Df Pr(&gt;Chisq)    \ntext             0.182  1     0.6696    \nexpected        89.150  2     &lt;2e-16 ***\ntext:expected   88.705  2     &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# Score test - the \"chi-squared test of independence\"\nanova(cmod0, cmod1, test = \"Rao\")\n\n\nAnalysis of Deviance Table\n\nModel 1: count ~ text + expected\nModel 2: count ~ text * expected\n  Resid. Df Resid. Dev Df Deviance    Rao  Pr(&gt;Chi)    \n1         8     96.372                                 \n2         6      7.667  2   88.705 81.071 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results of the likelihood ratio test and that of the score test are slightly different, but they test the same hypothesis. Here, we reject the null hypothesis of independence between text and expected: there is indeed an interaction present.\nThe results are somehow meaningless. First, one would need to check that there is no interaction between text and image before marginalizing, which is unlikely because part of the confusion would stem from inconsistent description along with the photo: if the display is a picture of six toothbrushes and the text description is 1 pack of 1, this is confusing. If the image agrees with the text, we expect people to answer correctly.\nRather than perform the test of Lee & Choi (2019), the comparison of interest in my humble opinion is consistent (text one vs expected one has the same proportion as text 6 vs expected 6), and similarly for inconsistent. To test this, it suffices to permute entries and change the labels of the expected factor.\n\n\n\n\nTable 1: Contingency table for null hypothesis of independence looking at the correspondance between text description and expectation of customers.\n\n\n\nnot sure\nincorrect\ncorrect\n\n\n\n\n1\n10\n12\n81\n\n\n6\n12\n17\n68\n\n\n\n\n\n\nThe test statistic for this hypothesis is 2 with a \\(p\\)-value of 0.37. There is no evidence here that people have different levels of confusion if the text mentions a different quantity."
  },
  {
    "objectID": "example/counts.html#study-2---bertrand-and-mullainathan-2004",
    "href": "example/counts.html#study-2---bertrand-and-mullainathan-2004",
    "title": "Count data",
    "section": "Study 2 - Bertrand and Mullainathan (2004)",
    "text": "Study 2 - Bertrand and Mullainathan (2004)\nWhile by far the most common, there are more specialized hypothesis that can be considered with design. The following example showcases a test of symmetry for a square contingency table.\nWe consider a study from Bertrand & Mullainathan (2004), who study racial discrimination in hiring based on the consonance of applicants names. The authors created curriculum vitae for four applicants and randomly allocated them a name, either one typical of a white person or a black person. The response is a count indicating how many of the applicants were called back (out of two black and two white) depending on their origin.\nIf there was no racial discrimination (null hypothesis), we would expect the average number of times a white applicant was called back (but no black applicant) to be the same as a single black applicant (but no white). Only the entries for different numbers of call-back (either 0 vs 2, 0 vs 1 or 1 vs 2 for either race) are instructive about our question of interest. The data are reported in Table 2.\n\n\n\n\nTable 2: Contingency table for the racial discrimination in labor market.\n\n\n\nno\n1W\n2W\n\n\n\n\nno\n1103\n74\n19\n\n\n1B\n33\n46\n18\n\n\n2B\n6\n7\n17\n\n\n\n\n\n\nThe hypothesis of symmetry postulates that the proportion on either side of the diagonal are the same, so \\(p_{ij}=p_{ji}\\). Under the null hypothesis model, our best estimate of the proportion is thus \\((n_{ij} + n_{ji})/2\\), which is the sample average of those cells. The statistic is analogous to Fisher’s goodness of fit test, except that the expected counts are estimated here. There are \\(J^2\\) entries and we have \\(J(J-1)/2\\) constraints (the degrees of freedom).\nThe test statistic reduces to \\[X^2 = \\sum_{i,j} \\frac{(E_{ij}-O_{ij})^2}{E_{ij}} = \\sum_{j=1}^J \\frac{(n_{ij} - n_{ji})^2}{n_{ij}+n_{ji}}.\\] The statistic is 27.31, to be compared with a \\(\\chi^2_3\\) benchmark (three off-diagonal entries). The \\(p\\)-value is \\(5\\times 10^{-6}\\), highly suggestive of racial discrimination."
  },
  {
    "objectID": "example/index.html",
    "href": "example/index.html",
    "title": "Examples",
    "section": "",
    "text": "This section will contain annotated R code along with worked out examples. If time permits, I will also include videos of me life-coding, so you can see me making programming mistakes in real time!\nUseful resources for learning R, the tidyverse and Rmarkdown basics include\n\nThe Introduction to R and RStudio by Open Intro Stat\nTeacups, giraffes & statistics: basic statistical concepts and programming\nthe notebook RYouWithMe from R-Ladies Sydney\nthe book R for Data Science, which adheres to the tidyverse principles.\nthe R package DoSStoolkit, developped at the University of Toronto.\nthe introverse documentation.\nthe RStudio cheatsheets, also available from RStudio menu in Help &gt; Cheat Sheets\nNorman Matloff’s Fast Lane to Learning R!\n\nTo install all R packages used throughout the course, use the command\n\n\nCode\nlibs &lt;- c(\"afex\", \"car\", \"dplyr\", \"emmeans\", \"effectsize\", \n         \"ggplot2\", \"lme4\", \"lmerTest\", \"mediation\", \"nlme\", \n         \"patchwork\", \"pwr\", \"remotes\", \"tidyr\", \"WebPower\")\nfor(lib in libs){\n  if(!lib %in% installed.packages()[,\"Package\"]){\n   install.packages(lib)\n  }\n}\n# Load package containing databases\nremotes::install_github(\"lbelzile/hecedsm\")"
  },
  {
    "objectID": "example/introduction.html",
    "href": "example/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "The code created in the video can be downloaded here.\nThere’s a set of videos that walks through each section below. To make it easier for you to jump around the video examples, I cut the long video into smaller pieces and included them all in one YouTube playlist.\n\nRStudio interface\nLoading data\nCleaning and transforming data\nSummary statistics\nCreating graphics\n\nYou can also watch the playlist (and skip around to different sections) here:"
  },
  {
    "objectID": "example/introduction.html#working-directory-and-packages",
    "href": "example/introduction.html#working-directory-and-packages",
    "title": "Introduction",
    "section": "Working directory and packages",
    "text": "Working directory and packages\nIf you start your analysis in a new project, the default directory is where the project lies: getwd() will indicate where R expects to see files. You can load packages using library, provided they have been installed beforehand using install.packages(). A package is a collection of function and datasets that you can access for various tasks: you need only buy a reference textbook once (installing packages), whereas you can read of anything if you take the book off your bookshelf from there onwards (loading the package). For the most part, we will work with the tidyverse package, a simple wrapper that loads multiple libraries that adhere to common principles (tidy data analysis) and whose code more closely ressemble logical workflow and proper English instructions than base R. However, the function syntax is not set in stone and evolves over time.\nYou will sometimes encounter the double colon syntax package::function: normally, you can only access functions and objects from packages that are loaded, but to avoid disambiguation and clarity it is sometimes helpful to specify directly which package the function is from (especially if many packages have functions that go by the same name). When you load a package, it will hide existing functions in already loaded packages: thus, but if you only need to access/use a single function from a package, it may be simpler to refer to it directly."
  },
  {
    "objectID": "example/introduction.html#importing-data",
    "href": "example/introduction.html#importing-data",
    "title": "Introduction",
    "section": "Importing data",
    "text": "Importing data\nThe data used in the course has been preprocessed and cleaned and can be directly adressed by name using the command\n\n\nCode\ndata(databasename, package = 'hecedsm')\n\n\nwhere databasename is replaced by the name of one of the datasets.\nIn more general settings, however, the first task you need to undertake is to download and manipulate the data in a format that is amenable to conducing statistical analysis. This can take many forms: data sets can be found directly in R packages, downloaded directly given a web address (URL) or else downloaded into a data folder and loaded into the environment.\nIt is good practice not to manipulate the raw data using a spreadsheet software like Microsoft Excel, which is infamous for it’s awkward manipulation (the French version Office doesn’t recognize or save .csv files as being comma-separated values, strings are converted to dates and numerical values, oftentimes inconsistently, the number of columns is limited, etc. Rather, keep a copy of the raw data, a script in which you manipulate it and, if necessary, a clean and tidy copy in the database. This ensures that you do not mistakenly change records and can easily modify your data analysis if you realize that the data were incorrect at some stage due to manipulation or incorrect interpretation.\nDepending on the format (particularly if you have SAS, SPSS or Stata formatted datasets, there may be hidden labels that contain information about the variables. The haven package, part of the tidyverse, imports the metadata alongside with the observations."
  },
  {
    "objectID": "example/introduction.html#study",
    "href": "example/introduction.html#study",
    "title": "Introduction",
    "section": "Study",
    "text": "Study\nWe consider data from Baumann et al. (1992). The abstract of the paper provides a brief description of the study\n\nThis study investigated the effectiveness of explicit instruction in think aloud as a means to promote elementary students’ comprehension monitoring abilities. Sixty-six fourth-grade students were randomly assigned to one of three experimental groups: (a) a Think-Aloud (TA) group, in which students were taught various comprehension monitoring strategies for reading stories (e.g., self-questioning, prediction, retelling, rereading) through the medium of thinking aloud; (b) a Directed reading-Thinking Activity (DRTA) group, in which students were taught a predict-verify strategy for reading and responding to stories; or (c) a Directed reading Activity (DRA) group, an instructed control, in which students engaged in a noninteractive, guided reading of stories.\n\nWe have multiple columns for each of the tests (pre-intervention and post-intervention) from Baumann et al. (1992). For the time being, we focus on the first, pretest, which was used to act as control and to ensure that the random allocation of students to the three treatment groups resulted in similar average performances. This allows allows us to perform paired comparisons at a later stage by subtracting post- and pre-intervention scores to check their progress.\nThe reader is invited at this stage to look at the description of the first task (Pretest 1) and the findings presented on p.148 of Baumann et al. (1992). We aim to reproduce their results here.\n\n\nCode\n# Load packages\nlibrary(tidyverse)\n# Load data from the package\ndata(BSJ92, package = 'hecedsm')\n# Look at data to check variable type\nglimpse(BSJ92)\n\n\nRows: 66\nColumns: 6\n$ group     &lt;fct&gt; DR, DR, DR, DR, DR, DR, DR, DR, DR, DR, DR, DR, DR, DR, DR, …\n$ pretest1  &lt;int&gt; 4, 6, 9, 12, 16, 15, 14, 12, 12, 8, 13, 9, 12, 12, 12, 10, 8…\n$ pretest2  &lt;int&gt; 3, 5, 4, 6, 5, 13, 8, 7, 3, 8, 7, 2, 5, 2, 2, 10, 5, 5, 3, 4…\n$ posttest1 &lt;int&gt; 5, 9, 5, 8, 10, 9, 12, 5, 8, 7, 12, 4, 4, 8, 6, 9, 3, 5, 4, …\n$ posttest2 &lt;int&gt; 4, 5, 3, 5, 9, 8, 5, 5, 7, 7, 4, 4, 6, 8, 4, 10, 3, 5, 5, 3,…\n$ posttest3 &lt;int&gt; 41, 41, 43, 46, 46, 45, 45, 32, 33, 39, 42, 45, 39, 44, 36, …\n\n\nIn R, categorical variables are stored as objects of type factor and numerical values as &lt;dbl&gt;, which stands for double.\nWe are now ready to proceed with the data analysis."
  },
  {
    "objectID": "example/introduction.html#summary-statistics",
    "href": "example/introduction.html#summary-statistics",
    "title": "Introduction",
    "section": "Summary statistics",
    "text": "Summary statistics\nWe have seen that although they may come from the same population, the sample estimates will never coincide with the population values exactly because of sampling variability. We compute summary statistics by treatment group: for this, we use the pipe, |&gt;, which takes the object on the left and passes it on the right (this is the logical workflow), then group_by to split into separate groups and summarize to create a new tibble containing the descriptive statistics. The result is stored in summary_stats (note the assignment using the &lt;- operator) on the first line.\n\n\nCode\nsummary_stats &lt;- BSJ92 |&gt;\n  group_by(group) |&gt;\n  summarize(mean = mean(pretest1),\n            sd = sd(pretest1), # \"sd = standard deviation, i.e., sqrt(variance)\"\n            n = n()) \n\n\nWe can print the table in Rmarkdown.\n\n\nSummary statistics of pretest 1.\n\n\ngroup\nmean\nsd\nn\n\n\n\n\nDR\n10.5\n3\n22\n\n\nDRTA\n9.7\n3\n22\n\n\nTA\n9.1\n3\n22\n\n\n\n\n\n\nCheck that the summary statistics match the ones reported in the paper.\nIf you pay closer attention to the data and their description, you will note that the response variable, which represents the number of correctly identified text insertions (out of 16) is discrete and bounded above by 16 and below by zero. The averages (and maximum) are high. The article mentions that the second task posttest1 was made more difficult due to fear that student improvement would lead them to successfully detect most statements (with more people getting the maximum possible score, thus making comparisons of capabilities more difficult). One would need to rescale the scores if we wished to make meaningful comparisons between the two because the denominator differ."
  },
  {
    "objectID": "example/introduction.html#graphics",
    "href": "example/introduction.html#graphics",
    "title": "Introduction",
    "section": "Graphics",
    "text": "Graphics\nSummary statistics are useful indicators, but they potentially hide aspects of the data. It is useful to plot the data (and sometimes include graphics in articles) to show the reader features or artefacts of the latter.\nOur data here consists of a categorical variable, group, and a numerical variable, pretest1. There are multiple choices of graphics, including dot plots, box-and-whisker plots, or simply scatterplots, jittered to avoid overplotting. These can be combined using raincloud plots. Below, I show a boxplot, the data points and finally the sample average.\nWe explore below some potential choices and present a final product that is camery-ready for publication. I strongly advice that you produce graphics that are standalone: they should be interpretable once extracted from the paper with their caption.\n\n\nCode\nset.seed(2021)\n# because of jittering below, the position changes\n# every time you generate the figure\n# the above ensures that the pseudo-random numbers\n# are the same (useful for papers!)\nggplot(data = BSJ92,\n       #aesthetics: which variables to map where\n       aes(x = group, \n           y = pretest1,\n           col = group)) +\n  geom_boxplot(\n    width = 0.2, # change width\n    position = position_nudge(x = -0.2, y = 0)) + \n  # offset to the left\n  # add rugs for observations\n  # move to right of boxplot\n  geom_point(shape = 95, # a horizontal bar\n             size = 4, # size of bar\n             alpha = 0.5, # transparency 1=opaque, 0=transparent\n             position = position_nudge(x = 0.2, y = 0)) +\n  # add averages\n  geom_point(data = summary_stats,\n             aes(y = mean),\n             shape = 4, # 4 is cross (x)\n             size = 2,# double size of point\n             position = position_nudge(x = -0.2, y = 0)) + \n  # add meaningful labels\n  labs(x = \"learning strategy\",\n       y = \"\", # put in subtitle (no head tilting)\n       title = \"Pre-test scores\",\n       subtitle = \"Number of sentences that don't belong to the story found (out of 16).\",\n       caption = \"Group averages are indicated with 'x'.\") + \n  theme_minimal() + # change theme (color of background, etc.)\n  theme(legend.position = \"none\") #remove legend\n\n\n\n\n\nResults of pretest 1 based on treatment allocation."
  },
  {
    "objectID": "example/manova.html",
    "href": "example/manova.html",
    "title": "Multivariate analysis of variance",
    "section": "",
    "text": "The data presented in this example vignette is inspired by a study from Anandarajan et al. (2002), who looked at the impact of communication means through different disclosure format on the perceived risk of organization on the brink of bankruptcy in accountancy. There is a single between-subject factor for the disclore format, and three measures of the performance, ratings for the interest rate premium assessed, for the ability to service debt and that to improve profitability."
  },
  {
    "objectID": "example/manova.html#data",
    "href": "example/manova.html#data",
    "title": "Multivariate analysis of variance",
    "section": "Data",
    "text": "Data\nWe first load the data from the package and inspect the content.\n\n\nCode\ndata(AVC02, package = \"hecedsm\")\nstr(AVC02)\n\n\nClasses 'tbl_df', 'tbl' and 'data.frame':   132 obs. of  4 variables:\n $ format       : Factor w/ 3 levels \"integrated note\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ prime        : num  0.5 1.5 0.5 0.25 1.5 1.25 2 0.5 1.5 0.5 ...\n $ debt         : int  3 4 1 4 3 3 3 5 2 2 ...\n $ profitability: int  3 2 2 5 2 2 2 3 2 3 ...\n\n\nCode\nxtabs(~ format, data = AVC02)\n\n\nformat\n        integrated note        stand-alone note modified auditor report \n                     40                      45                      47 \n\n\nThe data are unbalanced by condition. In general, we need them to be roughly balanced. The manova function will not be usable and we need to enforce sum-to-zero constraints to get sensible outputs, otherwise answers will be off."
  },
  {
    "objectID": "example/manova.html#model-fitting",
    "href": "example/manova.html#model-fitting",
    "title": "Multivariate analysis of variance",
    "section": "Model fitting",
    "text": "Model fitting\nAfter having done so, we can fit the multivariate linear model with lm by binding columns on the left of the ~ sign to gather the response vectors.\n\n\nCode\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\nmodel &lt;- lm(cbind(prime, debt, profitability) ~ format, \n            data = AVC02)\n\n\nWe can check the residual correlation matrix to see if there was a strong dependence between our measurements. The benefit of MANOVA is to be able to leverage this correlation, if any.\n\n\nCode\ncor(resid(model))\n\n\n\n\n              prime  debt profitability\nprime          1.00 -0.40         -0.54\ndebt          -0.40  1.00          0.65\nprofitability -0.54  0.65          1.00\n\n\nWe can look at the global mean for each variable and the estimated mean differences for all groups, including the reference which is omitted. It’s easy to see that the mean differences sum to zero.\n\n\nCode\ndummy.coef(model)\n\n\nFull coefficients are \n                                                               \n(Intercept):             prime        1.280511                 \n                          debt        2.881521                 \n                 profitability        2.597695                 \nformat:                        integrated note stand-alone note\n                         prime    -0.099261229     -0.013844563\n                          debt     0.068479117     -0.059298660\n                 profitability     0.127304965      0.002304965\n                                       \n(Intercept):                           \n                                       \n                                       \nformat:         modified auditor report\n                            0.113105792\n                           -0.009180457\n                           -0.129609929"
  },
  {
    "objectID": "example/manova.html#multivariate-analysis-of-variance",
    "href": "example/manova.html#multivariate-analysis-of-variance",
    "title": "Multivariate analysis of variance",
    "section": "Multivariate analysis of variance",
    "text": "Multivariate analysis of variance\nNext, we compute the multivariate analysis of variance table and the follow-up with the univariate functions. By default, we can add a multiplicity correction for the tests, using Holm-Bonferonni with option 'holm'. For the MANOVA test, there are multiple statistics to pick from, including Pillai, Wilks, Hotelling-Lawley and Roy. The default is Pillai, which is more robust to departures from the model hypothesis, but Wilks is also popular choice among practitioners.\n\n\nCode\ncar::Manova(model, test = \"Pillai\")\n\n\n\nType II MANOVA Tests: Pillai test statistic\n       Df test stat approx F num Df den Df Pr(&gt;F)\nformat  2   0.02581  0.55782      6    256 0.7637"
  },
  {
    "objectID": "example/manova.html#effect-sizes",
    "href": "example/manova.html#effect-sizes",
    "title": "Multivariate analysis of variance",
    "section": "Effect sizes",
    "text": "Effect sizes\nWe can compute effect sizes overall for the MANOVA statistic using the correspondance with the \\(F\\) distribution, and also the individual effect size variable per variable. Here, the values returned are partial \\(\\widehat{\\eta}^2\\) measures.\n\n\nCode\neffectsize::eta_squared(car::Manova(model))\n\n\n# Effect Size for ANOVA (Type II)\n\nParameter | Eta2 (partial) |       95% CI\n-----------------------------------------\nformat    |           0.01 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nCode\n# Since it's a one-way between-subject MANOVA, no partial measure\neffectsize::eta_squared(model, partial = FALSE)\n\n\n# Effect Size for ANOVA (Type I)\n\nResponse      | Parameter |     Eta2 |       95% CI\n---------------------------------------------------\nprime         |    format |     0.01 | [0.00, 1.00]\ndebt          |    format | 2.23e-03 | [0.00, 1.00]\nprofitability |    format |     0.02 | [0.00, 1.00]"
  },
  {
    "objectID": "example/manova.html#post-hoc-analyses",
    "href": "example/manova.html#post-hoc-analyses",
    "title": "Multivariate analysis of variance",
    "section": "Post-hoc analyses",
    "text": "Post-hoc analyses\nWe can continue with descriptive discriminant analysis for the post-hoc comparisons. To fit the model using the lda function from the MASS package, we swap the role of the experimental factor and responses in the formula. The output shows the weights for the linear combinations.\n\n\nCode\nMASS::lda(format ~ prime + debt + profitability,\n          data = AVC02)\n\n\nCall:\nlda(format ~ prime + debt + profitability, data = AVC02)\n\nPrior probabilities of groups:\n        integrated note        stand-alone note modified auditor report \n              0.3030303               0.3409091               0.3560606 \n\nGroup means:\n                           prime     debt profitability\nintegrated note         1.181250 2.950000      2.725000\nstand-alone note        1.266667 2.822222      2.600000\nmodified auditor report 1.393617 2.872340      2.468085\n\nCoefficients of linear discriminants:\n                     LD1         LD2\nprime          0.5171202  0.16521149\ndebt           0.6529450  0.97286350\nprofitability -1.2016530 -0.04412713\n\nProportion of trace:\n   LD1    LD2 \n0.9257 0.0743 \n\n\nInterpretation of these is beyond the scope of the course, but you can find information about linear discriminant analysis in good textbooks (TODO add reference)."
  },
  {
    "objectID": "example/manova.html#model-assumptions",
    "href": "example/manova.html#model-assumptions",
    "title": "Multivariate analysis of variance",
    "section": "Model assumptions",
    "text": "Model assumptions\nThe next step before writing about any of our conclusions is to check the model assumptions. As before, we could check for each variable in turn whether the variance are the same in each group. Here, we rather check equality of covariance matrix. The test has typically limited power, but unfortunately is very sensitive to departure from the multivariate normality assumption, so sometimes rejection are false positive.\n\n\nCode\nwith(AVC02,\nbiotools::boxM(cbind(prime, debt, profitability),\n               grouping = format))\n\n\n\n    Box's M-test for Homogeneity of Covariance Matrices\n\ndata:  cbind(prime, debt, profitability)\nChi-Sq (approx.) = 21.274, df = 12, p-value = 0.0465\n\n\nHere, there is a smallish \\(p\\)-value, so weak evidence against equality of covariance matrices. The data were generated from normal distribution, but the small \\(p\\)-value is an artifact of the rounding of the Likert scale.\nWe can test the normality assumption using univariate quantile-quantile plots or tests of normality, including Shapiro-Wilks.\n\n\nCode\npar(mfrow = c(1,3)) # one row, 3 columns for plot layout\ncar::qqPlot(rstudent(model)[,1], \n            id = FALSE, # don't flag outliers\n            ylab = \"studentized residuals (prime)\")\ncar::qqPlot(rstudent(model)[,2], \n            id = FALSE, \n            ylab = \"studentized residuals (debt)\")\ncar::qqPlot(rstudent(model)[,3], \n            id = FALSE, \n            ylab = \"studentized residuals (profitability)\")\n\n\n\n\n\nWe see severe rounding impacts for debt and profitability. There is little to be done about this, but the sample size are large enough that this shouldn’t be too much a concern.\nWe can also test the multivariate normality assumption. The latter supposes that observations in each group have the same mean. To get this, we detrend using multivariate linear model by subtracting the mean of each group. Thus, our input is the matrix of residuals, which must be transposed for the function to be happy.\n\n\nCode\nmvnormtest::mshapiro.test(U = t(resid(model)))\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Z\nW = 0.96982, p-value = 0.004899\n\n\nThere is (unsurprisingly) strong evidence against multivariate normality, but it matters less due to sample size. This is a consequence of the discrete univariate measurements, which explain rejection of the null (for data to be multivariate normal, each of the response must be univariate normal and the dependence structure must also match.\nSince assumptions are doubtful, we recommend using Pilai’s trace as test statistic for the MANOVA."
  },
  {
    "objectID": "example/moderatedmediation.html",
    "href": "example/moderatedmediation.html",
    "title": "Moderated mediation",
    "section": "",
    "text": "This example from Roczniewska & Higgins (2019) is an instance of moderated mediation model with two postulated mediators. While such models can be fitted with the PROCESS macros (Hayes, 2022), we illustrate below the postulated causal diagram, the nonparametric bootstrap. We pay particular attention to verification of the linear model assumptions.\nThe context of the research is a natural experiment (or quasi-experiment): the authors study changes in behaviour due to modifications in the schooling system.\nThe statistical model in Roczniewska & Higgins (2019) can be implicitly deduced from Table 2: the manipulated variable is the message framing and there are three postulated mediators (message fluency, message engagement, process fairness), the latter of which interacts or moderate with the change in outcome, and two moderators assumed to impact both mediators through linear interactions, termed promotion focus and prevention focus.\nThe mean response from the linear models are\n\\[\n\\begin{align}\n\\mathsf{E}(\\texttt{PF}) &= \\alpha_0 + \\alpha_1 \\texttt{Fram} + \\alpha_2\\texttt{Prom} + \\alpha_3 \\texttt{Prev}\\\\&\\quad  + \\alpha_4 \\texttt{Fram}\\times\\texttt{Prom} +  \\alpha_5 \\texttt{Fram}\\times\\texttt{Prev}\n\\end{align}\n\\tag{1}\\]\n\\[\n\\begin{align}\n\\mathsf{E}(\\texttt{ME}) &= \\beta_0 + \\beta_1 \\texttt{Fram} + \\beta_2\\texttt{Prom} + \\beta_3 \\texttt{Prev}\\\\&\\quad  + \\beta_4 \\texttt{Fram}\\times\\texttt{Prom} +  \\beta_5 \\texttt{Fram}\\times\\texttt{Prev}\n\\end{align}\n\\tag{2}\\]\n\\[\n\\begin{align}\n\\mathsf{E}(\\texttt{MF}) &= \\alpha_0 + \\alpha_1 \\texttt{Fram} + \\alpha_2\\texttt{Prom} + \\alpha_3 \\texttt{Prev}\\\\&\\quad  + \\alpha_4 \\texttt{Fram}\\times\\texttt{Prom} +  \\alpha_5 \\texttt{Fram}\\times\\texttt{Prev}\n\\end{align}\n\\tag{3}\\]\n\\[\n\\begin{align}\n\\mathsf{E}(\\texttt{Open}) &= \\gamma_0 + \\gamma_1 \\texttt{Fram} + \\gamma_2\\texttt{Prom} + \\gamma_3 \\texttt{Prev}\\\\&\\quad  + \\gamma_4 \\texttt{Fram}\\times\\texttt{Prom} +  \\gamma_5 \\texttt{Fram}\\times\\texttt{Prev} \\\\& +\\gamma_6 \\texttt{PF} + \\gamma_7\\texttt{ME} + \\gamma_8 \\texttt{MF} +  \\gamma_9\\texttt{CO} \\\\&\\quad+ \\gamma_{10} \\texttt{ME} \\times \\texttt{CO} +\n+ \\gamma_{11} \\texttt{PF} \\times \\texttt{CO} +\n+ \\gamma_{12} \\texttt{MF} \\times \\texttt{CO}\n\\end{align}\n\\tag{4}\\]\nyielding a total of 31 coefficients for \\(n=198\\) observations.\n\n\nCode\ndata(RH19_S1, package = \"hecedsm\")\nset.seed(80667)\njoint_reg &lt;- function(data, indices = 1:nrow(data)){\n  mod1 &lt;- lm(fairness ~ \n               regfocus * (promem + prevem), \n             data = data[indices,])\n  mod2 &lt;- lm(engagement ~ \n               regfocus * (promem + prevem), \n             data = data[indices,])\n  mod3 &lt;- lm(fluency ~ \n               regfocus * (promem + prevem), \n             data = data[indices,])\n  resp &lt;- lm(dopenness ~\n               regfocus * (promem + prevem) +\n               (fairness + fluency + \n       engagement) * change,\n             data = data[indices,])\n  coefs &lt;- c(coef(mod1), \n             coef(mod2), \n             coef(mod3), \n             coef(resp))\n  return(coefs)\n}\nnpboot &lt;- boot::boot(\n  data = RH19_S1, \n  statistic = joint_reg,\n  R = 9999, # number of bootstrap replicates\n  sim = \"ordinary\")\nresults &lt;- rbind(\n  npboot$t0, \n  apply(npboot$t, 2, \n        quantile, c(0.025, 0.975)))\n\n\n\n\n\nCode\ntestImplications &lt;- function( covariance.matrix, sample.size ){\n    library(ggm)\n    tst &lt;- function(i){ pcor.test( pcor(i,covariance.matrix), length(i)-2, sample.size )$pvalue }\ntos &lt;- function(i){ paste(i,collapse=\" \") }\nimplications &lt;- list(c(\"M1\",\"M2\",\"V1\",\"V2\",\"X\"),\n        c(\"M1\",\"M3\",\"V1\",\"V2\",\"X\"),\n        c(\"M1\",\"W\"),\n        c(\"M2\",\"M3\",\"V1\",\"V2\",\"X\"),\n        c(\"M2\",\"W\"),\n        c(\"M3\",\"W\"),\n        c(\"V1\",\"V2\"),\n        c(\"V1\",\"W\"),\n        c(\"V1\",\"X\"),\n        c(\"V2\",\"W\"),\n        c(\"V2\",\"X\"),\n        c(\"W\",\"X\"))\n    data.frame( implication=unlist(lapply(implications,tos)),\n        pvalue=unlist( lapply( implications, tst ) ) )\n\n}\n\n\n\n\n\n\n\n\nReferences\n\nHayes, A. F. (2022). Introduction to mediation, moderation, and conditional process analysis: A regression-based approach (3rd ed.). Guilford Press.\n\n\nRoczniewska, M., & Higgins, E. T. (2019). Messaging organizational change: How regulatory fit relates to openness to change through fairness perceptions. Journal of Experimental Social Psychology, 85, 103882. https://doi.org/10.1016/j.jesp.2019.103882"
  },
  {
    "objectID": "example/onewayanova.html",
    "href": "example/onewayanova.html",
    "title": "One-way analysis of variance",
    "section": "",
    "text": "The R code created in the video can be downloaded here and the SPSS code here.\nThere’s a set of videos that walks through each section below. To make it easier for you to jump around the video examples, I cut the long video into smaller pieces and included them all in one YouTube playlist.\n\nANOVA table\nContrasts and estimated marginal means\nMultiple testing\n\nYou can also watch the playlist (and skip around to different sections) here:"
  },
  {
    "objectID": "example/onewayanova.html#hypothesis-testing",
    "href": "example/onewayanova.html#hypothesis-testing",
    "title": "One-way analysis of variance",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nWe can begin by testing whether the group average for the initial measurements at the beginning of the study, prior to any treatment) have the same mean. Strong indication against this null hypothesis would be evidence of a potential problem with the randomization. We compute the one-way analysis of variance table, which includes quantities that enter the F-statistic (named after its large-sample null distribution, which is an F-distribution).1\nWe use the lm function function to fit the model: an analysis of variance is a special case of linear model, in which the explanatory variables are categorical variables. The first argument of the function is a formula response ~ treatment, where treatment is the factor or categorical variable indicating the treatment.\nThe function anova is a method: when applied to the result of a call to lm, it produces an analysis of variance table including among other things the following information:\n\nthe value of the test statistic (F value)\nthe between and within sum of square (these are quantities that enter in the formula of the statistic)\nthe degrees of freedom of the F null distribution (column Df): these specify the parameters of the large-sample approximation for the null distribution, which is our default benchmark.\nthe mean square, which are sum of squares divided by the degrees of freedom.\nThe p-value (Pr(&gt;F)), which gives the probability of observing an outcome as extreme if there was no difference.\n\nWe need to decide beforehand the level of the test (typically 5% or lower): this is the percentage of times we will reject the null hypothesis when its true based on observing an extreme outcome. We are asked to perform a binary decision (reject or fail to reject): if the p-value is less than the level, we ‘reject’ the null hypothesis of equal (population) means.\n\n\nCode\ndata(BSJ92, package = 'hecedsm')\nmod_pre &lt;- aov(formula = pretest1 ~ group,\n                     data = BSJ92)\nanova_tab &lt;- broom::tidy(anova(mod_pre))\n# Save the output in a tibble to get more meaningful column names\n# Elements include `statistic`, `df`, `p.value`\n\n\n\n\nAnalysis of variance table for pre-test 1\n\n\nTerms\nDegrees of freedom\nSum of squares\nMean square\nStatistic\np-value\n\n\n\n\ngroup\n2\n20.58\n10.288\n1.13\n0.33\n\n\nResiduals\n63\n572.45\n9.087\n\n\n\n\n\n\n\n\nThere isn’t strong evidence of difference in strength between groups prior to intervention. We can report the findings as follows:\nWe carried a one-way analysis for the pre-test results to ensure that the group abilities are the same in each treatment group; results show no significant differences at the 5% level (\\(F\\) (2, 63) = 1.13, \\(p\\) = 0.329).\nA similar result for the scores of the first post-test as response variable lead to strong evidence of difference between teaching methods.\n\n\nAnalysis of variance table for post-test 1\n\n\nTerms\nDegrees of freedom\nSum of squares\nMean square\nStatistic\np-value\n\n\n\n\ngroup\n2\n108.12\n54.061\n5.32\n0.01\n\n\nResiduals\n63\n640.50\n10.167"
  },
  {
    "objectID": "example/onewayanova.html#contrasts-and-estimated-marginal-means",
    "href": "example/onewayanova.html#contrasts-and-estimated-marginal-means",
    "title": "One-way analysis of variance",
    "section": "Contrasts and estimated marginal means",
    "text": "Contrasts and estimated marginal means\nWhile the \\(F\\) test may strongly indicate that the means of each group are different, it doesn’t indicate which group is different from the rest. Because we can compare different groups doesn’t mean these comparisons are of any scientific interest and going fishing by looking at all pairwise differences is not necessarily the best strategy.\n\n\nCode\nlibrary(emmeans) #load package\nmod_post &lt;- aov(posttest1 ~ group, data = BSJ92)\nemmeans_post &lt;- emmeans(object = mod_post, \n                        specs = \"group\")\n\n\n\n\n\nEstimated group averages with standard errors and 95% confidence intervals for post-test 1.\n\n\nTerms\nMarginal mean\nStandard error\nDegrees of freedom\nLower limit (CI)\nUpper limit (CI)\n\n\n\n\nDR\n6.68\n0.68\n63\n5.32\n8.04\n\n\nDRTA\n9.77\n0.68\n63\n8.41\n11.13\n\n\nTA\n7.77\n0.68\n63\n6.41\n9.13\n\n\n\n\n\n\n\nThus, we can see that DRTA has the highest average, followed by TA and directed reading (DR). The purpose of Baumann et al. (1992) was to make a particular comparison between treatment groups. From the abstract:\n\nThe primary quantitative analyses involved two planned orthogonal contrasts—effect of instruction (TA + DRTA vs. 2 x DRA) and intensity of instruction (TA vs. DRTA)—for three whole-sample dependent measures: (a) an error detection test, (b) a comprehension monitoring questionnaire, and (c) a modified cloze test.\n\nA contrast is a particular linear combination of the different groups, i.e., a sum of weighted mean the coefficients of which sum to zero. To test the hypothesis of Baumann et al. (1992) and writing \\(\\mu\\) to denote the population average, we have \\(\\mathscr{H}_0: \\mu_{\\mathrm{TA}} + \\mu_{\\mathrm{DRTA}} = 2 \\mu_{\\mathrm{DRA}}\\) or rewritten slightly \\[\\begin{align*}\n\\mathscr{H}_0: - 2 \\mu_{\\mathrm{DR}} + \\mu_{\\mathrm{DRTA}} + \\mu_{\\mathrm{TA}} = 0.\n\\end{align*}\\] with weights \\((-2, 1, 1)\\); the order of the levels for the treatment are (\\(\\mathrm{DRA}\\), \\(\\mathrm{DRTA}\\), \\(\\mathrm{TA}\\)) and it must match that of the coefficients. An equivalent formulation is \\((2, -1, -1)\\) or \\((1, -1/2, -1/2)\\): in either case, the estimated differences will be different (up to a constant multiple or a sign change). The vector of weights for \\(\\mathscr{H}_0: \\mu_{\\mathrm{TA}} = \\mu_{\\mathrm{DRTA}}\\) is, e.g.,(\\(0\\), \\(-1\\), \\(1\\)): the zero appears because the first component, \\(\\mathrm{DRA}\\) doesn’t appear. The two contrasts are orthogonal: these contrasts are special because the tests use disjoint bits of information about the sample.2\n\n\nCode\n# Identify the order of the level of the variables\nwith(BSJ92, levels(group))\n\n\n[1] \"DR\"   \"DRTA\" \"TA\"  \n\n\nCode\n# DR, DRTA, TA (alphabetical)\ncontrasts_list &lt;- list(\n  \"C1: DRTA+TA vs 2DR\" = c(-2, 1, 1), \n  # Contrasts: linear combination of means, coefficients sum to zero\n  # 2xDR = DRTA + TA =&gt; -2*DR + 1*DRTA + 1*TA = 0 and -2+1+1 = 0\n  \"C1: average (DRTA+TA) vs DR\" = c(-1, 0.5, 0.5), \n  #same thing, but halved so in terms of average\n  \"C2: DRTA vs TA\" = c(0, 1, -1),\n  \"C2: TA vs DRTA\" = c(0, -1, 1) \n  # same, but sign flipped\n)\ncontrasts_post &lt;- \n  contrast(object = emmeans_post,\n           method = contrasts_list)\ncontrasts_summary_post &lt;- summary(contrasts_post)\n\n\n\n\n\nEstimated contrasts for post-test 1.\n\n\nContrast\nEstimate\nStandard error\nDegrees of freedom\nt statistic\np-value\n\n\n\n\nC1: DRTA+TA vs 2DR\n4.18\n1.67\n63\n2.51\n0.01\n\n\nC1: average (DRTA+TA) vs DR\n2.09\n0.83\n63\n2.51\n0.01\n\n\nC2: DRTA vs TA\n2.00\n0.96\n63\n2.08\n0.04\n\n\nC2: TA vs DRTA\n-2.00\n0.96\n63\n-2.08\n0.04\n\n\n\n\n\n\n\nWe can look at these differences; since DRTA versus TA is a pairwise difference, we could have obtained the \\(t\\)-statistic directly from the pairwise contrasts using pairs(emmeans_post). Note that the two different ways of writing the comparison between DR and the average of the other two methods yield different point estimates, but same inference (same \\(p\\)-values). For contrast \\(C_{1b}\\), we get half the estimate (but the standard error is also halved) and likewise for the second contrasts we get an estimate of \\(\\mu_{\\mathrm{DRTA}} - \\mu_{\\mathrm{TA}}\\) in the first case (\\(C_2\\)) and \\(\\mu_{\\mathrm{TA}} - \\mu_{\\mathrm{DRTA}}\\): the difference in group averages is the same up to sign.\nWhat is the conclusion of our analysis of contrasts? It looks like the methods involving teaching aloud have a strong impact on reading comprehension relative to only directed reading. The evidence is not as strong when we compare the method that combines directed reading-thinking activity and thinking aloud."
  },
  {
    "objectID": "example/onewayanova.html#multiple-testing",
    "href": "example/onewayanova.html#multiple-testing",
    "title": "One-way analysis of variance",
    "section": "Multiple testing",
    "text": "Multiple testing\nIn this example, we computed two contrasts (excluding the equivalent formulations) so since these comparisons are planned, we could provide the \\(p\\)-values as is. However, if we had computed many more tests, it would make sense to account for these so as not to inflate type I error (judicial mistake consisting in sending an innocent to jail).\n\n\nCode\ncontrasts_list &lt;- list(\n  \"C1: DRTA+TA vs 2DR\" = c(-2, 1, 1), \n  \"C2: DRTA vs TA\" = c(0, 1, -1)\n)\ncontrasts_post_scheffe &lt;- \n  contrast(object = emmeans_post,\n           method = contrasts_list,\n           adjust = \"scheffe\") # for arbitrary contrasts\n# extract p-values\npvals_scheffe &lt;- summary(contrasts_post_scheffe)$p.value\npvals_scheffe\n\n\n[1] 0.04951625 0.12333575\n\n\nCode\n# Compute Bonferroni and Holm-Bonferroni correction\ncontrasts_post &lt;- \n  contrast(object = emmeans_post,\n           method = contrasts_list,\n           adjust = \"none\") #default for custom contrasts\nraw_pval &lt;- summary(contrasts_post)$p.value\np.adjust(p = raw_pval, method = \"bonferroni\")\n\n\n[1] 0.02920552 0.08313211\n\n\nCode\np.adjust(p = raw_pval, method = \"holm\") #Bonferroni-Holm method\n\n\n[1] 0.02920552 0.04156606\n\n\nIf we look at the p-values with the Scheffé’s method for custom contrasts, we get 0.015 for contrast 1 and 0.042 for contrast 2: since we are only making two tests here, these are much bigger than the \\(p\\)-values from Holm’s method which are 0.03 and 0.04. To try and avoid making type I error, we need to be more stringent to decide on rejection and this translates into bigger \\(p\\)-values, so lower power to detect. Try to use the less stringent method that still controls for the family wise error rate to preserve your power!"
  },
  {
    "objectID": "example/onewayanova.html#model-assumptions",
    "href": "example/onewayanova.html#model-assumptions",
    "title": "One-way analysis of variance",
    "section": "Model assumptions",
    "text": "Model assumptions\nSince we have no repeated measurements and there were no detectable difference apriori between students, we can postulate that the records are independent.\nWe could test whether the variance are equal: in this case, there is limited evidence of unequal variance. The data are of course not normal (because they consist of the counts of the number of insertions detected by pupils, which are integer-valued). However, we can see if there are extreme values and whether the residuals are far from normal. The simulated quantile-quantile plot shows that all points more or less align with the straight line and all fall within the confidence intervals, so there is no problem with this normality assumption (which anyway matters little).\nAre measurements additive? After assigning the pre-test 1, the experimenters adjusted the scale and made the post-test harder to avoid having maximum scores (considering that students also were more experienced).\n\nBecause the students performed at a higher-than-expected level on Pretest 1 (61% of all intruded sentences were correctly identified), the experimenters were concerned about a potential post-intervention ceiling effect on this posttest, an occurrence which could mask group differences. To reduce the likelihood of a ceiling effect, the intruded sentences for Posttest 1 were written so their inconsistencies were more subtle than those included in Pretest 1, which explains the somewhat lower level of performance on Posttest 1 (51% of all intruded sentences correctly identified).\n\nThis seems to have been successful since the maximum score is 15 out of 16 intrusions, while there were two students who scored 16 on the pre-test.\nThe next step is checking that the variability is the same in each group. Assuming equal variance is convenient because we can use more information (the whole sample) to estimate the level of uncertainty rather than solely the observations from each group. The more observations we use to estimate the variance, the more reliable our measure is (assuming that the variance were equal in each group to begin with).\n\n\nCode\n# test for equality of variance\ncar::leveneTest(posttest1 ~ group, data = BSJ92)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  2  2.1297 0.1273\n      63               \n\n\nCode\n# Quantile-quantile plot\ncar::qqPlot(x = mod_post, # lm object\n            ylab = 'empirical quantiles', # change y-axis label\n            id = FALSE) # Don't print to console 'outlying' observations\n\n\n\n\n\nCode\n# Residual plot (linearity, but useless for one way ANOVA)\ncar::residualPlot(mod_post)\n\n\n\n\n\nIf we were worried about the possibility of unequal variances, we could fit the model by estimating the variance separately in each group. This does not materially change the conclusions about teaching methods relative to the directed reading benchmark.\n\n\nCode\noneway.test(posttest1 ~ group, data = BSJ92)\n\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  posttest1 and group\nF = 6.9878, num df = 2.00, denom df = 41.13, p-value = 0.00244"
  },
  {
    "objectID": "example/onewayanova.html#auxiliary-and-concomitant-observation",
    "href": "example/onewayanova.html#auxiliary-and-concomitant-observation",
    "title": "One-way analysis of variance",
    "section": "Auxiliary and concomitant observation",
    "text": "Auxiliary and concomitant observation\nThe purpose of a good experimental design is to reduce the variability to better detect treatment effects. In the above example, we could have added a concomitant variable (the pre-test score) that captures the individual variability. This amounts to doing a paired comparison between post- and pre-test results. It helps with the analysis because it absorbs the baseline strength of individual students: by subtracting their records, we get their individual average out of the equation and thus there is less variability.\n\n\nCode\nanova_post_c &lt;- lm(posttest1 ~ offset(pretest1) + group,\n                   data = BSJ92) \nanova_tab_c &lt;- broom::tidy(anova(anova_post_c)) #anova table\n\n\nCompare this ANOVA table with the preceding. We could repeat the same procedure to compute the contrasts.\nUsing auxiliary information allows one to reduce the intrinsic variability: the estimated variance \\(\\widehat{\\sigma}^2\\) is 6.66 with the auxiliary information and 9.09 without: since we reduce the level of background noise, we get a higher signal-to-noise ratio. As a result, the p-value for the global test is smaller than with only posttest1 as response.\n\n\n\nAnalysis of variance table\n\n\nTerms\nDegrees of freedom\nSum of squares\nMean square\nStatistic\np-value\n\n\n\n\ngroup\n2\n168.21\n84.106\n12.64\n0\n\n\nResiduals\n63\n419.32\n6.656"
  },
  {
    "objectID": "example/onewayanova.html#footnotes",
    "href": "example/onewayanova.html#footnotes",
    "title": "One-way analysis of variance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe F stands for Fisher, who pioneered much of the work on experimental design.↩︎\nThis technical term means that the two vectors defining the contrasts are orthogonal: their inner product is thus zero: \\((-2 \\times 0) + (1 \\times -1) + (1 \\times 1) = 0\\). In practice, we specify contrasts because they answer questions of scientific interest, not because of their fancy mathematical properties.↩︎"
  },
  {
    "objectID": "example/repeated.html",
    "href": "example/repeated.html",
    "title": "Repeated measure ANOVA",
    "section": "",
    "text": "The following demonstration explains how to compute repeated measures ANOVA that include within-subject factors. Download the R code or the SPSS code. Such models can also be fitted using linear mixed models, see the R code or the SPSS code for a demonstration."
  },
  {
    "objectID": "example/repeated.html#data",
    "href": "example/repeated.html#data",
    "title": "Repeated measure ANOVA",
    "section": "Data",
    "text": "Data\nWe first load the data from the package and inspect the content.\n\n\nCode\ndata(HOSM22_E3, package = \"hecedsm\")\nstr(HOSM22_E3)\n\n\nClasses 'tbl_df', 'tbl' and 'data.frame':   126 obs. of  4 variables:\n $ id        : Factor w/ 63 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 2 2 3 3 4 4 5 5 ...\n $ waiting   : Factor w/ 2 levels \"long\",\"short\": 2 2 1 1 1 1 2 2 1 1 ...\n $ ratingtype: Factor w/ 2 levels \"experience\",\"prediction\": 2 1 2 1 2 1 2 1 2 1 ...\n $ imscore   : num  2.33 3.33 1.25 1.92 1 ...\n\n\nCode\nwith(HOSM22_E3, table(waiting)/2)\n\n\nwaiting\n long short \n   32    31 \n\n\nFrom this, we can see that there each student is assigned to a single waiting time, but that they have both rating types. Since there are 63 students, the study is unbalanced but by a single person; this may be due to exclusions."
  },
  {
    "objectID": "example/repeated.html#model-fitting",
    "href": "example/repeated.html#model-fitting",
    "title": "Repeated measure ANOVA",
    "section": "Model fitting",
    "text": "Model fitting\nWe use the afex package (analysis of factorial design) with the aov_ez. We need to specify the identifier of the subjects (id), the response variable (dv) and both between- (between) and within-subject (within) factors. Each of those names must be quoted (strings)\n\n\nCode\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\nfmod &lt;- afex::aov_ez(id = \"id\", \n                     dv = \"imscore\", \n                     between = \"waiting\",\n                     within = \"ratingtype\",\n                     data = HOSM22_E3)\nanova(fmod)\n\n\nAnova Table (Type 3 tests)\n\nResponse: imscore\n                   num Df den Df     MSE       F      ges    Pr(&gt;F)    \nwaiting                 1     61 2.48926 11.2551 0.126246   0.00137 ** \nratingtype              1     61 0.68953 38.4330 0.120236 5.388e-08 ***\nwaiting:ratingtype      1     61 0.68953  0.0819 0.000291   0.77575    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# MANOVA tests\nfmod$Anova\n\n\n\nType III Repeated Measures MANOVA Tests: Pillai test statistic\n                   Df test stat approx F num Df den Df    Pr(&gt;F)    \n(Intercept)         1   0.89286   508.37      1     61 &lt; 2.2e-16 ***\nwaiting             1   0.15577    11.26      1     61   0.00137 ** \nratingtype          1   0.38652    38.43      1     61 5.388e-08 ***\nwaiting:ratingtype  1   0.00134     0.08      1     61   0.77575    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe output includes the \\(F\\)-tests for the two main effects and the interaction and global effect sizes \\(\\widehat{\\eta}^2\\) (ges). There is no output for tests of sphericity, since there are only two measurements per person and thus a single mean different within-subject (so the test to check equality doesn’t make sense with a single number). We could however compare variance between groups using Levene’s test\nNotice that the degrees of freedom for the denominator of the test are based on the number of participants, here 63."
  },
  {
    "objectID": "example/twowayanova.html",
    "href": "example/twowayanova.html",
    "title": "Two-way analysis of variance",
    "section": "",
    "text": "The R code can be downloaded here and the SPSS code here.\nThere’s a set of videos that walks through each section below. To make it easier for you to jump around the video examples, I cut the long video into smaller pieces and included them all in one YouTube playlist.\n\nANOVA table\nInteraction plot\nContrasts and marginal means\nEffect size and power\nSPSS walkthrough\n\nYou can also watch the playlist (and skip around to different sections) here:\n\n\n\n\n\n\nWe consider data from a replication by Chandler (2016) of Study 4a of Janiszewski and Uy (2008). Both studies measured the amount of adjustment when presented with vague or precise range of value for objects, with potential encouragement for adjusting more the value.\n\n\nChandler (2016) described the effect in the replication report:\n\nJaniszewski and Uy (2008) conceptualized people’s attempt to adjust following presentation of an anchor as movement along a subjective representation scale by a certain number of units. Precise numbers (e.g. $9.99) imply a finer-resolution scale than round numbers (e.g. $10). Consequently, adjustment along a subjectively finer resolution scale will result in less objective adjustment than adjustment by the same number of units along a subjectively coarse resolution scale.\n\nThe experiment is a 2 by 2 factorial design (two-way ANOVA) with anchor (either round or precise) and magnitude (0 for small, 1 for big adjustment) as experimental factors. A total of 120 students were recruited and randomly assigned to one of the four experimental sub-condition, for a total of 30 observations per subgroup (anchor, magnitude). The response variable is majust, the mean adjustment for the price estimate of the item. The dataset is available from the R package hecedsm as C16.\n\n\nCode\n# Load packages\nlibrary(dplyr)   # data manipulation\nlibrary(ggplot2) # graphics\nlibrary(emmeans) # contrasts, etc.\nlibrary(car) # companion to applied regression\n# Example of two-way ANOVA with balanced design\ndata(C16, package = \"hecedsm\")\n# Check for balance\nxtabs(formula = ~ anchor + magnitude,\n      data = C16)\n\n\n         magnitude\nanchor     0  1\n  round   30 30\n  precise 30 30\n\n\nWe can see that there are 30 observations in each group in the replication, as advertised.\n\n\n\nIn R, the function aov fits an analysis of variance model for balanced data. Analysis of variance are simple instances of linear regression models, and the main difference between fitting the model using aov and lm is the default parametrization used. In more general settings (including continuous covariates), we will use lm as a workshorse to fit the model, with an option to set up the contrasts so the output matches our expectations (and needs).\n\n\nCode\n# Fit two-way ANOVA model\nmod &lt;- aov(madjust ~ anchor * magnitude,\n           data = C16)\n# Analysis of variance table\nsummary(mod)\n\n\n                  Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nanchor             1  0.777   0.777   6.277   0.0136 *  \nmagnitude          1  8.796   8.796  71.058 1.09e-13 ***\nanchor:magnitude   1  0.002   0.002   0.013   0.9088    \nResiduals        116 14.359   0.124                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe model is fitted as before by specifying the response ~ explanatories: the * notation is a shortcut to specify anchor + magnitude + anchor:magnitude, with the last term separated by a semi-colon : denoting an interaction between two variables. Here, the experimental factors anchor and magnitude are crossed, as it is possible to be in both experimental groups simultaneously.\n\n\n\nThe interaction.plot function in base R allows one to create an interaction (or profile) plot for a two-way design. More generally, we can simply compute the group means for each combination of the experimental conditions, map the mean response to the \\(y\\)-axis of a graph and add the experimental factors to other dimensions (\\(x\\)-axis, panel, color, etc.)\n\n\nCode\nC16 |&gt;\n ggplot(mapping = aes(x = anchor,\n                      y = madjust,\n                      color = magnitude)) +\n  geom_jitter(width = 0.1,\n              alpha = 0.1) +\n  stat_summary(aes(group = magnitude), \n               fun = mean, \n               geom = \"line\") +\n  # Change position of labels\n  labs(y = \"\",\n       subtitle = \"Mean adjustment\") +\n  theme_classic() + # change theme\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nIn our example, the interaction plot shows a large main effect for magnitude, a smaller one for anchor and no evidence of interaction — despite the uncertainty associated with the estimation, the lines are very close to being parallel. Overlaying the jitter observations shows there is quite a bit of spread, but with limited overlap. Despite the graphical evidence hinting that the interaction isn’t significant, we will fit the two-way analysis of variance model with the interaction unless we invalidate our statistical inference.\nThe emmip function allows one to return a plot automagically.\n\n\nCode\n# Interaction plot\nemmeans::emmip(mod, \n               magnitude ~ anchor, \n               CIs = TRUE) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nBecause our dataset is balanced, the marginal means (the summary statistics obtained by grouping the data for a single factor) and the marginal effects (obtained by calculating the average cell means by either row or column) will coincide. There are multiple functions that allow one to obtain estimates means for cells, rows or columns, including functionalities, notably emmeans from the eponymous package and model.tables\n\n\nCode\n# Get grand mean, cell means, etc.\nmodel.tables(mod, type = \"means\")\n\n\nTables of means\nGrand mean\n            \n0.001155778 \n\n anchor \nanchor\n   round  precise \n 0.08162 -0.07931 \n\n magnitude \nmagnitude\n       0        1 \n-0.26959  0.27190 \n\n anchor:magnitude \n         magnitude\nanchor    0       1      \n  round   -0.1854  0.3487\n  precise -0.3537  0.1951\n\n\nCode\n# Cell means\nemmeans(object = mod, \n        specs = c(\"anchor\", \"magnitude\"),\n        type = \"response\")\n\n\n anchor  magnitude emmean     SE  df lower.CL upper.CL\n round   0         -0.185 0.0642 116  -0.3127  -0.0582\n precise 0         -0.354 0.0642 116  -0.4810  -0.2265\n round   1          0.349 0.0642 116   0.2215   0.4759\n precise 1          0.195 0.0642 116   0.0679   0.3223\n\nConfidence level used: 0.95 \n\n\nCode\n# Marginal means\nemmeans(object = mod, \n        specs = \"anchor\", \n        type = \"response\")\n\n\n anchor   emmean     SE  df lower.CL upper.CL\n round    0.0816 0.0454 116 -0.00834   0.1716\n precise -0.0793 0.0454 116 -0.16928   0.0107\n\nResults are averaged over the levels of: magnitude \nConfidence level used: 0.95 \n\n\nCode\nemmeans(object = mod, \n        specs = \"anchor\", \n        type = \"response\")\n\n\n anchor   emmean     SE  df lower.CL upper.CL\n round    0.0816 0.0454 116 -0.00834   0.1716\n precise -0.0793 0.0454 116 -0.16928   0.0107\n\nResults are averaged over the levels of: magnitude \nConfidence level used: 0.95 \n\n\nCode\n# These match summary statistics\nC16 |&gt;\n  group_by(magnitude) |&gt;\n  summarize(margmean = mean(madjust))\n\n\n# A tibble: 2 × 2\n  magnitude margmean\n  &lt;fct&gt;        &lt;dbl&gt;\n1 0           -0.270\n2 1            0.272\n\n\nCode\nC16 |&gt;\n  group_by(anchor) |&gt;\n  summarize(margmean = mean(madjust))\n\n\n# A tibble: 2 × 2\n  anchor  margmean\n  &lt;fct&gt;      &lt;dbl&gt;\n1 round     0.0816\n2 precise  -0.0793\n\n\nSince the data are balanced, we can look at the (default) analysis of variance table produced using anova function1\n\n\nCode\nanova(mod)\n\n\nAnalysis of Variance Table\n\nResponse: madjust\n                  Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nanchor             1  0.7770  0.7770  6.2768   0.01362 *  \nmagnitude          1  8.7962  8.7962 71.0584 1.089e-13 ***\nanchor:magnitude   1  0.0016  0.0016  0.0132   0.90879    \nResiduals        116 14.3595  0.1238                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe output confirms our intuition that there is not much different from zero, with a strong effect for magnitude of adjustment and a significant, albeit smaller one, for anchor type.\n\n\n\nWhile the conclusions are probably unambiguous due to the large evidence, it would be useful to check the model assumptions.\nThe sample size is just enough to forego normality checks, but the quantile-quantile plot can be useful to detect outliers and extremes. Outside of one potential value much lower than it’s group mean, there is no cause for concern.\n\n\nCode\ncar::qqPlot(mod, id = FALSE)\n\n\n\n\n\nWith 30 observations per group and no appearance of outlier, we need rather to worry about additivity and possibly heterogeneity arising from the treatment. Independence is plausible based on the context.\nThe Tukey-Anscombe plot of residuals against fitted values (the group means) indicate no deviation, but the variance appears to be larger for the two groups with a large adjustment. Because the response takes negative values, we can simply proceed with fitting a two-way analysis in which each of the subgroups has mean \\(\\mu_{ij}\\) and standard deviation \\(\\sigma_{ij}\\): in other words, only the data for each subgroup (anchor, magnitude) are used to estimate the summary statistics of that group.\n\n\nCode\n# Evidence of unequal variance\nggplot(data = data.frame(residuals = resid(mod),\n                         fitted = fitted(mod)),\n       mapping = aes(x = fitted,\n                     y = residuals)) +\n   geom_jitter(width = 0.03, height = 0) +\n  theme_classic()\n\n\n\n\n\nCode\n# Equality of variance - Brown-Forsythe\ncar::leveneTest(mod) \n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)  \ngroup   3  2.8133 0.0424 *\n      116                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nGiven the Brown-Forsythe test output, we can try fitting a different variance in each group, as there are enough observations for this. The function gls in the nlme package fits such models; the weight argument being setup with a constant variance (~1) per each combination of the crossed factors anchor * magnitude.\n\n\nCode\n# Fit a variance per group\nmod2 &lt;- nlme::gls(\n  model = madjust ~ anchor * magnitude,\n  data = C16,\n  weights = nlme::varIdent(\n    form = ~ 1 | anchor * magnitude))\n\n# Different ANOVA - we use type II here\ncar::Anova(mod2, type = 2)\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: madjust\n                 Df   Chisq Pr(&gt;Chisq)    \nanchor            1  6.7708   0.009266 ** \nmagnitude         1 79.9238  &lt; 2.2e-16 ***\nanchor:magnitude  1  0.0132   0.908595    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can see the unequal std. deviation per group when passing the model with unequal variance and unequal means and computing the estimated marginal means. The package emmeans automatically adjusts for these changes.\n\n\nCode\nemmeans(object = mod2, \n        specs = c(\"anchor\", \"magnitude\"))\n\n\n anchor  magnitude emmean     SE   df lower.CL upper.CL\n round   0         -0.185 0.0676 29.1  -0.3236  -0.0473\n precise 0         -0.354 0.0422 29.0  -0.4401  -0.2674\n round   1          0.349 0.0796 29.0   0.1859   0.5115\n precise 1          0.195 0.0618 29.2   0.0688   0.3215\n\nDegrees-of-freedom method: satterthwaite \nConfidence level used: 0.95 \n\n\nCode\n# Compute pairwise difference for anchor\nmarg_effect &lt;- emmeans(object = mod2, \n        specs = \"anchor\") |&gt; \n  pairs()\nmarg_effect\n\n\n contrast        estimate     SE  df t.ratio p.value\n round - precise    0.161 0.0642 100   2.505  0.0138\n\nResults are averaged over the levels of: magnitude \nDegrees-of-freedom method: satterthwaite \n\n\nCode\n# To get a data frame with data\n# broom::tidy(marg_effect)\n\n\nWe can then pass the output to car::Anova to print the analysis of variance table. The \\(p\\)-value for the main effect of anchor is 0.014 in the equal variance model. With unequal variance, different tests give different values: the \\(p\\)-value is 0.009 if we use type II effects (the correct choice here), 0.035 with type III effects2 and the emmeans package returns Welch’s test for the pairwise difference with Satterwaite’s degree of freedom approximation if we average over magnitude to account for the difference in variance, this time with a \\(p\\)-value of 0.014. These differences in output are somewhat notable: with borderline statistical significance, they may lead to different conclusions if one blindly dichotomize the results. Clearly stating which test and how the results are obtained is crucial for transparent reporting, as is providing the code and data. Let your readers make their own mind by reporting \\(p\\)-values.\n\n\n\nRegardless of the model, it should be clearly stated that there is some evidence of heterogeneity. We should also report sample size per group, mention the repartition (\\(n=30\\) per group).\nIn the present case, we can give information about the main effects and stop here, but giving an indication about the size of the adjustment (by reporting estimated marginal means) is useful. Note that emmeans gives a (here spurious) warning about the main effects (row or column average) since there is a potential interaction — as we all but ruled out the latter, we proceed nevertheless.\n\n\nCode\nemm_marg &lt;- emmeans::emmeans(\n  object = mod2,\n  specs = \"anchor\"\n)\n\n\nThere are many different options to get the same results with emmeans. The specs indicates the list of factors which we want to keep, whereas by gives the one we want to have separate analysis for. In formula, we could get the simple effects for anchor by level of magnitude using ~ anchor | magnitude, or set specs = anchor and by = magnitude. We can pass the result to pairs to obtain pairwise differences.\n\n\nCode\n# Simple effects for anchor\nemm_simple &lt;- emmeans::emmeans(\n  object = mod,\n  specs = \"anchor\",\n  by = \"magnitude\"\n)\n# Compute pairwise differences within each magnitude\npairs(emm_simple)\n\n\nmagnitude = 0:\n contrast        estimate     SE  df t.ratio p.value\n round - precise    0.168 0.0908 116   1.853  0.0665\n\nmagnitude = 1:\n contrast        estimate     SE  df t.ratio p.value\n round - precise    0.154 0.0908 116   1.690  0.0936\n\n\n\n\n\nBy default, emmeans will compute adjustments for pairwise difference using Tukey’s honest significant difference method if there are more than one pairwise comparison. The software cannot easily guess the degrees of freedom, the number of tests, etc.\nThere are also tests which are not of interest: for example, one probably wouldn’t want to compute the difference between the adjustment for (small magnitude and round) versus (large magnitude and precise).\nIf we were interested in looking at all pairwise differences, we could keep all of the cells means.\n\n\nCode\nemmeans(object = mod2, \n        specs = c(\"magnitude\", \"anchor\"), \n        contr = \"pairwise\")\n\n\n$emmeans\n magnitude anchor  emmean     SE   df lower.CL upper.CL\n 0         round   -0.185 0.0676 29.1  -0.3236  -0.0473\n 1         round    0.349 0.0796 29.0   0.1859   0.5115\n 0         precise -0.354 0.0422 29.0  -0.4401  -0.2674\n 1         precise  0.195 0.0618 29.2   0.0688   0.3215\n\nDegrees-of-freedom method: satterthwaite \nConfidence level used: 0.95 \n\n$contrasts\n contrast                                estimate     SE   df t.ratio p.value\n magnitude0 round - magnitude1 round       -0.534 0.1044 56.7  -5.115  &lt;.0001\n magnitude0 round - magnitude0 precise      0.168 0.0797 48.7   2.112  0.1636\n magnitude0 round - magnitude1 precise     -0.381 0.0916 57.6  -4.156  0.0006\n magnitude1 round - magnitude0 precise      0.702 0.0901 44.2   7.795  &lt;.0001\n magnitude1 round - magnitude1 precise      0.154 0.1008 54.6   1.524  0.4306\n magnitude0 precise - magnitude1 precise   -0.549 0.0748 51.5  -7.333  &lt;.0001\n\nDegrees-of-freedom method: satterthwaite \nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nNotice now the mention about Tukey’s effect. When there is heterogeneity of variance or unbalanced effects, the actual method employed is called Games-Howell correction.\n\n\n\n\nWe now reproduce the results of Study 1 of Maglio & Polman (2014). Data were obtained from the Open Science Foundation and are available in the MP14_S1 dataset in package hecedsm.\n\nWe carried out a 2 (orientation: toward, away from) × 4 (station: Spadina, St. George, Bloor-Yonge, Sherbourne) analysis of variance (ANOVA) on closeness ratings.\n\n\n\nCode\ndata(MP14_S1, package = \"hecedsm\")\nxtabs(~ direction + station, data = MP14_S1)\n\n\n         station\ndirection Spadina St. George Bloor-Yonge Sherbourne\n     east      26         26          23         26\n     west      25         25          26         25\n\n\nThe counts are not balanced, but not far from being equal in each group.\nBased on the actual study, it is quite clear we expect there will be an interaction if there is an actual effect. Lack of interaction would entail that subjective distance is perceived the same way regardless of the direction of travel.\n\n\nSince the data are unbalanced, we can fit the model using lm. The default parametrization of the linear model uses the first alphanumerical value for the factor as reference category and coefficients encode differences relative to this particular average. We can obtain the sum-to-zero by setting the option contr.sum3\n\n\nCode\nstr(MP14_S1) # look at data\n\n\ntibble [202 × 3] (S3: tbl_df/tbl/data.frame)\n $ station  : Factor w/ 4 levels \"Spadina\",\"St. George\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ direction: Factor w/ 2 levels \"east\",\"west\": 1 1 1 1 1 1 1 1 1 1 ...\n $ distance : int [1:202] 1 2 2 3 1 1 2 1 3 2 ...\n\n\nCode\n# Set up contrasts\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\nmodel &lt;- lm(distance ~ station*direction, \n            data = MP14_S1)\nsummary(model) # the coefficients are global mean\n\n\n\nCall:\nlm(formula = distance ~ station * direction, data = MP14_S1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6538 -0.6400  0.1154  0.3913  2.8077 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          2.65916    0.07294  36.458  &lt; 2e-16 ***\nstation1             0.48776    0.12587   3.875 0.000146 ***\nstation2            -0.45455    0.12587  -3.611 0.000388 ***\nstation3            -0.75866    0.12771  -5.941 1.29e-08 ***\ndirection1           0.04109    0.07294   0.563 0.573870    \nstation1:direction1  0.46584    0.12587   3.701 0.000280 ***\nstation2:direction1  0.52353    0.12587   4.159 4.79e-05 ***\nstation3:direction1 -0.33289    0.12771  -2.607 0.009853 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.036 on 194 degrees of freedom\nMultiple R-squared:  0.3813,    Adjusted R-squared:  0.359 \nF-statistic: 17.08 on 7 and 194 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Test only the interaction\ncar::Anova(model, type = 3) \n\n\nAnova Table (Type III tests)\n\nResponse: distance\n                   Sum Sq  Df   F value    Pr(&gt;F)    \n(Intercept)       1426.15   1 1329.1917 &lt; 2.2e-16 ***\nstation             77.58   3   24.1005 2.665e-13 ***\ndirection            0.34   1    0.3173    0.5739    \nstation:direction   52.41   3   16.2832 1.765e-09 ***\nResiduals          208.15 194                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nWe can use type II or type III tests: here, because the interaction term is significant, we could look at simple effects but these are not of scientific interest. Rather, we compute the cell averages and proceed with our contrast setup.\nBecause of the discreteness of the data (a Likert scale ranging from 1 to 5), there is limited potential for outlyingness. If you look at the normal quantile-quantile plot, you should see a marked staircase pattern due to the ties. Brown–Forsythe’s test gives no evidence of unequal variance per group, so we proceed with the model.\n\n\n\nWhile we can do plots ourselves, the emmip function allows us to return a nice looking profile plot with ggplot2.\n\n\nCode\nemmeans::emmip(object = model, \n               # trace.factor ~ x.factor\n               formula = direction ~ station,\n               CIs = TRUE) +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\n# Interaction plot \n# average of each subgroup, with +/- 1 std. error\n\nMP14_S1 |&gt;\n  group_by(direction, station) |&gt;\n  summarize(mean = mean(distance),\n            se = sigma(model) / sqrt(n()),\n            lower = mean - se,\n            upper = mean + se) |&gt;\n  ggplot(mapping = aes(x = station,\n                       y = mean, \n                       group = direction,\n                       col = direction)) + \n  geom_line() +\n  geom_errorbar(aes(ymin = lower, \n                    ymax = upper),\n                width = 0.2) +\n  geom_point() +\n  scale_colour_grey() +\n  labs(title = \"subjective distance\",\n       subtitle = \"mean (1 std. error)\",\n       x = \"subway station\",\n       y = \"\",\n       caption = \"stations are ordered from west to east\") +\n  theme_classic() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nIf the data lend some support for this claim, we could consider the following follow-up questions:\n\nsymmetry\nwhether the average perceived distance for St. George (east) and Bloor–Yonge (west) is the same as vice-versa (both one station away from Bay).\nsame, but for Spadina and Sherbourne which are two stations away from Bay.\nif the perceived distance for stations Spadina or Sherbourne, which are two away from Bay, are viewed as being twice as distance as St. George and Bloor–Yonge.\n\nFor the experiment to make sense, we should check that travel time or distance is indeed roughly the same between each station. These hypotheses can be expressed in terms of contrasts.\nIn emmeans, we keep both factors in the model and begin by computing the cell means and looking at the order of the factors and ordering of the levels to properly set our contrast vectors.\nThe hypothesis of symmetry is slightly complicated, as we want to test a model which has the same perceived distance for distance, but comparing stations one apart and two apart in the same direction of travel. This leads to four contrast vectors, but we set up the hypothesis test to look at them simultaneously using an \\(F\\)-test. If we impose same mean perceived distance for each station with the symmetry, we would have four average instead of the eight cells: the null distribution for the mean comparison will be a Fisher distribution with \\(\\nu_1=8-4=4\\) degrees of freedom.\n\n\nCode\nemm &lt;- emmeans(model, \n               specs = c(\"direction\", \"station\"))\nlevels(emm)\n\n\n$direction\n[1] \"east\" \"west\"\n\n$station\n[1] \"Spadina\"     \"St. George\"  \"Bloor-Yonge\" \"Sherbourne\" \n\n\nCode\ncontrast_list &lt;- list(\n \"2 station (opposite)\" = c(1, 0, 0, 0, 0, 0, 0, -1),\n \"1 station (opposite)\" = c(0, 0, 1, 0, 0, -1, 0, 0),\n \"2 station (travel)\" = c(0, 1, 0, 0, 0, 0, -1, 0),\n \"1 station (travel)\" = c(0, 0, 0, 1, -1, 0, 0, 0)\n)\n# Hypothesis test (symmetry)\nemm |&gt; \n  contrast(method = contrast_list) |&gt; \n  test(joint = TRUE)\n\n\n df1 df2 F.ratio p.value\n   4 194   1.416  0.2300\n\n\nPerhaps unsurprisingly, there is not much difference and we cannot detect departure from symmetry.\nThe other hypothesis tests, which look at perceived differences one direction or another, quantify the difference in subjective distance depending on whether the station is in the direction of travel or opposite.\n\\[\\begin{align*}\n\\mathscr{H}_{01} &: \\mu_{\\text{E, SG}} + \\mu_{\\text{W, BY}} = \\mu_{\\text{W, SG}} +\\mu_{\\text{E, BY}} \\\\\n\\mathscr{H}_{02} &: \\mu_{\\text{E, Sp}} +\\mu_{\\text{W, Sh}} = \\mu_{\\text{W, Sp}} + \\mu_{\\text{E, Sh}} \\\\\n\\mathscr{H}_{03} &: 2(\\mu_{\\text{E, SG}} + \\mu_{\\text{W, BY}}) = \\mu_{\\text{E, Sp}} + \\mu_{\\text{W, Sh}} \\\\\n\\mathscr{H}_{04} &: 2(\\mu_{\\text{W, SG}} + \\mu_{\\text{E, BY}}) = \\mu_{\\text{W, Sp}} + \\mu_{\\text{E, Sh}}\n\\end{align*}\\]\nHypotheses 3 and 4 could be tested jointly, using the same trick we employed for symmetry to impose the linear restrictions on the parameters. We will consider only the pairwise differences in the sequel.\nSince we consider arbitrary contrasts between the means of the eight cells, we can account for multiple testing within the family by using Scheffé’s method. In emmeans, the adjust = \"scheffe\" argument sets up the contrasts, but the output will be incorrect unless we specify the number of groups (for main effects, \\(n_a-1\\), for all cells, \\(n_an_b-1\\)) through scheffe.rank.\nTo account for the other tests, we can also reduce the level to decrease the probability of making a type I error.\n\n\nCode\ncustom_contrasts &lt;- list(\n \"2 station, opposite vs same\" = \n   c(1, -1, 0, 0, 0, 0, -1, 1),\n \"1 station, opposite vs same\" = \n   c(0, 0, 1, -1, -1, 1, 0, 0),\n \"1 vs 2 station (opposite)\" = \n   c(-1, 0, 2, 0, 0, 2, 0, -1),\n \"1 vs 2 station (same)\" = \n   c(0, -1, 0, 2, 2, 0, -1, 0)\n)\n# Set up contrasts with correction\ncont_emm &lt;- contrast(\n  object = emm, \n  method = custom_contrasts,\n  adjust = \"scheffe\")\n# Tests and p-values\ncont_emm |&gt;\n  test(scheffe.rank = 7)\n\n\n contrast                    estimate    SE  df t.ratio p.value\n 2 station, opposite vs same     2.24 0.410 194   5.470  0.0002\n 1 station, opposite vs same     1.71 0.415 194   4.129  0.0206\n 1 vs 2 station (opposite)       2.27 0.644 194   3.525  0.0942\n 1 vs 2 station (same)           1.09 0.665 194   1.636  0.9120\n\nP value adjustment: scheffe method with rank 7 \n\n\nCode\n# Tests with confidence intervals\ncont_emm |&gt;\n  confint(scheffe.rank = 7,\n          level = 0.99)\n\n\n contrast                    estimate    SE  df lower.CL upper.CL\n 2 station, opposite vs same     2.24 0.410 194    0.450     4.04\n 1 station, opposite vs same     1.71 0.415 194   -0.102     3.53\n 1 vs 2 station (opposite)       2.27 0.644 194   -0.546     5.08\n 1 vs 2 station (same)           1.09 0.665 194   -1.821     4.00\n\nConfidence level used: 0.99 \nConf-level adjustment: scheffe method with rank 7 \n\n\nEven with a correction for multiple testing, there is strong difference in perceived distance for one station away (direction of travel vs opposite) and similarly for two stations. There is mild evidence that, in one direction, the perceived distance between stations is not equal. A potential criticism, in addition to the design of the scale, would be about real distance between stations may not be the same."
  },
  {
    "objectID": "example/twowayanova.html#study-1---balanced-data",
    "href": "example/twowayanova.html#study-1---balanced-data",
    "title": "Two-way analysis of variance",
    "section": "",
    "text": "We consider data from a replication by Chandler (2016) of Study 4a of Janiszewski and Uy (2008). Both studies measured the amount of adjustment when presented with vague or precise range of value for objects, with potential encouragement for adjusting more the value.\n\n\nChandler (2016) described the effect in the replication report:\n\nJaniszewski and Uy (2008) conceptualized people’s attempt to adjust following presentation of an anchor as movement along a subjective representation scale by a certain number of units. Precise numbers (e.g. $9.99) imply a finer-resolution scale than round numbers (e.g. $10). Consequently, adjustment along a subjectively finer resolution scale will result in less objective adjustment than adjustment by the same number of units along a subjectively coarse resolution scale.\n\nThe experiment is a 2 by 2 factorial design (two-way ANOVA) with anchor (either round or precise) and magnitude (0 for small, 1 for big adjustment) as experimental factors. A total of 120 students were recruited and randomly assigned to one of the four experimental sub-condition, for a total of 30 observations per subgroup (anchor, magnitude). The response variable is majust, the mean adjustment for the price estimate of the item. The dataset is available from the R package hecedsm as C16.\n\n\nCode\n# Load packages\nlibrary(dplyr)   # data manipulation\nlibrary(ggplot2) # graphics\nlibrary(emmeans) # contrasts, etc.\nlibrary(car) # companion to applied regression\n# Example of two-way ANOVA with balanced design\ndata(C16, package = \"hecedsm\")\n# Check for balance\nxtabs(formula = ~ anchor + magnitude,\n      data = C16)\n\n\n         magnitude\nanchor     0  1\n  round   30 30\n  precise 30 30\n\n\nWe can see that there are 30 observations in each group in the replication, as advertised.\n\n\n\nIn R, the function aov fits an analysis of variance model for balanced data. Analysis of variance are simple instances of linear regression models, and the main difference between fitting the model using aov and lm is the default parametrization used. In more general settings (including continuous covariates), we will use lm as a workshorse to fit the model, with an option to set up the contrasts so the output matches our expectations (and needs).\n\n\nCode\n# Fit two-way ANOVA model\nmod &lt;- aov(madjust ~ anchor * magnitude,\n           data = C16)\n# Analysis of variance table\nsummary(mod)\n\n\n                  Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nanchor             1  0.777   0.777   6.277   0.0136 *  \nmagnitude          1  8.796   8.796  71.058 1.09e-13 ***\nanchor:magnitude   1  0.002   0.002   0.013   0.9088    \nResiduals        116 14.359   0.124                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe model is fitted as before by specifying the response ~ explanatories: the * notation is a shortcut to specify anchor + magnitude + anchor:magnitude, with the last term separated by a semi-colon : denoting an interaction between two variables. Here, the experimental factors anchor and magnitude are crossed, as it is possible to be in both experimental groups simultaneously.\n\n\n\nThe interaction.plot function in base R allows one to create an interaction (or profile) plot for a two-way design. More generally, we can simply compute the group means for each combination of the experimental conditions, map the mean response to the \\(y\\)-axis of a graph and add the experimental factors to other dimensions (\\(x\\)-axis, panel, color, etc.)\n\n\nCode\nC16 |&gt;\n ggplot(mapping = aes(x = anchor,\n                      y = madjust,\n                      color = magnitude)) +\n  geom_jitter(width = 0.1,\n              alpha = 0.1) +\n  stat_summary(aes(group = magnitude), \n               fun = mean, \n               geom = \"line\") +\n  # Change position of labels\n  labs(y = \"\",\n       subtitle = \"Mean adjustment\") +\n  theme_classic() + # change theme\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nIn our example, the interaction plot shows a large main effect for magnitude, a smaller one for anchor and no evidence of interaction — despite the uncertainty associated with the estimation, the lines are very close to being parallel. Overlaying the jitter observations shows there is quite a bit of spread, but with limited overlap. Despite the graphical evidence hinting that the interaction isn’t significant, we will fit the two-way analysis of variance model with the interaction unless we invalidate our statistical inference.\nThe emmip function allows one to return a plot automagically.\n\n\nCode\n# Interaction plot\nemmeans::emmip(mod, \n               magnitude ~ anchor, \n               CIs = TRUE) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nBecause our dataset is balanced, the marginal means (the summary statistics obtained by grouping the data for a single factor) and the marginal effects (obtained by calculating the average cell means by either row or column) will coincide. There are multiple functions that allow one to obtain estimates means for cells, rows or columns, including functionalities, notably emmeans from the eponymous package and model.tables\n\n\nCode\n# Get grand mean, cell means, etc.\nmodel.tables(mod, type = \"means\")\n\n\nTables of means\nGrand mean\n            \n0.001155778 \n\n anchor \nanchor\n   round  precise \n 0.08162 -0.07931 \n\n magnitude \nmagnitude\n       0        1 \n-0.26959  0.27190 \n\n anchor:magnitude \n         magnitude\nanchor    0       1      \n  round   -0.1854  0.3487\n  precise -0.3537  0.1951\n\n\nCode\n# Cell means\nemmeans(object = mod, \n        specs = c(\"anchor\", \"magnitude\"),\n        type = \"response\")\n\n\n anchor  magnitude emmean     SE  df lower.CL upper.CL\n round   0         -0.185 0.0642 116  -0.3127  -0.0582\n precise 0         -0.354 0.0642 116  -0.4810  -0.2265\n round   1          0.349 0.0642 116   0.2215   0.4759\n precise 1          0.195 0.0642 116   0.0679   0.3223\n\nConfidence level used: 0.95 \n\n\nCode\n# Marginal means\nemmeans(object = mod, \n        specs = \"anchor\", \n        type = \"response\")\n\n\n anchor   emmean     SE  df lower.CL upper.CL\n round    0.0816 0.0454 116 -0.00834   0.1716\n precise -0.0793 0.0454 116 -0.16928   0.0107\n\nResults are averaged over the levels of: magnitude \nConfidence level used: 0.95 \n\n\nCode\nemmeans(object = mod, \n        specs = \"anchor\", \n        type = \"response\")\n\n\n anchor   emmean     SE  df lower.CL upper.CL\n round    0.0816 0.0454 116 -0.00834   0.1716\n precise -0.0793 0.0454 116 -0.16928   0.0107\n\nResults are averaged over the levels of: magnitude \nConfidence level used: 0.95 \n\n\nCode\n# These match summary statistics\nC16 |&gt;\n  group_by(magnitude) |&gt;\n  summarize(margmean = mean(madjust))\n\n\n# A tibble: 2 × 2\n  magnitude margmean\n  &lt;fct&gt;        &lt;dbl&gt;\n1 0           -0.270\n2 1            0.272\n\n\nCode\nC16 |&gt;\n  group_by(anchor) |&gt;\n  summarize(margmean = mean(madjust))\n\n\n# A tibble: 2 × 2\n  anchor  margmean\n  &lt;fct&gt;      &lt;dbl&gt;\n1 round     0.0816\n2 precise  -0.0793\n\n\nSince the data are balanced, we can look at the (default) analysis of variance table produced using anova function1\n\n\nCode\nanova(mod)\n\n\nAnalysis of Variance Table\n\nResponse: madjust\n                  Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nanchor             1  0.7770  0.7770  6.2768   0.01362 *  \nmagnitude          1  8.7962  8.7962 71.0584 1.089e-13 ***\nanchor:magnitude   1  0.0016  0.0016  0.0132   0.90879    \nResiduals        116 14.3595  0.1238                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe output confirms our intuition that there is not much different from zero, with a strong effect for magnitude of adjustment and a significant, albeit smaller one, for anchor type.\n\n\n\nWhile the conclusions are probably unambiguous due to the large evidence, it would be useful to check the model assumptions.\nThe sample size is just enough to forego normality checks, but the quantile-quantile plot can be useful to detect outliers and extremes. Outside of one potential value much lower than it’s group mean, there is no cause for concern.\n\n\nCode\ncar::qqPlot(mod, id = FALSE)\n\n\n\n\n\nWith 30 observations per group and no appearance of outlier, we need rather to worry about additivity and possibly heterogeneity arising from the treatment. Independence is plausible based on the context.\nThe Tukey-Anscombe plot of residuals against fitted values (the group means) indicate no deviation, but the variance appears to be larger for the two groups with a large adjustment. Because the response takes negative values, we can simply proceed with fitting a two-way analysis in which each of the subgroups has mean \\(\\mu_{ij}\\) and standard deviation \\(\\sigma_{ij}\\): in other words, only the data for each subgroup (anchor, magnitude) are used to estimate the summary statistics of that group.\n\n\nCode\n# Evidence of unequal variance\nggplot(data = data.frame(residuals = resid(mod),\n                         fitted = fitted(mod)),\n       mapping = aes(x = fitted,\n                     y = residuals)) +\n   geom_jitter(width = 0.03, height = 0) +\n  theme_classic()\n\n\n\n\n\nCode\n# Equality of variance - Brown-Forsythe\ncar::leveneTest(mod) \n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)  \ngroup   3  2.8133 0.0424 *\n      116                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nGiven the Brown-Forsythe test output, we can try fitting a different variance in each group, as there are enough observations for this. The function gls in the nlme package fits such models; the weight argument being setup with a constant variance (~1) per each combination of the crossed factors anchor * magnitude.\n\n\nCode\n# Fit a variance per group\nmod2 &lt;- nlme::gls(\n  model = madjust ~ anchor * magnitude,\n  data = C16,\n  weights = nlme::varIdent(\n    form = ~ 1 | anchor * magnitude))\n\n# Different ANOVA - we use type II here\ncar::Anova(mod2, type = 2)\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: madjust\n                 Df   Chisq Pr(&gt;Chisq)    \nanchor            1  6.7708   0.009266 ** \nmagnitude         1 79.9238  &lt; 2.2e-16 ***\nanchor:magnitude  1  0.0132   0.908595    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can see the unequal std. deviation per group when passing the model with unequal variance and unequal means and computing the estimated marginal means. The package emmeans automatically adjusts for these changes.\n\n\nCode\nemmeans(object = mod2, \n        specs = c(\"anchor\", \"magnitude\"))\n\n\n anchor  magnitude emmean     SE   df lower.CL upper.CL\n round   0         -0.185 0.0676 29.1  -0.3236  -0.0473\n precise 0         -0.354 0.0422 29.0  -0.4401  -0.2674\n round   1          0.349 0.0796 29.0   0.1859   0.5115\n precise 1          0.195 0.0618 29.2   0.0688   0.3215\n\nDegrees-of-freedom method: satterthwaite \nConfidence level used: 0.95 \n\n\nCode\n# Compute pairwise difference for anchor\nmarg_effect &lt;- emmeans(object = mod2, \n        specs = \"anchor\") |&gt; \n  pairs()\nmarg_effect\n\n\n contrast        estimate     SE  df t.ratio p.value\n round - precise    0.161 0.0642 100   2.505  0.0138\n\nResults are averaged over the levels of: magnitude \nDegrees-of-freedom method: satterthwaite \n\n\nCode\n# To get a data frame with data\n# broom::tidy(marg_effect)\n\n\nWe can then pass the output to car::Anova to print the analysis of variance table. The \\(p\\)-value for the main effect of anchor is 0.014 in the equal variance model. With unequal variance, different tests give different values: the \\(p\\)-value is 0.009 if we use type II effects (the correct choice here), 0.035 with type III effects2 and the emmeans package returns Welch’s test for the pairwise difference with Satterwaite’s degree of freedom approximation if we average over magnitude to account for the difference in variance, this time with a \\(p\\)-value of 0.014. These differences in output are somewhat notable: with borderline statistical significance, they may lead to different conclusions if one blindly dichotomize the results. Clearly stating which test and how the results are obtained is crucial for transparent reporting, as is providing the code and data. Let your readers make their own mind by reporting \\(p\\)-values.\n\n\n\nRegardless of the model, it should be clearly stated that there is some evidence of heterogeneity. We should also report sample size per group, mention the repartition (\\(n=30\\) per group).\nIn the present case, we can give information about the main effects and stop here, but giving an indication about the size of the adjustment (by reporting estimated marginal means) is useful. Note that emmeans gives a (here spurious) warning about the main effects (row or column average) since there is a potential interaction — as we all but ruled out the latter, we proceed nevertheless.\n\n\nCode\nemm_marg &lt;- emmeans::emmeans(\n  object = mod2,\n  specs = \"anchor\"\n)\n\n\nThere are many different options to get the same results with emmeans. The specs indicates the list of factors which we want to keep, whereas by gives the one we want to have separate analysis for. In formula, we could get the simple effects for anchor by level of magnitude using ~ anchor | magnitude, or set specs = anchor and by = magnitude. We can pass the result to pairs to obtain pairwise differences.\n\n\nCode\n# Simple effects for anchor\nemm_simple &lt;- emmeans::emmeans(\n  object = mod,\n  specs = \"anchor\",\n  by = \"magnitude\"\n)\n# Compute pairwise differences within each magnitude\npairs(emm_simple)\n\n\nmagnitude = 0:\n contrast        estimate     SE  df t.ratio p.value\n round - precise    0.168 0.0908 116   1.853  0.0665\n\nmagnitude = 1:\n contrast        estimate     SE  df t.ratio p.value\n round - precise    0.154 0.0908 116   1.690  0.0936\n\n\n\n\n\nBy default, emmeans will compute adjustments for pairwise difference using Tukey’s honest significant difference method if there are more than one pairwise comparison. The software cannot easily guess the degrees of freedom, the number of tests, etc.\nThere are also tests which are not of interest: for example, one probably wouldn’t want to compute the difference between the adjustment for (small magnitude and round) versus (large magnitude and precise).\nIf we were interested in looking at all pairwise differences, we could keep all of the cells means.\n\n\nCode\nemmeans(object = mod2, \n        specs = c(\"magnitude\", \"anchor\"), \n        contr = \"pairwise\")\n\n\n$emmeans\n magnitude anchor  emmean     SE   df lower.CL upper.CL\n 0         round   -0.185 0.0676 29.1  -0.3236  -0.0473\n 1         round    0.349 0.0796 29.0   0.1859   0.5115\n 0         precise -0.354 0.0422 29.0  -0.4401  -0.2674\n 1         precise  0.195 0.0618 29.2   0.0688   0.3215\n\nDegrees-of-freedom method: satterthwaite \nConfidence level used: 0.95 \n\n$contrasts\n contrast                                estimate     SE   df t.ratio p.value\n magnitude0 round - magnitude1 round       -0.534 0.1044 56.7  -5.115  &lt;.0001\n magnitude0 round - magnitude0 precise      0.168 0.0797 48.7   2.112  0.1636\n magnitude0 round - magnitude1 precise     -0.381 0.0916 57.6  -4.156  0.0006\n magnitude1 round - magnitude0 precise      0.702 0.0901 44.2   7.795  &lt;.0001\n magnitude1 round - magnitude1 precise      0.154 0.1008 54.6   1.524  0.4306\n magnitude0 precise - magnitude1 precise   -0.549 0.0748 51.5  -7.333  &lt;.0001\n\nDegrees-of-freedom method: satterthwaite \nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nNotice now the mention about Tukey’s effect. When there is heterogeneity of variance or unbalanced effects, the actual method employed is called Games-Howell correction."
  },
  {
    "objectID": "example/twowayanova.html#study-2---unbalanced-data",
    "href": "example/twowayanova.html#study-2---unbalanced-data",
    "title": "Two-way analysis of variance",
    "section": "",
    "text": "We now reproduce the results of Study 1 of Maglio & Polman (2014). Data were obtained from the Open Science Foundation and are available in the MP14_S1 dataset in package hecedsm.\n\nWe carried out a 2 (orientation: toward, away from) × 4 (station: Spadina, St. George, Bloor-Yonge, Sherbourne) analysis of variance (ANOVA) on closeness ratings.\n\n\n\nCode\ndata(MP14_S1, package = \"hecedsm\")\nxtabs(~ direction + station, data = MP14_S1)\n\n\n         station\ndirection Spadina St. George Bloor-Yonge Sherbourne\n     east      26         26          23         26\n     west      25         25          26         25\n\n\nThe counts are not balanced, but not far from being equal in each group.\nBased on the actual study, it is quite clear we expect there will be an interaction if there is an actual effect. Lack of interaction would entail that subjective distance is perceived the same way regardless of the direction of travel.\n\n\nSince the data are unbalanced, we can fit the model using lm. The default parametrization of the linear model uses the first alphanumerical value for the factor as reference category and coefficients encode differences relative to this particular average. We can obtain the sum-to-zero by setting the option contr.sum3\n\n\nCode\nstr(MP14_S1) # look at data\n\n\ntibble [202 × 3] (S3: tbl_df/tbl/data.frame)\n $ station  : Factor w/ 4 levels \"Spadina\",\"St. George\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ direction: Factor w/ 2 levels \"east\",\"west\": 1 1 1 1 1 1 1 1 1 1 ...\n $ distance : int [1:202] 1 2 2 3 1 1 2 1 3 2 ...\n\n\nCode\n# Set up contrasts\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\nmodel &lt;- lm(distance ~ station*direction, \n            data = MP14_S1)\nsummary(model) # the coefficients are global mean\n\n\n\nCall:\nlm(formula = distance ~ station * direction, data = MP14_S1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6538 -0.6400  0.1154  0.3913  2.8077 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          2.65916    0.07294  36.458  &lt; 2e-16 ***\nstation1             0.48776    0.12587   3.875 0.000146 ***\nstation2            -0.45455    0.12587  -3.611 0.000388 ***\nstation3            -0.75866    0.12771  -5.941 1.29e-08 ***\ndirection1           0.04109    0.07294   0.563 0.573870    \nstation1:direction1  0.46584    0.12587   3.701 0.000280 ***\nstation2:direction1  0.52353    0.12587   4.159 4.79e-05 ***\nstation3:direction1 -0.33289    0.12771  -2.607 0.009853 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.036 on 194 degrees of freedom\nMultiple R-squared:  0.3813,    Adjusted R-squared:  0.359 \nF-statistic: 17.08 on 7 and 194 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Test only the interaction\ncar::Anova(model, type = 3) \n\n\nAnova Table (Type III tests)\n\nResponse: distance\n                   Sum Sq  Df   F value    Pr(&gt;F)    \n(Intercept)       1426.15   1 1329.1917 &lt; 2.2e-16 ***\nstation             77.58   3   24.1005 2.665e-13 ***\ndirection            0.34   1    0.3173    0.5739    \nstation:direction   52.41   3   16.2832 1.765e-09 ***\nResiduals          208.15 194                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nWe can use type II or type III tests: here, because the interaction term is significant, we could look at simple effects but these are not of scientific interest. Rather, we compute the cell averages and proceed with our contrast setup.\nBecause of the discreteness of the data (a Likert scale ranging from 1 to 5), there is limited potential for outlyingness. If you look at the normal quantile-quantile plot, you should see a marked staircase pattern due to the ties. Brown–Forsythe’s test gives no evidence of unequal variance per group, so we proceed with the model.\n\n\n\nWhile we can do plots ourselves, the emmip function allows us to return a nice looking profile plot with ggplot2.\n\n\nCode\nemmeans::emmip(object = model, \n               # trace.factor ~ x.factor\n               formula = direction ~ station,\n               CIs = TRUE) +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\n# Interaction plot \n# average of each subgroup, with +/- 1 std. error\n\nMP14_S1 |&gt;\n  group_by(direction, station) |&gt;\n  summarize(mean = mean(distance),\n            se = sigma(model) / sqrt(n()),\n            lower = mean - se,\n            upper = mean + se) |&gt;\n  ggplot(mapping = aes(x = station,\n                       y = mean, \n                       group = direction,\n                       col = direction)) + \n  geom_line() +\n  geom_errorbar(aes(ymin = lower, \n                    ymax = upper),\n                width = 0.2) +\n  geom_point() +\n  scale_colour_grey() +\n  labs(title = \"subjective distance\",\n       subtitle = \"mean (1 std. error)\",\n       x = \"subway station\",\n       y = \"\",\n       caption = \"stations are ordered from west to east\") +\n  theme_classic() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nIf the data lend some support for this claim, we could consider the following follow-up questions:\n\nsymmetry\nwhether the average perceived distance for St. George (east) and Bloor–Yonge (west) is the same as vice-versa (both one station away from Bay).\nsame, but for Spadina and Sherbourne which are two stations away from Bay.\nif the perceived distance for stations Spadina or Sherbourne, which are two away from Bay, are viewed as being twice as distance as St. George and Bloor–Yonge.\n\nFor the experiment to make sense, we should check that travel time or distance is indeed roughly the same between each station. These hypotheses can be expressed in terms of contrasts.\nIn emmeans, we keep both factors in the model and begin by computing the cell means and looking at the order of the factors and ordering of the levels to properly set our contrast vectors.\nThe hypothesis of symmetry is slightly complicated, as we want to test a model which has the same perceived distance for distance, but comparing stations one apart and two apart in the same direction of travel. This leads to four contrast vectors, but we set up the hypothesis test to look at them simultaneously using an \\(F\\)-test. If we impose same mean perceived distance for each station with the symmetry, we would have four average instead of the eight cells: the null distribution for the mean comparison will be a Fisher distribution with \\(\\nu_1=8-4=4\\) degrees of freedom.\n\n\nCode\nemm &lt;- emmeans(model, \n               specs = c(\"direction\", \"station\"))\nlevels(emm)\n\n\n$direction\n[1] \"east\" \"west\"\n\n$station\n[1] \"Spadina\"     \"St. George\"  \"Bloor-Yonge\" \"Sherbourne\" \n\n\nCode\ncontrast_list &lt;- list(\n \"2 station (opposite)\" = c(1, 0, 0, 0, 0, 0, 0, -1),\n \"1 station (opposite)\" = c(0, 0, 1, 0, 0, -1, 0, 0),\n \"2 station (travel)\" = c(0, 1, 0, 0, 0, 0, -1, 0),\n \"1 station (travel)\" = c(0, 0, 0, 1, -1, 0, 0, 0)\n)\n# Hypothesis test (symmetry)\nemm |&gt; \n  contrast(method = contrast_list) |&gt; \n  test(joint = TRUE)\n\n\n df1 df2 F.ratio p.value\n   4 194   1.416  0.2300\n\n\nPerhaps unsurprisingly, there is not much difference and we cannot detect departure from symmetry.\nThe other hypothesis tests, which look at perceived differences one direction or another, quantify the difference in subjective distance depending on whether the station is in the direction of travel or opposite.\n\\[\\begin{align*}\n\\mathscr{H}_{01} &: \\mu_{\\text{E, SG}} + \\mu_{\\text{W, BY}} = \\mu_{\\text{W, SG}} +\\mu_{\\text{E, BY}} \\\\\n\\mathscr{H}_{02} &: \\mu_{\\text{E, Sp}} +\\mu_{\\text{W, Sh}} = \\mu_{\\text{W, Sp}} + \\mu_{\\text{E, Sh}} \\\\\n\\mathscr{H}_{03} &: 2(\\mu_{\\text{E, SG}} + \\mu_{\\text{W, BY}}) = \\mu_{\\text{E, Sp}} + \\mu_{\\text{W, Sh}} \\\\\n\\mathscr{H}_{04} &: 2(\\mu_{\\text{W, SG}} + \\mu_{\\text{E, BY}}) = \\mu_{\\text{W, Sp}} + \\mu_{\\text{E, Sh}}\n\\end{align*}\\]\nHypotheses 3 and 4 could be tested jointly, using the same trick we employed for symmetry to impose the linear restrictions on the parameters. We will consider only the pairwise differences in the sequel.\nSince we consider arbitrary contrasts between the means of the eight cells, we can account for multiple testing within the family by using Scheffé’s method. In emmeans, the adjust = \"scheffe\" argument sets up the contrasts, but the output will be incorrect unless we specify the number of groups (for main effects, \\(n_a-1\\), for all cells, \\(n_an_b-1\\)) through scheffe.rank.\nTo account for the other tests, we can also reduce the level to decrease the probability of making a type I error.\n\n\nCode\ncustom_contrasts &lt;- list(\n \"2 station, opposite vs same\" = \n   c(1, -1, 0, 0, 0, 0, -1, 1),\n \"1 station, opposite vs same\" = \n   c(0, 0, 1, -1, -1, 1, 0, 0),\n \"1 vs 2 station (opposite)\" = \n   c(-1, 0, 2, 0, 0, 2, 0, -1),\n \"1 vs 2 station (same)\" = \n   c(0, -1, 0, 2, 2, 0, -1, 0)\n)\n# Set up contrasts with correction\ncont_emm &lt;- contrast(\n  object = emm, \n  method = custom_contrasts,\n  adjust = \"scheffe\")\n# Tests and p-values\ncont_emm |&gt;\n  test(scheffe.rank = 7)\n\n\n contrast                    estimate    SE  df t.ratio p.value\n 2 station, opposite vs same     2.24 0.410 194   5.470  0.0002\n 1 station, opposite vs same     1.71 0.415 194   4.129  0.0206\n 1 vs 2 station (opposite)       2.27 0.644 194   3.525  0.0942\n 1 vs 2 station (same)           1.09 0.665 194   1.636  0.9120\n\nP value adjustment: scheffe method with rank 7 \n\n\nCode\n# Tests with confidence intervals\ncont_emm |&gt;\n  confint(scheffe.rank = 7,\n          level = 0.99)\n\n\n contrast                    estimate    SE  df lower.CL upper.CL\n 2 station, opposite vs same     2.24 0.410 194    0.450     4.04\n 1 station, opposite vs same     1.71 0.415 194   -0.102     3.53\n 1 vs 2 station (opposite)       2.27 0.644 194   -0.546     5.08\n 1 vs 2 station (same)           1.09 0.665 194   -1.821     4.00\n\nConfidence level used: 0.99 \nConf-level adjustment: scheffe method with rank 7 \n\n\nEven with a correction for multiple testing, there is strong difference in perceived distance for one station away (direction of travel vs opposite) and similarly for two stations. There is mild evidence that, in one direction, the perceived distance between stations is not equal. A potential criticism, in addition to the design of the scale, would be about real distance between stations may not be the same."
  },
  {
    "objectID": "example/twowayanova.html#footnotes",
    "href": "example/twowayanova.html#footnotes",
    "title": "Two-way analysis of variance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn general, for unbalanced data, one would use car::Anova with type = 2 or type = 3 effects.↩︎\nThe type 3 effects compare the model with interactions and main effects to one that includes the interaction, but removes the main effects. Not of interest in the present context.↩︎\nThe second term specifies the default option for continuous variables.↩︎"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Content (): This page contains the readings, slides, and recorded lectures for the week. Read and watch these before our in-person class.\nExample (): This page contains fully annotated R code and other supplementary information that you can use as a reference for your evaluations. This is only a reference page—you don’t have to necessarily do anything here. Some sections also contain videos of me live coding the examples so you can see what it looks like to work in real time. This page will be very helpful as you work on your evaluations.\nEvaluations (): This page contains the instructions for each evaluation. Weekly check-ins and problem sets are due by noon on the day before class.\n\n\nIntroduction\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nExample\n\n\nEvaluations\n\n\n\n\n\n\nSession 1\n\n\n\n\nJanuary 8\n\n\nIntroduction to experimental designs and motivation\n\n\n\n\n\n\n\n\n\n\n\n\n\nJanuary 14\n\n\nWeekly check-in 1  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nJanuary 14\n\n\nProblem set 1  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nJanuary 8\n\n\nInstalling R and Rstudio\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession 2\n\n\n\n\nJanuary 15\n\n\nReview of key statistics concepts\n\n\n\n\n\n\n\n\n\n\n\n\n\nJanuary 15\n\n\nExample 1 – Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\nJanuary 21\n\n\nWeekly check-in 2  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nJanuary 21\n\n\nProblem set 2  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompletely randomized designs\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nExample\n\n\nEvaluations\n\n\n\n\n\n\nSession 3\n\n\n\n\nJanuary 22\n\n\nCompletely randomized designs with one factor\n\n\n\n\n\n\n\n\n\n\n\n\n\nJanuary 22\n\n\nExample 2 – One-way ANOVA\n\n\n\n\n\n\n\n\n\n\n\n\n\nJanuary 28\n\n\nWeekly check-in 3  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nJanuary 28\n\n\nProblem set 3  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession 4\n\n\n\n\nJanuary 29\n\n\nContrasts and multiple testing\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 4\n\n\nWeekly check-in 4  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 4\n\n\nProblem set 4  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession 5\n\n\n\n\nFebruary 5\n\n\nComplete factorial designs\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 5\n\n\nExample 3 – Two-way ANOVA\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 11\n\n\nWeekly check-in 5  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 11\n\n\nProblem set 5  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced topics\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nExample\n\n\nEvaluations\n\n\n\n\n\n\nSession 6\n\n\n\n\nFebruary 12\n\n\nMultiway factorial designs\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 12\n\n\nExample 4 – Three-way ANOVA\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 18\n\n\nWeekly check-in 6  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 18\n\n\nProblem set 6  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession 7\n\n\n\n\nFebruary 19\n\n\nBlock designs and analysis of covariance\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 19\n\n\nExample 5 – Analysis of covariance\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 25\n\n\nWeekly check-in 7  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 25\n\n\nProblem set 7  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBreak\n\n\n\n\nFebruary 24–March 1\n\n\nWinter break\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession 8\n\n\n\n\nMarch 4\n\n\nEffect sizes and the replication crisis\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 4\n\n\nExample 6 – Effect size and power\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 10\n\n\nWeekly check-in 8  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 10\n\n\nProblem set 8  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepeated measure designs\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nExample\n\n\nEvaluations\n\n\n\n\n\n\nSession 9\n\n\n\n\nMarch 11\n\n\nRepeated measures\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 11\n\n\nExample 7 – Repeated measures ANOVA\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 11\n\n\nExample 8 – MANOVA\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 17\n\n\nWeekly check-in 9  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 17\n\n\nProblem set 9  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession 10\n\n\n\n\nMarch 18\n\n\nMixed models\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 18\n\n\nExample 9 – Linear mixed models\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 24\n\n\nWeekly check-in 10  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 24\n\n\nProblem set 10  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausal inference\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nExample\n\n\nEvaluations\n\n\n\n\n\n\nBreak\n\n\n\n\nMarch 25\n\n\nIntro to causal inference\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession 11\n\n\n\n\nMarch 31\n\n\nWeekly check-in 11  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 29–April 1\n\n\nEaster\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession 12\n\n\n\n\nApril 8\n\n\nLinear mediation analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 8\n\n\nExample 10 – Linear causal mediation\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 14\n\n\nWeekly check-in 12  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 14\n\n\nProblem set 11  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategorical data\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nExample\n\n\nEvaluations\n\n\n\n\n\n\nSession 13\n\n\n\n\nApril 15\n\n\nCategorical data, nonparametric test and final review\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 15\n\n\nExample 11 – Nonparametric tests\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 15\n\n\nExample 12 – Count data\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 19\n\n\nWeekly check-in 13  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 19\n\n\nProblem set 12  (submit by 12:00:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinal\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nExample\n\n\nEvaluations\n\n\n\n\n\n\nEnd of term\n\n\n\n\nApril 19\n\n\nPaper review  (submit by 23:55:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 20\n\n\nFinal exam  (submit by 13:30:00)"
  }
]