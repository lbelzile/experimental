{
  "hash": "26853428aecd4893158af8b7718b0398",
  "result": {
    "markdown": "---\ntitle: \"Nonparametric tests\"\ntype: docs\neditor_options: \n  chunk_output_type: console\n---\n\n\n## Study 1: Distraction from smartwatches\n\nWe consider a within-subject design from @Brodeur:2021, who conducted an experiment to check distraction while driving from different devices including smartwatches using a virtual reality environment. The response is the number of road safety violations during the task.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(coin, quietly = TRUE)\ndata(BRLS21_T3, package = \"hecedsm\")\nstr(BRLS21_T3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nClasses 'tbl_df', 'tbl' and 'data.frame':\t124 obs. of  3 variables:\n $ task      : Factor w/ 4 levels \"phone\",\"watch\",..: 1 2 3 4 1 2 3 4 1 2 ...\n $ nviolation: int  8 4 10 12 1 5 0 7 5 6 ...\n $ id        : Factor w/ 31 levels \"1\",\"2\",\"3\",\"4\",..: 6 6 6 6 11 11 11 11 26 26 ...\n```\n:::\n\n```{.r .cell-code}\nxtabs(~ task + id, data = BRLS21_T3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         id\ntask      1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n  phone   1 1 1 1 1 1 1 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n  watch   1 1 1 1 1 1 1 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n  speaker 1 1 1 1 1 1 1 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n  texting 1 1 1 1 1 1 1 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n         id\ntask      27 28 29 30 31\n  phone    1  1  1  1  1\n  watch    1  1  1  1  1\n  speaker  1  1  1  1  1\n  texting  1  1  1  1  1\n```\n:::\n:::\n\n\nA quick inspection reveals that the data are balanced with four tasks and 31 individuals. We can view the within-subject design with a single replication as a complete block design (with `id` as block) and `task` as experimental manipulation.\n\nHow could we compare the different tasks? The data here are clearly very far from normally distributed and there are notable outliers among the residuals, as evidenced by @fig-normqqplot. Conclusions probably wouldn't be affected by using an analysis of variance, but it may be easier to convince reviewers that the findings are solid by ressorting to nonparametric procedures.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Normal quantile-quantile plot of the block design. There are many outliers](nonparametric_files/figure-html/fig-normqqplot-1.png){#fig-normqqplot fig-align='center' width=80%}\n:::\n:::\n\n\nBoth the Friedman and the Quade tests are obtained by computing ranks within each block (participant) and then performing a two-way analysis of variance. The Friedman test is less powerful than Quade's with a small number of groups. Both are applicable for block designs with a single factor. We can also obtain effect sizes for the rank test, termed Kendall's $W$. A value of 1 indicates complete agreement in the ranking: here, this would occur if the ranking of the number of violations was the same for each participant.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfriedman <- coin::friedman_test(\n  nviolation ~ task | id,\n  data = BRLS21_T3)\nquade <- coin::quade_test(\n  nviolation ~ task | id,\n  data = BRLS21_T3)\neff_size <- effectsize::kendalls_w(\n  x = \"nviolation\", \n  groups = \"task\", \n  blocks = \"id\", \n  data = BRLS21_T3)\n```\n:::\n\n\n\nThe Friedman test is obtained by replacing observations by the rank within each block (so rather than the number of violations per task, we compute the rank among the four tasks). The Friedman's test statistic is $18.97$ and is compared to a benchmark $\\chi^2_3$ distribution, yielding a $p$-value of $3\\times 10^{-4}$. The estimated agreement (effect size) is $0.2$.\n\nThe test reveals significant differences in the number of road safety violations across tasks. We could therefore perform all pairwise differences using the signed-rank test and adjust $p$-values to correct for the fact we have performed six hypothesis tests.\n\n\nTo do this, we modify the data and map them to wide-format (each line corresponds to an individual). We can then feed the data to compute differences, here for `phone` vs `watch`. We could proceed likewise for the five other pairwise comparisons and then adjust $p$-values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsmartwatch <- tidyr::pivot_wider(\n  data = BRLS21_T3,\n  names_from = task,\n  values_from = nviolation)\ncoin::wilcoxsign_test(phone ~ watch,\n                      data = smartwatch)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tAsymptotic Wilcoxon-Pratt Signed-Rank Test\n\ndata:  y by x (pos, neg) \n\t stratified by block\nZ = 0.35399, p-value = 0.7233\nalternative hypothesis: true mu is not equal to 0\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\nYou can think of the test as performing a paired $t$-test for the 31 signed ranks $R_i =\\mathsf{sign}(D_i) \\mathsf{rank}(|D_i|)$ and testing whether the mean is zero. The $p$-value obtained by doing this after discarding zeros is $0.73$, which is pretty much the same as the more complicated approximation.\n\n\n## Study 2: Online vs in-person meetings\n\n@Brucks.Levav:2022 measure the attention of participants based on condition using an eyetracker.\n\nWe compare here the time spend looking at the partner by experimental condition (face-to-face or videoconferencing). The authors used a Kruskal--Wallis test, but this is equivalent to Wilcoxon's rank-sum test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(BL22_E, package = \"hecedsm\")\nmww <- coin::wilcox_test(\n  partner_time ~ cond, \n  data = BL22_E, \n  conf.int = TRUE)\nwelch <- t.test(partner_time ~ cond, \n  data = BL22_E, \n  conf.int = TRUE)\nmww\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tAsymptotic Wilcoxon-Mann-Whitney Test\n\ndata:  partner_time by cond (f2f, video)\nZ = -6.4637, p-value = 1.022e-10\nalternative hypothesis: true mu is not equal to 0\n95 percent confidence interval:\n -50.694 -25.908\nsample estimates:\ndifference in location \n               -37.808 \n```\n:::\n:::\n\n\nThe output of the test includes, in addition to the $p$-value for the null hypothesis that both median time are the same, a confidence interval for the time difference (in seconds). This is obtained by computing all average of pairwise differences between observations of the two groups, so-called Walsh's averages. The Hodges--Lehmann estimate of location (the median of Walsh's differences) is $-37.81$ seconds, with a 95% confidence interval for the difference of $[-50.69, -25.91]$ seconds.\n\nThese can be compared with the usual Welch's two-sample $t$-test with unequal variance. The estimated mean difference is $-39.69$ seconds for face-to-face vs group video, with a 95% confidence interval of  $[-52.93, -26.45]$.\n\nIn either case, it's clear that the videoconferencing translates into longer time spent gazing at the partner than in-person meetings.\n\n\n",
    "supporting": [
      "nonparametric_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}