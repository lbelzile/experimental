{
  "hash": "65480ebd93e2cae75e96793ebd75f3ca",
  "result": {
    "markdown": "---\ntitle: \"Effect size and power\"\nlinktitle: \"Effect size and power\"\ntype: docs\neditor_options: \n  chunk_output_type: console\nexecute:\n  echo: true\n  eval: true\n  message: false\n  warning: false\n  cache: true\n  fig-align: 'center'\n  out-width: '80%'\n---\n\n\n\n\n\n# Notebook\n\nThis example is devoted to correct calculation and reporting of effect sizes across a variety of experimental designs, including multi-way analysis of variance with and without blocking factors. We make use of the comprehensive `effectsize` package throughout.^[There are many other alternatives.]\n\nWe also showcase how effect sizes can be recovered from the output of an analysis of variance table or from the reported test statistics and degrees of freedom, thus highlighting the impact of accurately reporting the latter.\n\nWe finally consider calculation of power in various replication studies using `WebPower` and G*Power; the latter considers the setup from [the Power exercise series](/example/power.html).\n\n\n::: {.cell hash='effectsizepower_cache/html/show-youtube-list_af5e1052e240a4e36668d3deec35b588'}\nThere's a set of videos that walks through each section below. To make it easier for you to jump around the video examples, I cut the long video into smaller pieces and included them all in [one YouTube playlist](https://www.youtube.com/playlist?list=PLUB8VZzxA8IuWyHUHnVzSjQ0-aAHAmUvr).\n\n- [Effect size and power](https://www.youtube.com/watch?v=https://youtu.be/2U8j19mKrWQ&list=PLUB8VZzxA8IuWyHUHnVzSjQ0-aAHAmUvr)\n- [Calculations using G*Power](https://www.youtube.com/watch?v=https://youtu.be/3f4ZCCVzkbY&list=PLUB8VZzxA8IuWyHUHnVzSjQ0-aAHAmUvr)\n\nYou can also watch the playlist (and skip around to different sections) here:\n\n<div class=\"ratio ratio-16x9\">\n<iframe src=\"https://www.youtube.com/embed/playlist?list=PLUB8VZzxA8IuWyHUHnVzSjQ0-aAHAmUvr\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n:::\n\n\n\n\n\n## Effect sizes\n\n\nEffect size typically serve three purpose: \n\n1. inform readers of the magnitude of the effect, \n2. provide a standardized quantity that can be combined with others in a meta-analysis, or \n3. serve as proxy in a power study to estimate the minimum number of observations needed.\n\nIf you report the exact value of the test statistic, the null distribution and (in short) all elements of an analysis of variance table in a complex design, it is possible by using suitable formulae to recover effect sizes, as they are functions of the test statistic summaries, degrees of freedom and correlation between observations (in the case of repeated measures).\n\n\nThe `effectsize` package includes a variety of estimators for standardized difference or ratio  of variance. For example, for the latter, we can retrieve Cohen's $f$ via `cohens_f`, $\\widehat{\\epsilon}^2$ via `epsilon_squared` or $\\widehat{\\omega}^2$ via `omega_squared`. \n\n\nBy default, in a design with more than one factor, the partial effects are returned (argument `partial = TRUE`) --- if there is a single factor, these coincide with the total effects and the distinction is immaterial. \n\n\nThe `effectsize` package reports confidence intervals^[Really, these are fiducial intervals based on confidence distributions.] calculated using the pivot method described in @Steiger:2004. Check the documentation at `?effectsize::effectsize_CIs` for more technical details.^[Note that, when the test statistic representing the proportion of variance explained is strictly positive, like a $F$ or $\\chi^2$ statistic, the corresponding effect size is an estimated percentage of variance returned by, e.g., $\\widehat{\\omega}^2$. To ensure consistency, the confidence intervals are one-sided, giving a lower bound (for the **minimum** effect size compatible with the data), while the upper bound is set to the maximum value, e.g., 1 for a proportion.]\n\nIn general, confidence intervals for effect sizes are very wide, including a large range of potential values and sometimes zero. This reflects the large uncertainty surrounding their estimation and should not be taken to mean that the estimated effect is null.\n\n## Example 1 - one-way ANOVA\n\nWe begin with the result of our [Example on one-way ANOVA](/example/onewayanova.html) in @Baumann:1992. If we consider the global $F$-test of equality in means, we can report as corresponding effect size the percentage of variance that is explained by the experimental condition, `group`.\n\n\n::: {.cell hash='effectsizepower_cache/html/unnamed-chunk-1_7e9c316e9bedb18316aa5918b55d1dc4'}\n\n```{.r .cell-code}\nlibrary(effectsize)\ndata(BSJ92, package = \"hecedsm\")\nmod <- aov(posttest2 - pretest2 ~ group,\n           data = BSJ92)\nprint_md(omega_squared(anova(mod), partial = FALSE))\n```\n\n::: {.cell-output-display}\nTable: Effect Size for ANOVA (Type I)\n\n|Parameter | Omega2|      95% CI |\n|:---------|------:|:------------|\n|group     |   0.16|[0.03, 1.00] |\nOne-sided CIs: upper bound fixed at [1.00].\n:::\n:::\n\n\nThe estimator employed is $\\widehat{\\omega}^2$ and could be obtained directly using the formula provided in the slides. For a proportion of variance, the number is medium according to @Cohen:1988 definition. Using $\\widehat{R}^2 \\equiv \\widehat{\\eta}^2$ as estimator instead would give an estimated proportion of variance of 0.188, a slightly higher number.\n\nHaving found a significant difference in mean between groups, one could be interested in computing estimated marginal means and contrasts based on the latter. The `emmeans` function has a method for computing effect size (Cohen's $d$) for pairwise differences if provided with the denominator standard deviation $\\sigma$ and the degrees of freedom associated with the latter (i.e., how many observations were left from the total sample size after subtracting the number of subgroup means). \n\n\n::: {.cell hash='effectsizepower_cache/html/unnamed-chunk-2_93c3bc39257f52dde7a68725a06c2cbe'}\n\n```{.r .cell-code}\nlibrary(emmeans)\npair_diff <- emmeans(\n  mod, spec = \"group\") |> \n  eff_size(sigma = sigma(mod), \n           edf = df.residual(mod))\n```\n:::\n\n\nThe confidence intervals reported by `emmeans` for $t$-tests are symmetric and different in nature from the one obtained previously.\n\nTechnical aside: while it is possible to create a $t$-statistic for a constrast by dividing the contrast estimator by it's standard error, the construction of Cohen's $d$ here for the contrast consisting of, e.g., the pairwise difference between `DRTA` and `TA` would take the form\n$$\nd_{\\text{DRTA}- \\text{TA}} = \\frac{\\mu_{\\text{DRTA}}- \\mu_{\\text{TA}}}{\\sigma},\n$$\nwhere the denominator stands for the standard deviation of the observations.^[It isn't always obvious when marginalizing out a one-way ANOVA from a complex design or when we have random effects or blocking factor what the estimated standard deviation should be, so it is left to the user to specify the correct quantity.]\n\n\n\n## Example 2: Sample size for replication studies\n\nArmed with effect sizes and a desired level of power, it is possible to determine the minimum number of observations that would yield such effect.\n\n\n@Johnson:2014 performs a replication study of Schnall, Benton, and Harvey (2008) who conjectured that physical cleanliness reduces the severity of moral judgments.\n\nThe following excerpt from the paper explain how sample size for the replication were calculated.\n\n> In Experiment 2, the critical test of the cleanliness manipulation on ratings of morality was significant, $F(1, 41) = 7.81$, $p=0.01$, $d=0.87$, $N=44$. Assuming $\\alpha=0.05$, the achieved power in this experiment was $0.80$. Our proposed research will attempt to replicate this experiment with a level of power = $0.99$. This will require a minimum of 100 participants (assuming equal sized groups with $d=0.87$) so we will collect data from 115 participants to ensure a properly powered sample in case of errors.\n\n\nThe first step is to try and compute the effect size, here Cohen's $d$, from the reported $F$ statistic to make sure it matches the quoted value.\n\n\n::: {.cell hash='effectsizepower_cache/html/unnamed-chunk-3_6ed554570ff1db26aa870c2c3b828abc'}\n\n```{.r .cell-code}\ndhat <- effectsize::F_to_d(\n  f = 7.81,\n  df = 1, \n  df_error = 41)\n```\n:::\n\n\nThis indeed coincides with the value reported for Cohen's $d$ estimator. We can then plug-in this value in the power function with the desired power level $0.99$ to find out a minimal number of 50 participants in each group, for a total of 100 if we do a pairwise comparison using a two-sample $t$-test.\n\n\n::: {.cell hash='effectsizepower_cache/html/unnamed-chunk-4_07e8ee5ddec78bdbef81c02c589bbdb3'}\n\n```{.r .cell-code}\nWebPower::wp.t(\n  d = 0.87,\n  power = 0.99,\n  type = \"two.sample\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTwo-sample t-test\n\n           n    d alpha power\n    49.53039 0.87  0.05  0.99\n\nNOTE: n is number in *each* group\nURL: http://psychstat.org/ttest\n```\n:::\n:::\n\n\nThe `effectsize` package includes many functions to convert $F$ and $t$ statistics to effect sizes.^[As the example of @Baumann:1992 showed, however, not all statistics can be meaningfully converted to effect size.]\n\n## Example 3: two-way ANOVA with unbalanced data\n\nWhile software can easily compute effect sizes, the user should not blindly rely on the output, but rather think about various elements using the following guiding principles:\n\n- we are interested in partial effects when there are multiple factors\n- the denominator should consist of the variance of the effect of interest (say factor $A$), the variance of blocking factors and random effects and that of all interactions associated with them.\n\nConsider next the [unbalanced two-way ANOVA example](/example/twowayanova.html) with Study 1 of @Maglio/Polman:2014.\n\nWe pass here directly the output of the model. We use the `lm` function with the mean-to-zero parametrization, since we have unbalanced data.\n\n\n::: {.cell hash='effectsizepower_cache/html/unnamed-chunk-5_a089fb15226d672ac20456f6fb4fa05c'}\n\n```{.r .cell-code}\ndata(MP14_S1, package = 'hecedsm')\n# Force mean-to-zero parametrization\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\n# Estimate two-way ANOVA with interaction\nmodel <- lm(distance ~ station*direction, \n            data = MP14_S1)\n# Test only the interaction\nout <- car::Anova(model, type = 3)\n```\n:::\n\n\n\nBy default, the variance terms for each factor and interaction are estimated using the `anova` call. When the data aren't balanced and you have multiple factors in the mean equation, these are the sequential sum of square estimates (type I). This means that the resulting effect size would depend on the order in which you specify the terms, an unappealing feature. The model can alternatively take as argument the analysis of variance table produced by the `Anova` function in package `car`, e.g., `car::Anova(..., type = 3)`. Note that it is of paramount importance to pass the correct arguments and to use the mean-to-zero parametrization in order to get sensible results. The package warns user about this.\n\n\n::: {.cell hash='effectsizepower_cache/html/unnamed-chunk-6_d1d2cce6f48a65a168e267d853a3a486'}\n\n```{.r .cell-code}\nomsq <- omega_squared(out)\n```\n:::\n\n\nThe estimated effect size for the main effect of `direction` is negative with $\\widehat{\\omega}^2_{\\langle \\text{direction}\\rangle}$: either reporting a negative value or zero. This reflects that the estimated effect is very insignificant.\n\nEquipped with the estimated effect size, we can now transform our partial $\\widehat{\\omega}^2_{\\langle\\text{AB}\\rangle}$ measure into an estimated Cohen's $f$ via \n$$\\widetilde{f} = \\left( \\frac{\\widehat{\\omega}^2}{1-\\widehat{\\omega}^2}\\right)^{1/2},$$ which is then fed into `WebPower` package functionality to compute the *post-hoc* power. Since we are dealing with a two-way ANOVA with 8 subgroups, we set `ng=8` and then `ndf` corresponding to the degrees of freedom of the estimated interaction (here $(n_a-1)\\times (n_b-1)=3$, the number of coefficients needed to capture the interaction).\n\nGiven all but one of the following collection \n\n1. the power,\n2. the number of groups and degrees of freedom from the design,\n3. the effect size and\n4. the sample size,\n\nit is possible to deduce the last one assuming a balanced sample. Below, we use the information to compute the so-called *post-hoc* power. Such terminology is misleading because there is no guarantee that we are under the alternative, and effect sizes are really noisy proxy so the range of potential values for the missing ingredient is oftentimes quite large. Because studies in the literature have inflated effect size, the power measures are more often than not misleading.\n\n\n::: {.cell hash='effectsizepower_cache/html/unnamed-chunk-7_3a0530daa65c9506cc2a684be4a1c5b5'}\n\n```{.r .cell-code}\n# Power calculations\n# Convert omega-squared to Cohen's f\ncohensf1 <- effectsize::eta2_to_f(\n  omsq$Omega2_partial[3])\nWebPower::wp.kanova(\n  n = 202, # sample size, assumed *balanced*\n  ndf = 3, # degrees of freedom\n  f = cohensf1, # Cohen's f estimate\n  ng = 8) # number of subgroups of ANOVA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMultiple way ANOVA analysis\n\n      n ndf ddf         f ng alpha     power\n    202   3 194 0.4764222  8  0.05 0.9999821\n\nNOTE: Sample size is the total sample size\nURL: http://psychstat.org/kanova\n```\n:::\n:::\n\n\nHere, the interaction is unusually strong (a fifth of the variance is explained by it!) and we have an extremely large *post-hoc* power estimate. This is rather unsurprising given the way the experiment was set up.\n\nWe can use the same function to determine how many observations the study would need to minimally achieve a certain power, below of 99% --- the number reported must be rounded up to the nearest integer. Depending on the design or function, this number may be the overall sample size or the sample size per group.\n\n\n::: {.cell hash='effectsizepower_cache/html/unnamed-chunk-8_c6c0e9db251c764ea3e1f1e8c16afa39'}\n\n```{.r .cell-code}\n# Sample size calculations\ncohensf2 <- cohens_f(out)$Cohens_f_partial[3]\nWebPower::wp.kanova(\n  ndf = 3, \n  f = cohensf1, \n  ng = 8, \n  power = 0.99)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMultiple way ANOVA analysis\n\n        n ndf      ddf         f ng alpha power\n    107.8   3 99.80004 0.4764222  8  0.05  0.99\n\nNOTE: Sample size is the total sample size\nURL: http://psychstat.org/kanova\n```\n:::\n\n```{.r .cell-code}\nWebPower::wp.kanova(\n  ndf = 3, \n  f = cohensf2, \n  ng = 8,\n  power = 0.99)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMultiple way ANOVA analysis\n\n           n ndf      ddf         f ng alpha power\n    97.61394   3 89.61394 0.5017988  8  0.05  0.99\n\nNOTE: Sample size is the total sample size\nURL: http://psychstat.org/kanova\n```\n:::\n:::\n\n\nThe total sample size using $\\widehat{\\omega}^2$ is 108, whereas using the biased estimator $\\widehat{f}$ directly (itself obtained from $\\widehat{\\eta}^2$) gives 98: this difference of 10 individuals can have practical implications.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}