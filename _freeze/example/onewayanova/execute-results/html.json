{
  "hash": "03e505e7ea1e3f029727a000843afad5",
  "result": {
    "markdown": "---\ntitle: \"One-way analysis of variance\"\ntype: docs\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n# Videos\n\nThe **R** code created in the video [can be downloaded here](/example/onewayanova-video.R) and the [SPSS code here](/example/oneway.sps).\n\n\nThere's a set of videos that walks through each section below. To make it easier for you to jump around the video examples, I cut the long video into smaller pieces and included them all in [one YouTube playlist](https://www.youtube.com/playlist?list=PLUB8VZzxA8IvHyjTG5P7ZfyTQWEUbFhcc).\n\n- [ANOVA table](https://www.youtube.com/watch?v=7ysgXYx6Rwg&list=PLUB8VZzxA8IvHyjTG5P7ZfyTQWEUbFhcc)\n- [Contrasts and estimated marginal means](https://www.youtube.com/watch?v=KJ99KgeApNs&list=PLUB8VZzxA8IvHyjTG5P7ZfyTQWEUbFhcc)\n- [Multiple testing](https://www.youtube.com/watch?v=dM1IkaVFy6w&list=PLUB8VZzxA8IvHyjTG5P7ZfyTQWEUbFhcc)\n\nYou can also watch the playlist (and skip around to different sections) here:\n\n<div class=\"ratio ratio-16x9\">\n<iframe src=\"https://www.youtube.com/embed/playlist?list=PLUB8VZzxA8IvHyjTG5P7ZfyTQWEUbFhcc\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n\n\n# Notebook\n\nThis notebook shows the various manipulation that an experimenter may undertake to test whether the population averages are the same based on some experimental data.\n\nRecall the setup of [Example 1](/example/introduction.html). \n\n\n\n\n\n## Hypothesis testing\n\nWe can begin by testing whether the group average for the initial measurements at the beginning of the study, prior to any treatment) have the same mean. Strong indication against this null hypothesis would be evidence of a potential problem with the randomization. We compute the one-way analysis of variance table, which includes quantities that enter the _F_-statistic (named after its large-sample null distribution, which is an _F_-distribution).^[The _F_ stands for Fisher, who pioneered much of the work on experimental design.]\n\n\nWe use the `lm` function function to fit the model: an analysis of variance is a special case of linear model, in which the explanatory variables are categorical variables. The first argument of the function is a formula `response ~ treatment`, where `treatment` is the factor or categorical variable indicating the treatment.\n\nThe function `anova` is a method: when applied to the result of a call to `lm`, it produces an analysis of variance table including among other things the following information:\n\n1) the value of the test statistic (`F value`)\n2) the between and within sum of square (these are quantities that enter in the formula of the statistic)\n3) the degrees of freedom of the _F_ null distribution (column `Df`): these specify the parameters of the large-sample approximation for the null distribution, which is our default benchmark.\n4) the mean square, which are sum of squares divided by the degrees of freedom.\n5) The _p_-value (`Pr(>F)`), which gives the probability of observing an outcome as extreme if there was no difference.\n\nWe need to decide beforehand the level of the test (typically 5\\% or lower): this is the percentage of times we will reject the null hypothesis when its true based on observing an extreme outcome. We are asked to perform a binary decision (reject or fail to reject): if the _p_-value is less than the level, we 'reject' the null hypothesis of equal (population) means.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(BSJ92, package = 'hecedsm')\nmod_pre <- aov(formula = pretest1 ~ group,\n                     data = BSJ92)\nanova_tab <- broom::tidy(anova(mod_pre))\n# Save the output in a tibble to get more meaningful column names\n# Elements include `statistic`, `df`, `p.value`\n```\n:::\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Analysis of variance table for pre-test 1</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Terms </th>\n   <th style=\"text-align:right;\"> Degrees of freedom </th>\n   <th style=\"text-align:right;\"> Sum of squares </th>\n   <th style=\"text-align:right;\"> Mean square </th>\n   <th style=\"text-align:right;\"> Statistic </th>\n   <th style=\"text-align:right;\"> p-value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> group </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 20.58 </td>\n   <td style=\"text-align:right;\"> 10.288 </td>\n   <td style=\"text-align:right;\"> 1.13 </td>\n   <td style=\"text-align:right;\"> 0.33 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Residuals </td>\n   <td style=\"text-align:right;\"> 63 </td>\n   <td style=\"text-align:right;\"> 572.45 </td>\n   <td style=\"text-align:right;\"> 9.087 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n\nThere isn't strong evidence of difference in strength between groups prior to intervention. We can report the findings as follows:\n\nWe carried a one-way analysis for the pre-test results to ensure that the group abilities are the same in each treatment group; results show no significant differences at the 5\\% level ($F$ (2, 63) = 1.13, $p$ = 0.329).\n\nA similar result for the scores of the first post-test as response variable lead to strong evidence of difference between teaching methods.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Analysis of variance table for post-test 1</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Terms </th>\n   <th style=\"text-align:right;\"> Degrees of freedom </th>\n   <th style=\"text-align:right;\"> Sum of squares </th>\n   <th style=\"text-align:right;\"> Mean square </th>\n   <th style=\"text-align:right;\"> Statistic </th>\n   <th style=\"text-align:right;\"> p-value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> group </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 108.12 </td>\n   <td style=\"text-align:right;\"> 54.061 </td>\n   <td style=\"text-align:right;\"> 5.32 </td>\n   <td style=\"text-align:right;\"> 0.01 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Residuals </td>\n   <td style=\"text-align:right;\"> 63 </td>\n   <td style=\"text-align:right;\"> 640.50 </td>\n   <td style=\"text-align:right;\"> 10.167 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n\n\n## Contrasts and estimated marginal means\n\nWhile the $F$ test may strongly indicate that the means of each group are different, \nit doesn't indicate which group is different from the rest.\nBecause we can compare different groups doesn't mean these comparisons are of any \nscientific interest and going fishing by looking at all pairwise differences is \nnot necessarily the best strategy. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(emmeans) #load package\nmod_post <- aov(posttest1 ~ group, data = BSJ92)\nemmeans_post <- emmeans(object = mod_post, \n                        specs = \"group\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Estimated group averages with standard errors and 95% confidence intervals for post-test 1.</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Terms </th>\n   <th style=\"text-align:right;\"> Marginal mean </th>\n   <th style=\"text-align:right;\"> Standard error </th>\n   <th style=\"text-align:right;\"> Degrees of freedom </th>\n   <th style=\"text-align:right;\"> Lower limit (CI) </th>\n   <th style=\"text-align:right;\"> Upper limit (CI) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> DR </td>\n   <td style=\"text-align:right;\"> 6.68 </td>\n   <td style=\"text-align:right;\"> 0.68 </td>\n   <td style=\"text-align:right;\"> 63 </td>\n   <td style=\"text-align:right;\"> 5.32 </td>\n   <td style=\"text-align:right;\"> 8.04 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> DRTA </td>\n   <td style=\"text-align:right;\"> 9.77 </td>\n   <td style=\"text-align:right;\"> 0.68 </td>\n   <td style=\"text-align:right;\"> 63 </td>\n   <td style=\"text-align:right;\"> 8.41 </td>\n   <td style=\"text-align:right;\"> 11.13 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> TA </td>\n   <td style=\"text-align:right;\"> 7.77 </td>\n   <td style=\"text-align:right;\"> 0.68 </td>\n   <td style=\"text-align:right;\"> 63 </td>\n   <td style=\"text-align:right;\"> 6.41 </td>\n   <td style=\"text-align:right;\"> 9.13 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThus, we can see that `DRTA` has the highest average, followed by `TA` and directed reading (`DR`).\nThe purpose of @Baumann:1992 was to make a particular comparison between treatment groups. \nFrom the abstract:\n\n> The primary quantitative analyses involved two planned orthogonal contrasts—effect of instruction (TA + DRTA vs. 2 x DRA) and intensity of instruction (TA vs. DRTA)—for three whole-sample dependent measures: (a) an error detection test, (b) a comprehension monitoring questionnaire, and (c) a modified cloze test.\n\nA **contrast** is a particular linear combination of the different groups, \ni.e., a sum of weighted mean the coefficients of which sum to zero. \nTo test the hypothesis of @Baumann:1992 and writing $\\mu$ to denote the population average, \nwe have $\\mathscr{H}_0: \\mu_{\\mathrm{TA}} + \\mu_{\\mathrm{DRTA}} = 2 \\mu_{\\mathrm{DRA}}$ \nor rewritten slightly\n$$\\begin{align*}\n\\mathscr{H}_0: - 2 \\mu_{\\mathrm{DR}} + \\mu_{\\mathrm{DRTA}} + \\mu_{\\mathrm{TA}} = 0.\n\\end{align*}$$\nwith weights $(-2, 1, 1)$; the order of the levels for the treatment are \n($\\mathrm{DRA}$, $\\mathrm{DRTA}$, $\\mathrm{TA}$) and it must match that of the coefficients.\nAn equivalent formulation is $(2, -1, -1)$ or $(1, -1/2, -1/2)$: in either case, the estimated differences will be different\n(up to a constant multiple or a sign change).\nThe vector of weights for $\\mathscr{H}_0:  \\mu_{\\mathrm{TA}} = \\mu_{\\mathrm{DRTA}}$ \nis, e.g.,($0$, $-1$, $1$): the zero appears because the first component, $\\mathrm{DRA}$ doesn't appear.\nThe two contrasts are orthogonal: these contrasts are special because the tests use disjoint bits of information \nabout the sample.^[This technical term means that the two vectors defining the contrasts are orthogonal: their inner product is thus zero:\n$(-2 \\times 0) + (1 \\times -1) + (1 \\times 1) = 0$. In practice, we specify contrasts because they answer\nquestions of scientific interest, not because of their fancy mathematical properties.] \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identify the order of the level of the variables\nwith(BSJ92, levels(group))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"DR\"   \"DRTA\" \"TA\"  \n```\n:::\n\n```{.r .cell-code}\n# DR, DRTA, TA (alphabetical)\ncontrasts_list <- list(\n  \"C1: DRTA+TA vs 2DR\" = c(-2, 1, 1), \n  # Contrasts: linear combination of means, coefficients sum to zero\n  # 2xDR = DRTA + TA => -2*DR + 1*DRTA + 1*TA = 0 and -2+1+1 = 0\n  \"C1: average (DRTA+TA) vs DR\" = c(-1, 0.5, 0.5), \n  #same thing, but halved so in terms of average\n  \"C2: DRTA vs TA\" = c(0, 1, -1),\n  \"C2: TA vs DRTA\" = c(0, -1, 1) \n  # same, but sign flipped\n)\ncontrasts_post <- \n  contrast(object = emmeans_post,\n           method = contrasts_list)\ncontrasts_summary_post <- summary(contrasts_post)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Estimated contrasts for post-test 1.</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Contrast </th>\n   <th style=\"text-align:right;\"> Estimate </th>\n   <th style=\"text-align:right;\"> Standard error </th>\n   <th style=\"text-align:right;\"> Degrees of freedom </th>\n   <th style=\"text-align:right;\"> t statistic </th>\n   <th style=\"text-align:right;\"> p-value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> C1: DRTA+TA vs 2DR </td>\n   <td style=\"text-align:right;\"> 4.18 </td>\n   <td style=\"text-align:right;\"> 1.67 </td>\n   <td style=\"text-align:right;\"> 63 </td>\n   <td style=\"text-align:right;\"> 2.51 </td>\n   <td style=\"text-align:right;\"> 0.01 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> C1: average (DRTA+TA) vs DR </td>\n   <td style=\"text-align:right;\"> 2.09 </td>\n   <td style=\"text-align:right;\"> 0.83 </td>\n   <td style=\"text-align:right;\"> 63 </td>\n   <td style=\"text-align:right;\"> 2.51 </td>\n   <td style=\"text-align:right;\"> 0.01 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> C2: DRTA vs TA </td>\n   <td style=\"text-align:right;\"> 2.00 </td>\n   <td style=\"text-align:right;\"> 0.96 </td>\n   <td style=\"text-align:right;\"> 63 </td>\n   <td style=\"text-align:right;\"> 2.08 </td>\n   <td style=\"text-align:right;\"> 0.04 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> C2: TA vs DRTA </td>\n   <td style=\"text-align:right;\"> -2.00 </td>\n   <td style=\"text-align:right;\"> 0.96 </td>\n   <td style=\"text-align:right;\"> 63 </td>\n   <td style=\"text-align:right;\"> -2.08 </td>\n   <td style=\"text-align:right;\"> 0.04 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nWe can look at these differences; since `DRTA` versus `TA` is a pairwise\ndifference, we could have obtained the $t$-statistic directly from the pairwise contrasts\nusing `pairs(emmeans_post)`. Note that the two different ways of writing the comparison between `DR` and the average of the other two methods yield different point estimates, but same inference (same $p$-values). For contrast $C_{1b}$, we get half the estimate (but the standard error is also halved) and likewise for the second contrasts we get an estimate of $\\mu_{\\mathrm{DRTA}} - \\mu_{\\mathrm{TA}}$ in the first case ($C_2$) and $\\mu_{\\mathrm{TA}} - \\mu_{\\mathrm{DRTA}}$: the difference in group averages is the same up to sign.\n\nWhat is the conclusion of our analysis of contrasts? \nIt looks like the methods involving teaching aloud have a strong impact on \nreading comprehension relative to only directed reading. The evidence is not as strong\nwhen we compare the method that combines directed reading-thinking activity and thinking aloud.\n\n## Multiple testing\n\nIn this example, we computed two contrasts (excluding the equivalent formulations) so\nsince these comparisons are planned, we could provide the $p$-values as is. However, if\nwe had computed many more tests, it would make sense to account for these so as not to inflate type I error (judicial mistake consisting in sending an innocent to jail).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrasts_list <- list(\n  \"C1: DRTA+TA vs 2DR\" = c(-2, 1, 1), \n  \"C2: DRTA vs TA\" = c(0, 1, -1)\n)\ncontrasts_post_scheffe <- \n  contrast(object = emmeans_post,\n           method = contrasts_list,\n           adjust = \"scheffe\") # for arbitrary contrasts\n# extract p-values\npvals_scheffe <- summary(contrasts_post_scheffe)$p.value\npvals_scheffe\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04951625 0.12333575\n```\n:::\n\n```{.r .cell-code}\n# Compute Bonferroni and Holm-Bonferroni correction\ncontrasts_post <- \n  contrast(object = emmeans_post,\n           method = contrasts_list,\n           adjust = \"none\") #default for custom contrasts\nraw_pval <- summary(contrasts_post)$p.value\np.adjust(p = raw_pval, method = \"bonferroni\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02920552 0.08313211\n```\n:::\n\n```{.r .cell-code}\np.adjust(p = raw_pval, method = \"holm\") #Bonferroni-Holm method\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02920552 0.04156606\n```\n:::\n:::\n\n\nIf we look at the _p_-values with the Scheffé's method for custom contrasts, we get 0.015 for contrast 1 and 0.042 for contrast 2: since we are only making two tests here, these are much bigger than the $p$-values from Holm's method which are 0.03 and 0.04. To try and avoid making type I error, we need to be more stringent to decide on rejection and this translates into bigger $p$-values, so lower power to detect. Try to use the less stringent method that still controls for the family wise error rate to preserve your power!\n\n## Model assumptions\n\nSince we have no repeated measurements and there were no detectable difference apriori between students, we can postulate that the records are independent.\n\nWe could test whether the variance are equal: in this case, there is limited evidence of unequal variance.\nThe data are of course not normal (because they consist of the counts of the number of insertions detected by pupils, which are integer-valued). However, we can see if there are extreme values and whether the residuals are far from normal. The simulated quantile-quantile plot shows that all points more or less align with the straight line and all fall within the confidence intervals, so there is no problem with this normality assumption (which anyway matters little). \n\nAre measurements additive? After assigning the pre-test 1, the experimenters adjusted the scale and made the post-test harder to avoid having maximum scores (considering that students also were more experienced). \n\n> Because the students performed at a higher-than-expected level on Pretest 1 (61% of all intruded sentences were correctly identified), the experimenters were concerned about a potential post-intervention ceiling effect on this posttest, an occurrence which could mask group differences. To reduce the likelihood of a ceiling effect, the intruded sentences for Posttest 1 were written so their inconsistencies were more subtle than those included in Pretest 1, which explains the somewhat lower level of performance on Posttest 1 (51% of all intruded sentences correctly identified).\n\nThis seems to have been successful since the maximum score is 15 out of 16 intrusions, while there were two students who scored 16 on the pre-test.\n\nThe next step is checking that the variability is the same in each group. Assuming equal variance is convenient because we can use more information (the whole sample) to estimate the level of uncertainty rather than solely the observations from each group. The more observations we use to estimate the variance, the more reliable our measure is (assuming that the variance were equal in each group to begin with). \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# test for equality of variance\ncar::leveneTest(posttest1 ~ group, data = BSJ92)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  2  2.1297 0.1273\n      63               \n```\n:::\n\n```{.r .cell-code}\n# Quantile-quantile plot\ncar::qqPlot(x = mod_post, # lm object\n            ylab = 'empirical quantiles', # change y-axis label\n            id = FALSE) # Don't print to console 'outlying' observations\n```\n\n::: {.cell-output-display}\n![](onewayanova_files/figure-html/testequalvariance-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Residual plot (linearity, but useless for one way ANOVA)\ncar::residualPlot(mod_post)\n```\n\n::: {.cell-output-display}\n![](onewayanova_files/figure-html/testequalvariance-2.png){width=672}\n:::\n:::\n\n\nIf we were worried about the possibility of unequal variances, we could fit the model by estimating the variance separately in each group. This does not materially change the conclusions about teaching methods relative to the directed reading benchmark.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noneway.test(posttest1 ~ group, data = BSJ92)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne-way analysis of means (not assuming equal variances)\n\ndata:  posttest1 and group\nF = 6.9878, num df = 2.00, denom df = 41.13, p-value = 0.00244\n```\n:::\n:::\n\n\n## Auxiliary and concomitant observation\n\nThe purpose of a good experimental design is to reduce the variability to better detect \ntreatment effects. In the above example, we could have added a concomitant variable (the pre-test score) \nthat captures the individual variability. This amounts to doing a paired comparison between post- and pre-test results. \nIt helps with the analysis because it absorbs the baseline strength of individual students: by subtracting their records, we get their individual average out of the equation and thus there is less variability.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_post_c <- lm(posttest1 ~ offset(pretest1) + group,\n                   data = BSJ92) \nanova_tab_c <- broom::tidy(anova(anova_post_c)) #anova table\n```\n:::\n\n\nCompare this ANOVA table with the preceding. We could repeat the same \nprocedure to compute the contrasts. \n\nUsing auxiliary information\nallows one to reduce the intrinsic variability: the estimated variance $\\widehat{\\sigma}^2$ is 6.66 with the auxiliary information and \n9.09 without: since we reduce the level of background noise, we get a higher signal-to-noise ratio. As a result, the _p_-value for the global test is smaller than with only `posttest1` as response.\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Analysis of variance table</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Terms </th>\n   <th style=\"text-align:right;\"> Degrees of freedom </th>\n   <th style=\"text-align:right;\"> Sum of squares </th>\n   <th style=\"text-align:right;\"> Mean square </th>\n   <th style=\"text-align:right;\"> Statistic </th>\n   <th style=\"text-align:right;\"> p-value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> group </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 168.21 </td>\n   <td style=\"text-align:right;\"> 84.106 </td>\n   <td style=\"text-align:right;\"> 12.64 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Residuals </td>\n   <td style=\"text-align:right;\"> 63 </td>\n   <td style=\"text-align:right;\"> 419.32 </td>\n   <td style=\"text-align:right;\"> 6.656 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n## References\n",
    "supporting": [
      "onewayanova_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n<link href=\"../site_libs/bsTable-3.3.7/bootstrapTable.min.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/bsTable-3.3.7/bootstrapTable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}