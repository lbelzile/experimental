---
title: "Analysis of covariance and moderation"
author: "Léo Belzile"
date: "`r Sys.Date()`"
output:
   xaringan::moon_reader:
      lib_dir: "libs"
      chakra: "libs/remark-latest.min.js"
      css: ["default", "css/ath-slides.css", "css/ath-inferno-fonts.css", "css/animate.css"]
      seal: false
      anchor_sections: false
      nature:
          highlightStyle: github
          highlightLines: false
          countIncrementalSlides: false
          ratio: "16:9"
      navigation:
        scroll: false
      editor_options: 
        chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      fig.retina = 3, 
                      fig.align = "center",
                      out.width="70%",
                      fig.asp = 0.5
                      )
                      
```

```{r packages-data, echo = FALSE, include=FALSE}
library(knitr)
options(knitr.kable.NA = '')
options(tidyverse.quiet = TRUE)
options(knitr.table.format = "html")
library(tidyverse)
library(patchwork)
```
```{r xaringanExtra, echo=FALSE}
xaringanExtra::use_xaringan_extra(c("tile_view","freezeframe","panelset","clipboard","broadcast"))
```

class: center middle main-title section-title-1

# Analysis of covariance and moderation

.class-info[

**Session 8**

.light[MATH 80667A: Experimental Design and Statistical Methods <br>
HEC Montréal
]

]

---
layout: false
name: ancova
class: center middle section-title section-title-7

# Analysis of covariance

---

layout: true
class: title title-7
---

# Covariates

.center[

.box-inv-7.large.sp-after-half[Covariate]
Explanatory measured **before** the experiment

Typically, cannot be acted upon.

.box-inv-7.large.sp-after-half[Example]
socioeconomic variables <br> environmental conditions

]


---

# IJLR: It's Just a Linear Regression...

All ANOVA models covered so far are linear regression model.

The latter says that 

$$\underset{\text{average response}\vphantom{l}}{\textsf{E}(Y_{i}\vphantom{\beta_p})}  = \underset{\text{linear (i.e., additive) combination of explanatories}}{\beta_0 + \beta_1 \mathrm{X}_{1i} + \cdots + \beta_p \mathrm{X}_{pi}}$$

In an ANOVA, the model matrix $\mathbf{X}$ simply includes columns with $-1$, $0$ and $1$ for group indicators that enforce sum-to-zero constraints.

---

# What's in a model?

In experimental designs, the explanatories are 

- experimental factors (categorical)
- continuous (dose-response)

.box-inv-7.medium[
Random assignment implies<br> no systematic difference between groups.
]

---

# ANCOVA = Analysis of covariance

 
- Analysis of variance with added continuous covariate(s) to reduce experimental error (similar to blocking).
- These continuous covariates are typically concomitant variables (measured alongside response).
- Including them in the mean response (as slopes) can help reduce the experimental error (residual error).

---

# Control to gain power!


.box-inv-7.sp-after-half[
Identify external sources of variations
]

- enhance balance of design (randomization)
- reduce mean squared error of residuals to increase power

These steps should in principle increase power **if** the variables used as control are correlated with the response.

---


# Example

Abstract of [van Stekelenburg et al. (2021)](https://doi.org/10.1177/09567976211007788)

> In three experiments with more than 1,500 U.S. adults who held false beliefs, participants first learned the value of scientific consensus and how to identify it. Subsequently, they read a news article with information about a scientific consensus opposing their beliefs. We found strong evidence that in the domain of genetically engineered food, this two-step communication strategy was more successful in correcting misperceptions than merely communicating scientific consensus.

???


Aart van Stekelenburg, Gabi Schaap, Harm Veling and Moniek Buijzen (2021), Boosting Understanding and Identification of Scientific Consensus Can Help to Correct False Beliefs, Psychological Science
https://doi.org/10.1177/09567976211007788
---

# Experiment 2: Genetically Engineered Food

We focus on a single experiment; preregistered exclusion criteria led to $n=442$ total sample size (unbalanced design).

Three experimental conditions:

.float-left.center[.box-7[`Boost`] .box-7[`Boost Plus`] .box-7[Consensus only (`consensus`)] ]


---

# Model formulation

Use `post` as response variable and `prior` beliefs as a control variable in the analysis of covariance.

> their response was measured on a visual analogue scale ranging from –⁠100 (I am 100% certain this is false) to 100 (I am 100% certain this is true) with 0 (I don’t know) in the middle.



---
# Plot of post vs prior response

```{r figExp2, eval = TRUE, echo = FALSE, out.width = '75%'}
knitr::include_graphics("img/07/Experiment2.png")
```

---

# Model formulation

Average for the $r$th replication of the $i$th experimental group is
$$\begin{align*}\mathsf{E}(\texttt{post}_{ir}) &= \mu + \alpha_i\texttt{condition}_i + \beta \texttt{prior}_{ir}.\\
\mathsf{Va}(\texttt{post}_{ir}) &= \sigma^2\end{align*}$$

We assume that there is no interaction between `condition` and `prior`
- the slopes for `prior` are the same for each `condition` group.
- the effect of prior is linear


---

# Contrasts of interest

1. Difference between average boosts (`Boost` and `BoostPlus`) and control (`consensus`)
2. Difference between `Boost` and `BoostPlus` (pairwise)

Inclusion of the `prior` score leads to increased precision for the mean (reduces variability).

---

# Contrasts with ANCOVA

- The estimated marginal means will be based on a fixed value of the covariate (rather than detrended values)
- In the `emmeans` package, the average of the covariate is used as value.
- the difference between levels of `condition` are the same for any value of `prior` (parallel lines), but the uncertainty changes.

Multiple testing adjustments:

- Methods of Bonferroni (prespecified number of tests) and Scheffé (arbitrary contrasts) still apply
- Can't use Tukey anymore (adjusted means are not independent anymore).

---

# Data analysis - loading data

```{r interaction, echo = FALSE, eval = TRUE, cache = TRUE}
library(emmeans)
options(contrasts = c("contr.sum", "contr.poly"))
data(SSVB21_S2, package = "hecedsm")
model1 <- lm(post ~ condition + prior, data = SSVB21_S2)
model2 <- lm(post ~ condition, data = SSVB21_S2)
levene <- car::leveneTest(
   rstudent(model1) ~ condition, 
   data = SSVB21_S2,
   center = 'mean')
```

```{r ancova1, echo = TRUE, eval = TRUE}
library(emmeans)
options(contrasts = c("contr.sum", "contr.poly"))
data(SSVB21_S2, package = "hecedsm")
# Check balance
with(SSVB21_S2, table(condition))
```


---

# Data analysis - scatterplot
.pull-left.small[
```{r ancova2, echo = TRUE, eval = FALSE}
library(ggplot2)
ggplot(data = SSVB21_S2,
       aes(x = prior, y = post)) + 
       geom_point() + 
       geom_smooth(method = "lm",
                   se = FALSE)
```
.small[Strong correlation; note responses that achieve max of scale.]
]
.pull-right[
```{r xaringanGraph, eval = TRUE, echo = FALSE, out.width = '70%'}
knitr::include_graphics("img/07/Experiment2_scatter.png")
```


]

---

# Data analysis - model
```{r ancova3, echo = TRUE, eval = FALSE}
# Check that the data are well randomized
car::Anova(lm(prior ~ condition, data = SSVB21_S2), type = 2)
# Fit linear model with continuous covariate
model1 <- lm(post ~ condition + prior, data = SSVB21_S2)
# Fit model without for comparison
model2 <- lm(post ~ condition, data = SSVB21_S2)
# Global test for differences
car::Anova(model1)
car::Anova(model2)
```


---

# Data analysis - ANOVA table
.pull-left.small[
```{r ancova3b, echo = FALSE, eval = TRUE}
kableExtra::kable(broom::tidy(car::Anova(model1, type = 2)), 
		 col.names = c("term",
			 "sum of squares",
			 "df",
			 "statistic",
			 "p-value"), 
		 digits = c(0,0,0,1,2,4)) %>% 
kableExtra::row_spec(2, bold = TRUE, color = "black", background = "yellow")
```
]
.pull-right.small[
```{r ancova3c, echo = FALSE, eval = TRUE}
kableExtra::kable(broom::tidy(car::Anova(model2, type = 2)), 
		 col.names = c("term",
			 "sum of squares",
			 "df",
			 "statistic",
			 "p-value"), 
		 digits = c(0,0,1,2,3)) %>% 
kableExtra::row_spec(2, bold = TRUE, color = "black", background = "yellow")
```
]

---

# Data analysis - contrasts

```{r ancova4a, echo = TRUE, eval = FALSE}
emm1 <- emmeans(model1, specs = "condition")
# Note order: Boost, BoostPlus, consensus
emm2 <- emmeans(model2, specs = "condition")
# Not comparable: since one is detrended and the other isn't
contrast_list <- list(
   "boost vs control" = c(0.5,  0.5, -1), 
   #av. boosts vs consensus
   "Boost vs BoostPlus" = c(1, -1,  0))
contrast(emm1, 
         method = contrast_list, 
         p.adjust = "holm")
```

---
# Data analysis - _t_-tests

.pull-left.small[
```{r ancova4b, echo = FALSE, eval = TRUE}
library(emmeans)
emm1 <- emmeans(model1, specs = "condition")
# Note order: Boost, BoostPlus, consensus
emm2 <- emmeans(model2, specs = "condition")
# Not comparable: since one is detrended and the other isn't
contrast_list <- list(
   "boost vs control" = c(0.5,  0.5, -1), 
   #av. boosts vs consensus
   "Boost vs BoostPlus" = c(1, -1,  0))
c1 <- contrast(emm1, 
         method = contrast_list, 
         p.adjust = "holm")
kableExtra::kable(c1,
                  col.names = c("contrast",
                                "estimate",
                                "se",
                                "df",
                                "t stat",
                                "p-value"), 
                  digits = c(2,2,2,0,2,2))
         
```
Contrasts with ANCOVA with `prior` (Holm-Bonferroni adjustment with $k=2$ tests)
]
.pull-right.small[
```{r ancova5a, echo = FALSE, eval = TRUE}
c2 <- contrast(emm2, 
         method = contrast_list, 
         p.adjust = "holm")
kableExtra::kable(c2,
                  col.names = c("contrast",
                                "estimate",
                                "se",
                                "df",
                                "t stat",
                                "p-value"), 
                  digits = c(2,2,2,0,2,2))
```
Contrasts for ANOVA (Holm-Bonferroni adjustment with $k=2$ tests)
]

---

# Data analysis - assumption checks

.pull-left.small[
```{r ancova5b, echo = TRUE, eval = FALSE}
# Test equality of variance
levene <- car::leveneTest(
   resid(model1) ~ condition, 
   data = SSVB21_S2,
   center = 'mean')
# Equality of slopes (interaction)
car::Anova(lm(post ~ condition * prior, 
           data = SSVB21_S2),
           model1, type = 2)
```

Levene's test of equality of variance: _F_ (`r broom::tidy(levene)$df`, `r broom::tidy(levene)$df.residual`) = `r round(broom::tidy(levene)$statistic, 2)` with a $p$-value of `r format.pval(broom::tidy(levene)$p.value,digits = 3)`.
]
.pull-right.small[


```{r kableEqSlope, eval = TRUE, echo = FALSE, cache = TRUE}
kableExtra::kable(broom::tidy(car::Anova(lm(post ~ condition * prior, 
		 data = hecedsm::SSVB21_S2), model1, type = 2)), 
		 col.names = c("term",
			 "sum of squares",
			 "df",
			 "statistic",
			 "p-value"), 
		 digits = c(0,0,0,1,2,3)) %>% 
kableExtra::row_spec(4, bold = TRUE, color = "black", background = "yellow")
```

Model with interaction `condition*prior`. Slopes don't differ between condition.
]

---

# The kitchen sink approach

.box-inv-7.medium.sp-after[Should we control for more stuff?]

.box-7.medium.sp-after[NO! ANCOVA is a design to reduce error]


- Randomization should ensure that there is no confounding
- No difference (on average) between group given a value of the covariate.
- If it isn't the case, adjustment matters

---

# Equal trends
.small[
- If trends are different, meaningful comparisons (?)
- Differences between groups depend on value of the covariate
]
```{r ancovadifftrend, echo = FALSE, eval = TRUE, out.width = '50%', cache = TRUE}
set.seed(123)
ng <- 24
x1 <- rgamma(n = ng, shape = 10, scale = 5)
x2 <- rgamma(n = ng, shape = 7, scale = 3)
dat <- data.frame(
  covariate = c(x1, x2),
  response = c(2 + 0.2*x1 + rnorm(ng, sd = 4), 5 + 0.25*x2 + rnorm(ng, sd = 4)),
  group = factor(rep(c("treatment", "control"), each = ng)))
ggplot(data = dat, aes(x = covariate, y = response, group = group, color = group)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  theme_classic() + 
  theme(legend.position = "bottom")
```
.small[Due to lack of overlap, comparisons hazardous as they entail extrapolation one way or another.]

---

# Testing equal slope

.box-7.sp-after[Compare two nested models]

- Null $\mathscr{H}_0$: model with covariate
- Alternative $\mathscr{H}_a$: model with interaction covariate * experimental factor

.center[Use `anova` to compare the models in **R**.]

---

layout: false
name: linear-mediation
class: center middle section-title section-title-6

# Moderation

---

layout: true
class: title title-6

---


# Moderator

A **moderator** $W$ modifies the direction or strength of the effect of an explanatory variable $X$ on a response $Y$ (interaction term).



```{tikz}
#| echo: false
#| cache: false
#| label: fig-dag-moderation
#| fig-cap: "Directed acyclic graph of moderation"
#| fig-align: 'center'
#| fig-ext: "png"
#| fig-dpi: 300
#| fig-width: 8
#| fig-height: 6
#| out-width: '50%'
\usetikzlibrary{positioning}
\begin{tikzpicture}[every node/.append style={minimum size=0.5cm}]
\node [draw,circle] (X) at (-2,0) {$X$};
\node [draw,circle] (Y) at (2,0) {$Y$};
\node [draw,circle] (W) at (0,1) {$W$};
\draw [-latex] (X) edge (Y);
\draw [-latex] (W) edge (0,0);
\end{tikzpicture}
```

.small[
Interactions are not limited to experimental factors: we can also have interactions with confounders, explanatories, mediators, etc.
]

---

# Moderation in a linear regression model

In a regression model, we simply include an **interaction** term to the model between $W$ and $X$.

For example, if $X$ is categorical with $K$ levels and $W$ is binary or continuous, imposing sum-to-zero constraints for $\alpha_1, \ldots, \alpha_K$ and $\beta_1, \ldots, \beta_K$ gives
$$
\underset{\text{average response of group $k$ at $w$}}{\mathrm{E}(Y \mid X=k, W=w)} = \underset{\text{intercept of group $k$}}{\alpha_0 +\alpha_k} + \underset{\text{slope of group $k$}}{(\beta_0 + \beta_k)} w
$$

---

# Testing for the interaction

Test jointly whether coefficients associated to $XW$ are zero, i.e., $$\beta_1 = \cdots = \beta_K=0.$$

The moderator $W$ can be continuous or categorical with $L \geq 2$ levels

The degrees of freedom (additional parameters for the interaction) in the $F$ test are

- $K-1$ for continuous $W$
   - are slopes parallel?
- $(K-1) \times (L-1)$ for categorical $W$
   - are all subgroup averages the same?

---

# Example

We consider data from [Garcia et al. (2010)](https://doi.org/10.1002/ejsp.644), a study on gender discrimination. Participants were given a fictional file where a women was turned down promotion in favour of male colleague despite her being clearly more experimented and qualified. 

The authors manipulated the decision of the participant, with choices:
.small[

- not to challenge the decision (no protest), 
- a request to reconsider based on individual qualities of the applicants (individual)
- a request to reconsider based on abilities of women (collective). 

]
The postulated moderator variable is `sexism`, which assesses pervasiveness of gender discrimination.

---
# Model fit

We fit the linear model with the interaction.

```{r}
#| eval: false
#| echo: true
data(GSBE10, package = "hecedsm")
lin_moder <- lm(respeval ~ protest*sexism, 
               data = GSBE10)
summary(lin_moder) # coefficients
car::Anova(lin_moder, type = 2) # tests
```

---

# ANOVA table

```{r}
#| label: tbl-testsmoder
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| tbl-cap: "Analysis of variance table for the linear moderation model."
data(GSBE10, package = "hecedsm")
lin_moder <- lm(respeval ~ protest*sexism, 
               data = GSBE10)
# summary(linmoder) # coefficients

options(knitr.kable.NA = '')
tab_anova <- broom::tidy(
    car::Anova(lin_moder, type = 2)[-1,])
tab_anova$p.value <- papaja::apa_p(tab_anova$p.value)
knitr::kable(tab_anova,
  booktabs = TRUE,
  col.names = c("term", "sum of squares","df","stat","p-value"),
  digits = c(0,2,1,2,3,3)) |>
  kableExtra::kable_styling()
```

---

# Effects

```{r}
#| eval: true
#| echo: false
#| warning: false
#| message: false
#| out-width: '65%'
ggplot(data = GSBE10,
       aes(x = sexism,
           y = respeval,
           color = protest,
           group = protest)) + 
  geom_point() +
  geom_smooth(se = FALSE, method = "lm", formula = y ~ x) +
  labs(subtitle = "evaluation of response",
       y = "",
       color = "experimental condition") +
  theme_classic() +
  theme(legend.position = "bottom")
```
.small[

Results won't necessarily be reliable outside of the range of observed values of sexism. 

]
---

# Comparisons between groups

Simple effects and comparisons must be done for a fixed value of sexism (since the slopes are not parallel).

The default value in `emmeans` is the mean value of `sexism`, but we could query for averages at different values of sexism (below for empirical quartiles).

```{r}
#| echo: true
#| eval: false
quart <-  quantile(GSBE10$sexism, probs = c(0.25, 0.5, 0.75))
emmeans(lin_moder, 
        specs = "protest", 
        by = "sexism",
        at = list("sexism" = quart))
```

.small[

With moderating *factors*, give weights to each sub-mean corresponding to the frequency of the moderator rather than equal-weight to each category (`weights = "prop"`).

]
---

# Johnson and Neyman method

The [Johnson and Neyman (1936)](https://doi.org/10.1007/bf02288864) method looks at the range of values of moderator $W$ for which difference between treatments (binary $X$) is not statistically significant.


```{r}
#| eval: false
#| echo: true
lin_moder2 <- lm(
  respeval ~ protest*sexism, 
  data = GSBE10 |> 
  # We dichotomize the manipulation, pooling protests together
  dplyr::mutate(protest = as.integer(protest != "no protest")))
# Test for equality of slopes/intercept for two protest groups
anova(lin_moder, lin_moder2)
# p-value of 0.18: fail to reject individual = collective.
```


---
# Syntax for plot

```{r}
#| eval: false
#| echo: true
jn <- interactions::johnson_neyman(
  model = lin_moder2, # linear model
  pred = protest, # binary experimental factor
  modx = sexism, # moderator
  control.fdr = TRUE, # control for false discovery rate
  mod.range = range(GSBE10$sexism)) # range of values for sexism
jn$plot

```


---

# Plot of Johnson−Neyman intervals

```{r}
#| eval: true
#| echo: false
#| fig-cap: "Johnson−Neyman plot for difference between protest and no protest as a function of sexism."
#| label: fig-jn
#| out-width: '70%'
# Fit model with interaction (binary)
lin_moder2 <- lm(
  respeval ~ protest*sexism, 
  data = GSBE10 |> 
  dplyr::mutate(protest = as.integer(protest != "no protest")))
# anova(lin_moder, lin_moder2)
jn <- interactions::johnson_neyman(
  model = lin_moder2, # linear model
  pred = protest, # binary experimental factor
  modx = sexism, # moderator
  control.fdr = TRUE, # control for false discovery rate
  mod.range = range(GSBE10$sexism)) # range of values for sexism
jn$plot
```

---

# Moderation

More generally, **moderation** refers to any explanatory variable (whether continuous or categorical) which **interacts** with the experimental manipulation.

- For categorical-categorical, this is a multiway ANOVA model
- For continuous-categorical, use linear regression


---

# Summary

.small[

* Inclusion of continuous covariates may help filtering out unwanted variability.
* These are typically variables measured before or alongside the response variable.
* This design reduce the residual error, leading to an increase in power (more ability to detect differences in average between experimental conditions).
* We are only interested in differences due to experimental condition (marginal effects).
* In general, there should be no interaction between covariates/blocking factors and experimental conditions. 
* This hypothesis can be assessed by comparing the models with and without interaction, if there are enough units (e.g., equality of slope for ANCOVA).
* Moderators are variables that interact with the experimental factor. We assess their presence by testing for an interaction in a linear regression model.

]
